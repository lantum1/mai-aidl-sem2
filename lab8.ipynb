{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –õ–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω–∞—è —Ä–∞–±–æ—Ç–∞ 8, —Å—Ç—É–¥–µ–Ω—Ç –£—Å—Ç–∏–Ω–æ–≤ –î–µ–Ω–∏—Å –ê–ª–µ–∫—Å–∞–Ω–¥—Ä–æ–≤–∏—á –ú8–û-406–ë-21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. –í—ã–±–æ—Ä –Ω–∞—á–∞–ª—å–Ω—ã—Ö —É—Å–ª–æ–≤–∏–π"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. –ù–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∑–∞–¥–∞—á–∏ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏\n",
    "–í –∫–∞—á–µ—Å—Ç–≤–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ –±—ã–ª –≤—ã–±—Ä–∞–Ω BCCD (https://github.com/Shenggan/BCCD_Dataset).\n",
    "\n",
    "–î–∞—Ç–∞—Å–µ—Ç –≤–∫–ª—é—á–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, –Ω–∞ –∫–æ—Ç–æ—Ä—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ç–∏–ø—ã –∫–ª–µ—Ç–æ–∫ –∫—Ä–æ–≤–∏, —Ç–∞–∫–∏–µ –∫–∞–∫ —ç—Ä–∏—Ç—Ä–æ—Ü–∏—Ç—ã, –ª–µ–π–∫–æ—Ü–∏—Ç—ã –∏ —Ç—Ä–æ–º–±–æ—Ü–∏—Ç—ã. –û–Ω –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω –¥–ª—è –∑–∞–¥–∞—á –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è –∏ –¥–µ—Ç–µ–∫—Ü–∏–∏ –æ–±—ä–µ–∫—Ç–æ–≤. –ê–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö —Å–æ–¥–µ—Ä–∂–∞—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–∏–ø–∞—Ö –∫–ª–µ—Ç–æ–∫ –∏ –∏—Ö –º–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏–∏ –≤ –≤–∏–¥–µ –ø—Ä—è–º–æ—É–≥–æ–ª—å–Ω—ã—Ö –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞—é—â–∏—Ö —Ä–∞–º–æ–∫ \n",
    "\n",
    "–û–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ –≤—ã–±–æ—Ä–∞:\n",
    "\n",
    "–° –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–π —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è, —ç—Ç–æ—Ç –¥–∞—Ç–∞—Å–µ—Ç –º–æ–∂–µ—Ç –±—ã—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å–∏—Å—Ç–µ–º, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–º–æ–≥–∞—é—Ç –≤ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–µ –∑–∞–±–æ–ª–µ–≤–∞–Ω–∏–π, —Ç–∞–∫–∏—Ö –∫–∞–∫ –∞–Ω–µ–º–∏—è, –ª–µ–π–∫–µ–º–∏—è, –∏–Ω—Ñ–µ–∫—Ü–∏–∏ –∏ –¥—Ä—É–≥–∏–µ —Ä–∞—Å—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ –∫—Ä–æ–≤–∏. –ú–æ–¥–µ–ª–∏ –¥–µ—Ç–µ–∫—Ü–∏–∏, –æ–±—É—á–µ–Ω–Ω—ã–µ –Ω–∞ BCCD, –º–æ–≥—É—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å –∏ –ø–æ–¥—Å—á–∏—Ç—ã–≤–∞—Ç—å —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ç–∏–ø—ã –∫–ª–µ—Ç–æ–∫ –∫—Ä–æ–≤–∏, —á—Ç–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É—Å–∫–æ—Ä—è–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. –í—ã–±–æ—Ä –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Precision - –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, –Ω–∞—Å–∫–æ–ª—å–∫–æ —É–≤–µ—Ä–µ–Ω—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –º–æ–¥–µ–ª–∏\n",
    "2) Recall - –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, –Ω–∞—Å–∫–æ–ª—å–∫–æ —Ö–æ—Ä–æ—à–æ –º–æ–¥–µ–ª—å –Ω–∞—Ö–æ–¥–∏—Ç –≤—Å–µ –Ω–∞—Å—Ç–æ—è—â–∏–µ –æ–±—ä–µ–∫—Ç—ã\n",
    "3) F1-Score - –≥–∞—Ä–º–æ–Ω–∏—á–µ—Å–∫–æ–µ —Å—Ä–µ–¥–Ω–µ–µ Precision –∏ Recall.\n",
    "4) mAP - —É—á–∏—Ç—ã–≤–∞–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å –∏ –ø–æ–∑–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ bounding box'–æ–≤ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. –°–æ–∑–¥–∞–Ω–∏–µ –±–µ–π–∑–ª–∞–π–Ω–∞ –∏ –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ü–æ–¥–≥–æ—Ç–æ–≤–∏–º –¥–∞—Ç–∞—Å–µ—Ç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "base_dir = 'BCCD_Dataset/BCCD'\n",
    "images_dir = os.path.join(base_dir, 'JPEGImages')\n",
    "annotations_dir = os.path.join(base_dir, 'Annotations')\n",
    "\n",
    "output_dir = 'bccd_yolo'\n",
    "images_out = os.path.join(output_dir, 'images')\n",
    "labels_out = os.path.join(output_dir, 'labels')\n",
    "\n",
    "for split in ['train', 'val']:\n",
    "    os.makedirs(os.path.join(images_out, split), exist_ok=True)\n",
    "    os.makedirs(os.path.join(labels_out, split), exist_ok=True)\n",
    "\n",
    "classes = ['RBC', 'WBC', 'Platelets']\n",
    "class_to_id = {name: i for i, name in enumerate(classes)}\n",
    "\n",
    "files = [f for f in os.listdir(annotations_dir) if f.endswith('.xml')]\n",
    "random.shuffle(files)\n",
    "split_index = int(0.8 * len(files))\n",
    "splits = {'train': files[:split_index], 'val': files[split_index:]}\n",
    "\n",
    "def convert_bbox(size, box):\n",
    "    dw = 1. / size[0]\n",
    "    dh = 1. / size[1]\n",
    "    x = (box[0] + box[1]) / 2.0\n",
    "    y = (box[2] + box[3]) / 2.0\n",
    "    w = box[1] - box[0]\n",
    "    h = box[3] - box[2]\n",
    "    return (x * dw, y * dh, w * dw, h * dh)\n",
    "\n",
    "for split, xml_files in splits.items():\n",
    "    for xml_file in xml_files:\n",
    "        xml_path = os.path.join(annotations_dir, xml_file)\n",
    "        tree = ET.parse(xml_path)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        image_name = root.find('filename').text\n",
    "        image_path = os.path.join(images_dir, image_name)\n",
    "        shutil.copy(image_path, os.path.join(images_out, split, image_name))\n",
    "\n",
    "        size = root.find('size')\n",
    "        w = int(size.find('width').text)\n",
    "        h = int(size.find('height').text)\n",
    "\n",
    "        label_path = os.path.join(labels_out, split, image_name.replace('.jpg', '.txt'))\n",
    "        with open(label_path, 'w') as f:\n",
    "            for obj in root.iter('object'):\n",
    "                cls = obj.find('name').text\n",
    "                if cls not in classes:\n",
    "                    continue\n",
    "                xml_box = obj.find('bndbox')\n",
    "                b = (\n",
    "                    float(xml_box.find('xmin').text),\n",
    "                    float(xml_box.find('xmax').text),\n",
    "                    float(xml_box.find('ymin').text),\n",
    "                    float(xml_box.find('ymax').text),\n",
    "                )\n",
    "                bb = convert_bbox((w, h), b)\n",
    "                f.write(f\"{class_to_id[cls]} {' '.join(map(str, bb))}\\n\")\n",
    "\n",
    "yaml_path = os.path.join(output_dir, 'data.yaml')\n",
    "with open(yaml_path, 'w') as f:\n",
    "    f.write(f\"\"\"train: {os.path.abspath(images_out + '/train')}\n",
    "val: {os.path.abspath(images_out + '/val')}\n",
    "\n",
    "nc: 3\n",
    "names: ['RBC', 'WBC', 'Platelets']\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. –û–±—É—á–∏—Ç—å —Å–≤–µ—Ä—Ç–æ—á–Ω—É—é –º–æ–¥–µ–ª—å (yolo11n) –∏–∑ ultralytics –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –∏ –æ—Ü–µ–Ω–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π –ø–æ –≤—ã–±—Ä–∞–Ω–Ω—ã–º –º–µ—Ç—Ä–∏–∫–∞–º –Ω–∞ –≤—ã–±—Ä–∞–Ω–Ω–æ–º –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5.35M/5.35M [00:00<00:00, 20.7MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.107 üöÄ Python-3.12.6 torch-2.6.0 CPU (Apple M1 Pro)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolo11n.pt, data=bccd_yolo/data.yaml, epochs=5, time=None, patience=100, batch=4, imgsz=640, save=True, save_period=-1, cache=False, device=cpu, workers=8, project=None, name=train8, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train8\n",
      "Overriding model.yaml nc=80 with nc=3\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
      " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
      " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
      " 23        [16, 19, 22]  1    431257  ultralytics.nn.modules.head.Detect           [3, [64, 128, 256]]           \n",
      "YOLO11n summary: 181 layers, 2,590,425 parameters, 2,590,409 gradients, 6.4 GFLOPs\n",
      "\n",
      "Transferred 448/499 items from pretrained weights\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/daustinov/study/multimedia/bccd_yolo/labels/train.cache... 291 images, 0 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 291/291 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/daustinov/study/multimedia/bccd_yolo/labels/val.cache... 73 images, 0 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 73/73 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs/detect/train8/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001429, momentum=0.9) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005), 87 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train8\u001b[0m\n",
      "Starting training for 5 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/5         0G      1.332      2.868      1.349        124        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 73/73 [01:07<00:00,  1.09it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:04<00:00,  2.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         73        987      0.921       0.21      0.488      0.282\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "        2/5         0G      1.292      1.745      1.246         51        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 73/73 [01:06<00:00,  1.10it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:03<00:00,  2.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         73        987      0.764      0.895      0.863      0.541\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "        3/5         0G      1.213      1.288       1.26         42        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 73/73 [01:02<00:00,  1.17it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:03<00:00,  2.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         73        987      0.775      0.887      0.875      0.577\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "        4/5         0G      1.148      1.155      1.217         48        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 73/73 [01:02<00:00,  1.16it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:04<00:00,  2.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         73        987      0.795       0.93      0.893      0.585\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "        5/5         0G      1.121      1.062      1.199        104        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 73/73 [01:12<00:00,  1.00it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:04<00:00,  2.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         73        987      0.822      0.885      0.896      0.597\n",
      "\n",
      "5 epochs completed in 0.098 hours.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer stripped from runs/detect/train8/weights/last.pt, 5.5MB\n",
      "Optimizer stripped from runs/detect/train8/weights/best.pt, 5.5MB\n",
      "\n",
      "Validating runs/detect/train8/weights/best.pt...\n",
      "Ultralytics 8.3.107 üöÄ Python-3.12.6 torch-2.6.0 CPU (Apple M1 Pro)\n",
      "YOLO11n summary (fused): 100 layers, 2,582,737 parameters, 0 gradients, 6.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:03<00:00,  2.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         73        987      0.823      0.883      0.896      0.596\n",
      "                   RBC         68        827      0.716      0.824       0.82      0.563\n",
      "                   WBC         71         73      0.935          1      0.971      0.771\n",
      "             Platelets         48         87      0.818      0.825      0.895      0.455\n",
      "Speed: 0.7ms preprocess, 47.3ms inference, 0.0ms loss, 3.7ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/train8\u001b[0m\n",
      "Ultralytics 8.3.107 üöÄ Python-3.12.6 torch-2.6.0 CPU (Apple M1 Pro)\n",
      "YOLO11n summary (fused): 100 layers, 2,582,737 parameters, 0 gradients, 6.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/daustinov/study/multimedia/bccd_yolo/labels/val.cache... 73 images, 0 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 73/73 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 19/19 [00:03<00:00,  4.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         73        987      0.823      0.883      0.896      0.596\n",
      "                   RBC         68        827      0.716      0.824       0.82      0.563\n",
      "                   WBC         71         73      0.935          1      0.971      0.771\n",
      "             Platelets         48         87      0.818      0.825      0.895      0.455\n",
      "Speed: 0.5ms preprocess, 45.9ms inference, 0.0ms loss, 3.9ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/train82\u001b[0m\n",
      "–ú–µ—Ç—Ä–∏–∫–∏ (yolo11n):\n",
      "Precision: 0.8232\n",
      "Recall: 0.8832\n",
      "F1-score: 0.8521\n",
      "mAP@0.5: 0.8955\n",
      "mAP@0.5:0.95: 0.5963\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 5\n",
    "\n",
    "model = YOLO('yolo11n.pt')\n",
    "\n",
    "results = model.train(\n",
    "    data='bccd_yolo/data.yaml',\n",
    "    epochs=EPOCHS,\n",
    "    imgsz=640,\n",
    "    device='cpu',\n",
    "    batch=BATCH_SIZE,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "metrics = model.val()\n",
    "precision = metrics.box.p.mean()\n",
    "recall = metrics.box.r.mean()\n",
    "f1_score = 2 * (precision * recall) / (precision + recall + 1e-9)\n",
    "map50 = metrics.box.map50.mean()\n",
    "map50_95 = metrics.box.map.mean()\n",
    "\n",
    "print(f\"–ú–µ—Ç—Ä–∏–∫–∏ (yolo11n):\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-score: {f1_score:.4f}\")\n",
    "print(f\"mAP@0.5: {map50:.4f}\")\n",
    "print(f\"mAP@0.5:0.95: {map50_95:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. –û–±—É—á–∏—Ç—å —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–Ω—É—é –º–æ–¥–µ–ª—å (rtdetr-l) –∏–∑ ultralytics –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –∏ –æ—Ü–µ–Ω–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π –ø–æ –≤—ã–±—Ä–∞–Ω–Ω—ã–º –º–µ—Ç—Ä–∏–∫–∞–º –Ω–∞ –≤—ã–±—Ä–∞–Ω–Ω–æ–º –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–û–±—É—á–∏–º –º–æ–¥–µ–ª—å rtdetr-l –∏–∑ Utralytics, —Ç–∞–∫ –∫–∞–∫ —Å–µ–º–µ–π—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π YoloV11 –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.107 üöÄ Python-3.12.6 torch-2.6.0 CPU (Apple M1 Pro)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=rtdetr-l.pt, data=bccd_yolo/data.yaml, epochs=2, time=None, patience=100, batch=4, imgsz=640, save=True, save_period=-1, cache=False, device=cpu, workers=8, project=None, name=train9, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train9\n",
      "Overriding model.yaml nc=80 with nc=3\n",
      "WARNING ‚ö†Ô∏è no model scale passed. Assuming scale='l'.\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1     25248  ultralytics.nn.modules.block.HGStem          [3, 32, 48]                   \n",
      "  1                  -1  6    155072  ultralytics.nn.modules.block.HGBlock         [48, 48, 128, 3, 6]           \n",
      "  2                  -1  1      1408  ultralytics.nn.modules.conv.DWConv           [128, 128, 3, 2, 1, False]    \n",
      "  3                  -1  6    839296  ultralytics.nn.modules.block.HGBlock         [128, 96, 512, 3, 6]          \n",
      "  4                  -1  1      5632  ultralytics.nn.modules.conv.DWConv           [512, 512, 3, 2, 1, False]    \n",
      "  5                  -1  6   1695360  ultralytics.nn.modules.block.HGBlock         [512, 192, 1024, 5, 6, True, False]\n",
      "  6                  -1  6   2055808  ultralytics.nn.modules.block.HGBlock         [1024, 192, 1024, 5, 6, True, True]\n",
      "  7                  -1  6   2055808  ultralytics.nn.modules.block.HGBlock         [1024, 192, 1024, 5, 6, True, True]\n",
      "  8                  -1  1     11264  ultralytics.nn.modules.conv.DWConv           [1024, 1024, 3, 2, 1, False]  \n",
      "  9                  -1  6   6708480  ultralytics.nn.modules.block.HGBlock         [1024, 384, 2048, 5, 6, True, False]\n",
      " 10                  -1  1    524800  ultralytics.nn.modules.conv.Conv             [2048, 256, 1, 1, None, 1, 1, False]\n",
      " 11                  -1  1    789760  ultralytics.nn.modules.transformer.AIFI      [256, 1024, 8]                \n",
      " 12                  -1  1     66048  ultralytics.nn.modules.conv.Conv             [256, 256, 1, 1]              \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14                   7  1    262656  ultralytics.nn.modules.conv.Conv             [1024, 256, 1, 1, None, 1, 1, False]\n",
      " 15            [-2, -1]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  3   2232320  ultralytics.nn.modules.block.RepC3           [512, 256, 3]                 \n",
      " 17                  -1  1     66048  ultralytics.nn.modules.conv.Conv             [256, 256, 1, 1]              \n",
      " 18                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 19                   3  1    131584  ultralytics.nn.modules.conv.Conv             [512, 256, 1, 1, None, 1, 1, False]\n",
      " 20            [-2, -1]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  3   2232320  ultralytics.nn.modules.block.RepC3           [512, 256, 3]                 \n",
      " 22                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 23            [-1, 17]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 24                  -1  3   2232320  ultralytics.nn.modules.block.RepC3           [512, 256, 3]                 \n",
      " 25                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 26            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 27                  -1  3   2232320  ultralytics.nn.modules.block.RepC3           [512, 256, 3]                 \n",
      " 28        [21, 24, 27]  1   7308017  ultralytics.nn.modules.head.RTDETRDecoder    [3, [256, 256, 256]]          \n",
      "rt-detr-l summary: 457 layers, 32,812,241 parameters, 32,812,241 gradients, 108.0 GFLOPs\n",
      "\n",
      "Transferred 926/941 items from pretrained weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/daustinov/study/multimedia/bccd_yolo/labels/train.cache... 291 images, 0 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 291/291 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/daustinov/study/multimedia/bccd_yolo/labels/val.cache... 73 images, 0 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 73/73 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs/detect/train9/labels.jpg... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001429, momentum=0.9) with parameter groups 143 weight(decay=0.0), 206 weight(decay=0.0005), 226 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train9\u001b[0m\n",
      "Starting training for 2 epochs...\n",
      "\n",
      "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/2         0G     0.8144      2.808     0.4955        112        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 73/73 [08:15<00:00,  6.79s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:33<00:00,  3.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         73        987       0.86      0.234      0.214      0.137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        2/2         0G     0.4867     0.7636     0.2374         46        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 73/73 [08:06<00:00,  6.66s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:29<00:00,  3.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         73        987      0.632      0.882      0.759        0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2 epochs completed in 0.291 hours.\n",
      "Optimizer stripped from runs/detect/train9/weights/last.pt, 66.1MB\n",
      "Optimizer stripped from runs/detect/train9/weights/best.pt, 66.1MB\n",
      "\n",
      "Validating runs/detect/train9/weights/best.pt...\n",
      "Ultralytics 8.3.107 üöÄ Python-3.12.6 torch-2.6.0 CPU (Apple M1 Pro)\n",
      "rt-detr-l summary: 302 layers, 31,989,905 parameters, 0 gradients, 103.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:28<00:00,  2.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         73        987      0.628      0.881      0.758        0.5\n",
      "                   RBC         68        827      0.432      0.839       0.63      0.429\n",
      "                   WBC         71         73       0.83          1      0.953      0.738\n",
      "             Platelets         48         87      0.622      0.805      0.692      0.334\n",
      "Speed: 0.8ms preprocess, 385.3ms inference, 0.0ms loss, 0.1ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/train9\u001b[0m\n",
      "Ultralytics 8.3.107 üöÄ Python-3.12.6 torch-2.6.0 CPU (Apple M1 Pro)\n",
      "rt-detr-l summary: 302 layers, 31,989,905 parameters, 0 gradients, 103.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/daustinov/study/multimedia/bccd_yolo/labels/val.cache... 73 images, 0 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 73/73 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 19/19 [00:29<00:00,  1.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         73        987      0.628      0.881      0.758        0.5\n",
      "                   RBC         68        827      0.432      0.839       0.63      0.429\n",
      "                   WBC         71         73       0.83          1      0.953      0.738\n",
      "             Platelets         48         87      0.622      0.805      0.692      0.334\n",
      "Speed: 0.6ms preprocess, 395.7ms inference, 0.0ms loss, 0.1ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/train92\u001b[0m\n",
      "–ú–µ—Ç—Ä–∏–∫–∏ (yolo8-vit):\n",
      "Precision: 0.6280\n",
      "Recall: 0.8813\n",
      "F1-score: 0.7334\n",
      "mAP@0.5: 0.7582\n",
      "mAP@0.5:0.95: 0.5001\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import RTDETR\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 2\n",
    "\n",
    "model = RTDETR('rtdetr-l.pt')\n",
    "\n",
    "results = model.train(\n",
    "    data='bccd_yolo/data.yaml',\n",
    "    epochs=EPOCHS,\n",
    "    imgsz=640,\n",
    "    device='cpu',\n",
    "    batch=BATCH_SIZE,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "metrics = model.val()\n",
    "precision = metrics.box.p.mean()\n",
    "recall = metrics.box.r.mean()\n",
    "f1_score = 2 * (precision * recall) / (precision + recall + 1e-9)\n",
    "map50 = metrics.box.map50.mean()\n",
    "map50_95 = metrics.box.map.mean()\n",
    "\n",
    "print(f\"–ú–µ—Ç—Ä–∏–∫–∏ (rtdetr-l):\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-score: {f1_score:.4f}\")\n",
    "print(f\"mAP@0.5: {map50:.4f}\")\n",
    "print(f\"mAP@0.5:0.95: {map50_95:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. –£–ª—É—á—à–µ–Ω–∏–µ –±–µ–π–∑–ª–∞–π–Ω–∞"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. –°—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –≥–∏–ø–æ—Ç–µ–∑—ã (–∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö, –ø–æ–¥–±–æ—Ä –º–æ–¥–µ–ª–µ–π, –ø–æ–¥–±–æ—Ä –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏ —Ç.–¥)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–¢–∞–∫ –∫–∞–∫ –º–æ–¥–µ–ª–∏ yolo11 –∏ rtdetr-l –∏—Å–ø–æ–ª—å–∑—É—é—Ç –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—é –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é, —Ç–æ –≤ –≥–∏–ø–æ—Ç–µ–∑—É –ø–æ —É–ª—É—á—à–µ–Ω–∏—é –±–µ–π–∑–ª–∞–π–Ω–∞ –Ω–µ –≤–∫–ª—é—á–∞—é —ç—Ç–æ.\n",
    "\n",
    "1. **–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤**. –ü–æ–¥–±–æ—Ä learning rate, batch size –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —ç–ø–æ—Ö\n",
    "2. **–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –¥—Ä—É–≥–æ–≥–æ –º–µ—Ç–æ–¥–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏**. –î–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏ –º–æ–∂–Ω–æ –ø–æ–ø—ã—Ç–∞—Ç—å—Å—è –∑–∞–º–µ–Ω–∏—Ç—å —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –Ω–∞ SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π, –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –ø–æ –º–µ—Ç—Ä–∏–∫–∞–º"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### –°–≤–µ—Ä—Ç–æ—á–Ω–∞—è –º–æ–¥–µ–ª—å yolo11n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.107 üöÄ Python-3.12.6 torch-2.6.0 CPU (Apple M1 Pro)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolo11n.pt, data=bccd_yolo/data.yaml, epochs=7, time=None, patience=100, batch=8, imgsz=640, save=True, save_period=-1, cache=False, device=cpu, workers=8, project=None, name=train10, exist_ok=False, pretrained=True, optimizer=AdamW, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.0005, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train10\n",
      "Overriding model.yaml nc=80 with nc=3\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
      " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
      " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
      " 23        [16, 19, 22]  1    431257  ultralytics.nn.modules.head.Detect           [3, [64, 128, 256]]           \n",
      "YOLO11n summary: 181 layers, 2,590,425 parameters, 2,590,409 gradients, 6.4 GFLOPs\n",
      "\n",
      "Transferred 448/499 items from pretrained weights\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/daustinov/study/multimedia/bccd_yolo/labels/train.cache... 291 images, 0 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 291/291 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/daustinov/study/multimedia/bccd_yolo/labels/val.cache... 73 images, 0 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 73/73 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs/detect/train10/labels.jpg... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.0005, momentum=0.937) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005), 87 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train10\u001b[0m\n",
      "Starting training for 7 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/7         0G      1.291      1.986      1.236        108        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [01:02<00:00,  1.69s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:13<00:00,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         73        987     0.0359      0.542      0.337      0.201\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "        2/7         0G      1.176      1.389      1.136         73        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [01:03<00:00,  1.71s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:11<00:00,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         73        987      0.571      0.549      0.575      0.392\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "        3/7         0G       1.15      1.238      1.124         43        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:54<00:00,  1.47s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:11<00:00,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         73        987      0.887      0.595      0.783      0.534\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "        4/7         0G      1.132      1.181      1.128         61        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:55<00:00,  1.51s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:11<00:00,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         73        987      0.759      0.822      0.892      0.602\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "        5/7         0G      1.109      1.103       1.11        121        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [01:02<00:00,  1.70s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:11<00:00,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         73        987       0.84      0.839      0.884        0.6\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "        6/7         0G      1.076       1.07      1.099         77        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:55<00:00,  1.49s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:11<00:00,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         73        987      0.821      0.911      0.894      0.602\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "        7/7         0G       1.09      1.057      1.115         83        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:57<00:00,  1.56s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:11<00:00,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         73        987       0.83      0.904      0.892      0.603\n",
      "\n",
      "7 epochs completed in 0.138 hours.\n",
      "Optimizer stripped from runs/detect/train10/weights/last.pt, 5.5MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer stripped from runs/detect/train10/weights/best.pt, 5.5MB\n",
      "\n",
      "Validating runs/detect/train10/weights/best.pt...\n",
      "Ultralytics 8.3.107 üöÄ Python-3.12.6 torch-2.6.0 CPU (Apple M1 Pro)\n",
      "YOLO11n summary (fused): 100 layers, 2,582,737 parameters, 0 gradients, 6.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:10<00:00,  2.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         73        987       0.83      0.905      0.892      0.602\n",
      "                   RBC         68        827      0.706      0.843      0.816      0.566\n",
      "                   WBC         71         73       0.95          1       0.99      0.784\n",
      "             Platelets         48         87      0.835      0.871      0.869      0.457\n",
      "Speed: 0.8ms preprocess, 139.8ms inference, 0.0ms loss, 4.6ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/train10\u001b[0m\n",
      "Ultralytics 8.3.107 üöÄ Python-3.12.6 torch-2.6.0 CPU (Apple M1 Pro)\n",
      "YOLO11n summary (fused): 100 layers, 2,582,737 parameters, 0 gradients, 6.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/daustinov/study/multimedia/bccd_yolo/labels/val.cache... 73 images, 0 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 73/73 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:03<00:00,  2.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         73        987       0.83      0.905      0.892      0.602\n",
      "                   RBC         68        827      0.706      0.843      0.816      0.566\n",
      "                   WBC         71         73       0.95          1       0.99      0.784\n",
      "             Platelets         48         87      0.835      0.871      0.869      0.457\n",
      "Speed: 0.8ms preprocess, 38.5ms inference, 0.0ms loss, 4.5ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/train102\u001b[0m\n",
      "–ú–µ—Ç—Ä–∏–∫–∏ (yolo11n):\n",
      "Precision: 0.8305\n",
      "Recall: 0.9046\n",
      "F1-score: 0.8659\n",
      "mAP@0.5: 0.8916\n",
      "mAP@0.5:0.95: 0.6022\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 7\n",
    "LEARNING_RATE = 0.0005\n",
    "\n",
    "model = YOLO('yolo11n.pt')\n",
    "\n",
    "results = model.train(\n",
    "    data='bccd_yolo/data.yaml',\n",
    "    epochs=EPOCHS,\n",
    "    imgsz=640,\n",
    "    device='cpu',\n",
    "    batch=BATCH_SIZE,\n",
    "    optimizer='AdamW',\n",
    "    lr0=LEARNING_RATE,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "metrics = model.val()\n",
    "precision = metrics.box.p.mean()\n",
    "recall = metrics.box.r.mean()\n",
    "f1_score = 2 * (precision * recall) / (precision + recall + 1e-9)\n",
    "map50 = metrics.box.map50.mean()\n",
    "map50_95 = metrics.box.map.mean()\n",
    "\n",
    "print(f\"–ú–µ—Ç—Ä–∏–∫–∏ (yolo11n):\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-score: {f1_score:.4f}\")\n",
    "print(f\"mAP@0.5: {map50:.4f}\")\n",
    "print(f\"mAP@0.5:0.95: {map50_95:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–Ω–∞—è –º–æ–¥–µ–ª—å rtdetr-l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.107 üöÄ Python-3.12.6 torch-2.6.0 CPU (Apple M1 Pro)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=rtdetr-l.pt, data=bccd_yolo/data.yaml, epochs=4, time=None, patience=100, batch=8, imgsz=640, save=True, save_period=-1, cache=False, device=cpu, workers=8, project=None, name=train11, exist_ok=False, pretrained=True, optimizer=AdamW, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.0005, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train11\n",
      "Overriding model.yaml nc=80 with nc=3\n",
      "WARNING ‚ö†Ô∏è no model scale passed. Assuming scale='l'.\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1     25248  ultralytics.nn.modules.block.HGStem          [3, 32, 48]                   \n",
      "  1                  -1  6    155072  ultralytics.nn.modules.block.HGBlock         [48, 48, 128, 3, 6]           \n",
      "  2                  -1  1      1408  ultralytics.nn.modules.conv.DWConv           [128, 128, 3, 2, 1, False]    \n",
      "  3                  -1  6    839296  ultralytics.nn.modules.block.HGBlock         [128, 96, 512, 3, 6]          \n",
      "  4                  -1  1      5632  ultralytics.nn.modules.conv.DWConv           [512, 512, 3, 2, 1, False]    \n",
      "  5                  -1  6   1695360  ultralytics.nn.modules.block.HGBlock         [512, 192, 1024, 5, 6, True, False]\n",
      "  6                  -1  6   2055808  ultralytics.nn.modules.block.HGBlock         [1024, 192, 1024, 5, 6, True, True]\n",
      "  7                  -1  6   2055808  ultralytics.nn.modules.block.HGBlock         [1024, 192, 1024, 5, 6, True, True]\n",
      "  8                  -1  1     11264  ultralytics.nn.modules.conv.DWConv           [1024, 1024, 3, 2, 1, False]  \n",
      "  9                  -1  6   6708480  ultralytics.nn.modules.block.HGBlock         [1024, 384, 2048, 5, 6, True, False]\n",
      " 10                  -1  1    524800  ultralytics.nn.modules.conv.Conv             [2048, 256, 1, 1, None, 1, 1, False]\n",
      " 11                  -1  1    789760  ultralytics.nn.modules.transformer.AIFI      [256, 1024, 8]                \n",
      " 12                  -1  1     66048  ultralytics.nn.modules.conv.Conv             [256, 256, 1, 1]              \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14                   7  1    262656  ultralytics.nn.modules.conv.Conv             [1024, 256, 1, 1, None, 1, 1, False]\n",
      " 15            [-2, -1]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  3   2232320  ultralytics.nn.modules.block.RepC3           [512, 256, 3]                 \n",
      " 17                  -1  1     66048  ultralytics.nn.modules.conv.Conv             [256, 256, 1, 1]              \n",
      " 18                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 19                   3  1    131584  ultralytics.nn.modules.conv.Conv             [512, 256, 1, 1, None, 1, 1, False]\n",
      " 20            [-2, -1]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  3   2232320  ultralytics.nn.modules.block.RepC3           [512, 256, 3]                 \n",
      " 22                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 23            [-1, 17]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 24                  -1  3   2232320  ultralytics.nn.modules.block.RepC3           [512, 256, 3]                 \n",
      " 25                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 26            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 27                  -1  3   2232320  ultralytics.nn.modules.block.RepC3           [512, 256, 3]                 \n",
      " 28        [21, 24, 27]  1   7308017  ultralytics.nn.modules.head.RTDETRDecoder    [3, [256, 256, 256]]          \n",
      "rt-detr-l summary: 457 layers, 32,812,241 parameters, 32,812,241 gradients, 108.0 GFLOPs\n",
      "\n",
      "Transferred 926/941 items from pretrained weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/daustinov/study/multimedia/bccd_yolo/labels/train.cache... 291 images, 0 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 291/291 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/daustinov/study/multimedia/bccd_yolo/labels/val.cache... 73 images, 0 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 73/73 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs/detect/train11/labels.jpg... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.0005, momentum=0.937) with parameter groups 143 weight(decay=0.0), 206 weight(decay=0.0005), 226 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train11\u001b[0m\n",
      "Starting training for 4 epochs...\n",
      "\n",
      "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/4         0G      1.229     0.9256      0.724        103        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [08:04<00:00, 13.09s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [01:24<00:00, 16.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         73        987    0.00412      0.109    0.00633    0.00156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        2/4         0G     0.7153      1.011     0.3001         66        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [07:48<00:00, 12.65s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [01:24<00:00, 16.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         73        987     0.0123      0.327     0.0586     0.0275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        3/4         0G     0.4957      1.193     0.2144         36        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [07:40<00:00, 12.45s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [01:24<00:00, 16.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         73        987     0.0125      0.331     0.0541     0.0324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        4/4         0G     0.4443       1.22     0.1933         55        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [08:04<00:00, 13.09s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [01:29<00:00, 17.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         73        987     0.0125      0.331     0.0445     0.0283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4 epochs completed in 0.623 hours.\n",
      "Optimizer stripped from runs/detect/train11/weights/last.pt, 66.1MB\n",
      "Optimizer stripped from runs/detect/train11/weights/best.pt, 66.1MB\n",
      "\n",
      "Validating runs/detect/train11/weights/best.pt...\n",
      "Ultralytics 8.3.107 üöÄ Python-3.12.6 torch-2.6.0 CPU (Apple M1 Pro)\n",
      "rt-detr-l summary: 302 layers, 31,989,905 parameters, 0 gradients, 103.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [01:21<00:00, 16.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         73        987     0.0125      0.331     0.0534     0.0321\n",
      "                   RBC         68        827     0.0375      0.994       0.16     0.0962\n",
      "                   WBC         71         73          0          0          0          0\n",
      "             Platelets         48         87          0          0          0          0\n",
      "Speed: 0.8ms preprocess, 1116.5ms inference, 0.0ms loss, 0.1ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/train11\u001b[0m\n",
      "Ultralytics 8.3.107 üöÄ Python-3.12.6 torch-2.6.0 CPU (Apple M1 Pro)\n",
      "rt-detr-l summary: 302 layers, 31,989,905 parameters, 0 gradients, 103.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/daustinov/study/multimedia/bccd_yolo/labels/val.cache... 73 images, 0 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 73/73 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:26<00:00,  2.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         73        987     0.0125      0.331     0.0534     0.0321\n",
      "                   RBC         68        827     0.0375      0.994       0.16     0.0962\n",
      "                   WBC         71         73          0          0          0          0\n",
      "             Platelets         48         87          0          0          0          0\n",
      "Speed: 1.0ms preprocess, 359.8ms inference, 0.0ms loss, 0.1ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/train112\u001b[0m\n",
      "–ú–µ—Ç—Ä–∏–∫–∏ (rtdetr-l):\n",
      "Precision: 0.0125\n",
      "Recall: 0.3313\n",
      "F1-score: 0.0241\n",
      "mAP@0.5: 0.0534\n",
      "mAP@0.5:0.95: 0.0321\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import RTDETR\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 4\n",
    "LEARNING_RATE = 0.0005\n",
    "\n",
    "model = RTDETR('rtdetr-l.pt')\n",
    "\n",
    "results = model.train(\n",
    "    data='bccd_yolo/data.yaml',\n",
    "    epochs=EPOCHS,\n",
    "    imgsz=640,\n",
    "    device='cpu',\n",
    "    batch=BATCH_SIZE,\n",
    "    optimizer='AdamW',\n",
    "    lr0=LEARNING_RATE,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "metrics = model.val()\n",
    "precision = metrics.box.p.mean()\n",
    "recall = metrics.box.r.mean()\n",
    "f1_score = 2 * (precision * recall) / (precision + recall + 1e-9)\n",
    "map50 = metrics.box.map50.mean()\n",
    "map50_95 = metrics.box.map.mean()\n",
    "\n",
    "print(f\"–ú–µ—Ç—Ä–∏–∫–∏ (rtdetr-l):\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-score: {f1_score:.4f}\")\n",
    "print(f\"mAP@0.5: {map50:.4f}\")\n",
    "print(f\"mAP@0.5:0.95: {map50_95:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –¥—Ä—É–≥–æ–≥–æ –º–µ—Ç–æ–¥–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### –°–≤–µ—Ä—Ç–æ—á–Ω–∞—è –º–æ–¥–µ–ª—å yolo11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.107 üöÄ Python-3.12.6 torch-2.6.0 CPU (Apple M1 Pro)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolo11n.pt, data=bccd_yolo/data.yaml, epochs=5, time=None, patience=100, batch=4, imgsz=640, save=True, save_period=-1, cache=False, device=cpu, workers=8, project=None, name=train14, exist_ok=False, pretrained=True, optimizer=SGD, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train14\n",
      "Overriding model.yaml nc=80 with nc=3\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
      " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
      " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
      " 23        [16, 19, 22]  1    431257  ultralytics.nn.modules.head.Detect           [3, [64, 128, 256]]           \n",
      "YOLO11n summary: 181 layers, 2,590,425 parameters, 2,590,409 gradients, 6.4 GFLOPs\n",
      "\n",
      "Transferred 448/499 items from pretrained weights\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/daustinov/study/multimedia/bccd_yolo/labels/train.cache... 291 images, 0 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 291/291 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/daustinov/study/multimedia/bccd_yolo/labels/val.cache... 73 images, 0 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 73/73 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs/detect/train14/labels.jpg... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01, momentum=0.937) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005), 87 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train14\u001b[0m\n",
      "Starting training for 5 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/5         0G      1.335      2.829      1.406        124        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 73/73 [01:06<00:00,  1.09it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:04<00:00,  2.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         73        987      0.917      0.183      0.334      0.215\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "        2/5         0G      1.254      1.643      1.262         51        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 73/73 [01:03<00:00,  1.16it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:04<00:00,  2.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         73        987      0.489      0.169      0.211      0.128\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "        3/5         0G      1.214      1.331      1.228         42        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 73/73 [01:00<00:00,  1.20it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:04<00:00,  2.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         73        987      0.814      0.881      0.892      0.586\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "        4/5         0G      1.146       1.16      1.187         48        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 73/73 [00:56<00:00,  1.30it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:04<00:00,  2.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         73        987      0.845      0.881        0.9      0.595\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "        5/5         0G      1.117      1.069      1.163        104        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 73/73 [00:55<00:00,  1.33it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:04<00:00,  2.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         73        987      0.829      0.904      0.904      0.599\n",
      "\n",
      "5 epochs completed in 0.090 hours.\n",
      "Optimizer stripped from runs/detect/train14/weights/last.pt, 5.5MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer stripped from runs/detect/train14/weights/best.pt, 5.5MB\n",
      "\n",
      "Validating runs/detect/train14/weights/best.pt...\n",
      "Ultralytics 8.3.107 üöÄ Python-3.12.6 torch-2.6.0 CPU (Apple M1 Pro)\n",
      "YOLO11n summary (fused): 100 layers, 2,582,737 parameters, 0 gradients, 6.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:04<00:00,  2.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         73        987      0.829      0.904      0.904      0.599\n",
      "                   RBC         68        827      0.666      0.862      0.826      0.558\n",
      "                   WBC         71         73      0.974          1      0.991      0.779\n",
      "             Platelets         48         87      0.847      0.851      0.896       0.46\n",
      "Speed: 1.0ms preprocess, 42.0ms inference, 0.0ms loss, 9.3ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/train14\u001b[0m\n",
      "Ultralytics 8.3.107 üöÄ Python-3.12.6 torch-2.6.0 CPU (Apple M1 Pro)\n",
      "YOLO11n summary (fused): 100 layers, 2,582,737 parameters, 0 gradients, 6.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/daustinov/study/multimedia/bccd_yolo/labels/val.cache... 73 images, 0 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 73/73 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 19/19 [00:04<00:00,  4.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         73        987      0.829      0.904      0.904      0.599\n",
      "                   RBC         68        827      0.666      0.862      0.826      0.558\n",
      "                   WBC         71         73      0.974          1      0.991      0.779\n",
      "             Platelets         48         87      0.847      0.851      0.896       0.46\n",
      "Speed: 0.8ms preprocess, 42.9ms inference, 0.0ms loss, 9.1ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/train142\u001b[0m\n",
      "–ú–µ—Ç—Ä–∏–∫–∏ (yolo11n):\n",
      "Precision: 0.8291\n",
      "Recall: 0.9042\n",
      "F1-score: 0.8651\n",
      "mAP@0.5: 0.9043\n",
      "mAP@0.5:0.95: 0.5991\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 5\n",
    "\n",
    "model = YOLO('yolo11n.pt')\n",
    "\n",
    "results = model.train(\n",
    "    data='bccd_yolo/data.yaml',\n",
    "    epochs=EPOCHS,\n",
    "    imgsz=640,\n",
    "    device='cpu',\n",
    "    optimizer='SGD',\n",
    "    batch=BATCH_SIZE,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "metrics = model.val()\n",
    "precision = metrics.box.p.mean()\n",
    "recall = metrics.box.r.mean()\n",
    "f1_score = 2 * (precision * recall) / (precision + recall + 1e-9)\n",
    "map50 = metrics.box.map50.mean()\n",
    "map50_95 = metrics.box.map.mean()\n",
    "\n",
    "print(f\"–ú–µ—Ç—Ä–∏–∫–∏ (yolo11n):\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-score: {f1_score:.4f}\")\n",
    "print(f\"mAP@0.5: {map50:.4f}\")\n",
    "print(f\"mAP@0.5:0.95: {map50_95:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–Ω–∞—è –º–æ–¥–µ–ª—å rtdetr-l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.107 üöÄ Python-3.12.6 torch-2.6.0 CPU (Apple M1 Pro)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=rtdetr-l.pt, data=bccd_yolo/data.yaml, epochs=2, time=None, patience=100, batch=4, imgsz=640, save=True, save_period=-1, cache=False, device=cpu, workers=8, project=None, name=train15, exist_ok=False, pretrained=True, optimizer=SGD, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train15\n",
      "Overriding model.yaml nc=80 with nc=3\n",
      "WARNING ‚ö†Ô∏è no model scale passed. Assuming scale='l'.\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1     25248  ultralytics.nn.modules.block.HGStem          [3, 32, 48]                   \n",
      "  1                  -1  6    155072  ultralytics.nn.modules.block.HGBlock         [48, 48, 128, 3, 6]           \n",
      "  2                  -1  1      1408  ultralytics.nn.modules.conv.DWConv           [128, 128, 3, 2, 1, False]    \n",
      "  3                  -1  6    839296  ultralytics.nn.modules.block.HGBlock         [128, 96, 512, 3, 6]          \n",
      "  4                  -1  1      5632  ultralytics.nn.modules.conv.DWConv           [512, 512, 3, 2, 1, False]    \n",
      "  5                  -1  6   1695360  ultralytics.nn.modules.block.HGBlock         [512, 192, 1024, 5, 6, True, False]\n",
      "  6                  -1  6   2055808  ultralytics.nn.modules.block.HGBlock         [1024, 192, 1024, 5, 6, True, True]\n",
      "  7                  -1  6   2055808  ultralytics.nn.modules.block.HGBlock         [1024, 192, 1024, 5, 6, True, True]\n",
      "  8                  -1  1     11264  ultralytics.nn.modules.conv.DWConv           [1024, 1024, 3, 2, 1, False]  \n",
      "  9                  -1  6   6708480  ultralytics.nn.modules.block.HGBlock         [1024, 384, 2048, 5, 6, True, False]\n",
      " 10                  -1  1    524800  ultralytics.nn.modules.conv.Conv             [2048, 256, 1, 1, None, 1, 1, False]\n",
      " 11                  -1  1    789760  ultralytics.nn.modules.transformer.AIFI      [256, 1024, 8]                \n",
      " 12                  -1  1     66048  ultralytics.nn.modules.conv.Conv             [256, 256, 1, 1]              \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14                   7  1    262656  ultralytics.nn.modules.conv.Conv             [1024, 256, 1, 1, None, 1, 1, False]\n",
      " 15            [-2, -1]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  3   2232320  ultralytics.nn.modules.block.RepC3           [512, 256, 3]                 \n",
      " 17                  -1  1     66048  ultralytics.nn.modules.conv.Conv             [256, 256, 1, 1]              \n",
      " 18                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 19                   3  1    131584  ultralytics.nn.modules.conv.Conv             [512, 256, 1, 1, None, 1, 1, False]\n",
      " 20            [-2, -1]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  3   2232320  ultralytics.nn.modules.block.RepC3           [512, 256, 3]                 \n",
      " 22                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 23            [-1, 17]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 24                  -1  3   2232320  ultralytics.nn.modules.block.RepC3           [512, 256, 3]                 \n",
      " 25                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 26            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 27                  -1  3   2232320  ultralytics.nn.modules.block.RepC3           [512, 256, 3]                 \n",
      " 28        [21, 24, 27]  1   7308017  ultralytics.nn.modules.head.RTDETRDecoder    [3, [256, 256, 256]]          \n",
      "rt-detr-l summary: 457 layers, 32,812,241 parameters, 32,812,241 gradients, 108.0 GFLOPs\n",
      "\n",
      "Transferred 926/941 items from pretrained weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/daustinov/study/multimedia/bccd_yolo/labels/train.cache... 291 images, 0 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 291/291 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/daustinov/study/multimedia/bccd_yolo/labels/val.cache... 73 images, 0 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 73/73 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs/detect/train15/labels.jpg... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01, momentum=0.937) with parameter groups 143 weight(decay=0.0), 206 weight(decay=0.0005), 226 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train15\u001b[0m\n",
      "Starting training for 2 epochs...\n",
      "\n",
      "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/2         0G      1.222      1.705     0.7222        112        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 73/73 [06:49<00:00,  5.61s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:29<00:00,  2.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         73        987     0.0171      0.243     0.0202    0.00642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        2/2         0G     0.9802      0.742     0.4727         46        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 73/73 [06:49<00:00,  5.61s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:31<00:00,  3.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         73        987      0.369      0.155     0.0283    0.00803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2 epochs completed in 0.245 hours.\n",
      "Optimizer stripped from runs/detect/train15/weights/last.pt, 66.1MB\n",
      "Optimizer stripped from runs/detect/train15/weights/best.pt, 66.1MB\n",
      "\n",
      "Validating runs/detect/train15/weights/best.pt...\n",
      "Ultralytics 8.3.107 üöÄ Python-3.12.6 torch-2.6.0 CPU (Apple M1 Pro)\n",
      "rt-detr-l summary: 302 layers, 31,989,905 parameters, 0 gradients, 103.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:30<00:00,  3.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         73        987      0.368      0.155      0.029    0.00827\n",
      "                   RBC         68        827     0.0711       0.45     0.0798     0.0226\n",
      "                   WBC         71         73      0.034     0.0137    0.00706    0.00216\n",
      "             Platelets         48         87          1          0          0          0\n",
      "Speed: 1.0ms preprocess, 413.7ms inference, 0.0ms loss, 0.1ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/train15\u001b[0m\n",
      "Ultralytics 8.3.107 üöÄ Python-3.12.6 torch-2.6.0 CPU (Apple M1 Pro)\n",
      "rt-detr-l summary: 302 layers, 31,989,905 parameters, 0 gradients, 103.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/daustinov/study/multimedia/bccd_yolo/labels/val.cache... 73 images, 0 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 73/73 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 19/19 [00:31<00:00,  1.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         73        987      0.368      0.155      0.029    0.00827\n",
      "                   RBC         68        827     0.0711       0.45     0.0798     0.0226\n",
      "                   WBC         71         73      0.034     0.0137    0.00706    0.00216\n",
      "             Platelets         48         87          1          0          0          0\n",
      "Speed: 0.7ms preprocess, 432.5ms inference, 0.0ms loss, 0.1ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/train152\u001b[0m\n",
      "–ú–µ—Ç—Ä–∏–∫–∏ (rtdetr-l):\n",
      "Precision: 0.3684\n",
      "Recall: 0.1545\n",
      "F1-score: 0.2177\n",
      "mAP@0.5: 0.0290\n",
      "mAP@0.5:0.95: 0.0083\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import RTDETR\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 2\n",
    "\n",
    "model = RTDETR('rtdetr-l.pt')\n",
    "\n",
    "results = model.train(\n",
    "    data='bccd_yolo/data.yaml',\n",
    "    epochs=EPOCHS,\n",
    "    imgsz=640,\n",
    "    device='cpu',\n",
    "    optimizer = 'SGD',\n",
    "    batch=BATCH_SIZE,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "metrics = model.val()\n",
    "precision = metrics.box.p.mean()\n",
    "recall = metrics.box.r.mean()\n",
    "f1_score = 2 * (precision * recall) / (precision + recall + 1e-9)\n",
    "map50 = metrics.box.map50.mean()\n",
    "map50_95 = metrics.box.map.mean()\n",
    "\n",
    "print(f\"–ú–µ—Ç—Ä–∏–∫–∏ (rtdetr-l):\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-score: {f1_score:.4f}\")\n",
    "print(f\"mAP@0.5: {map50:.4f}\")\n",
    "print(f\"mAP@0.5:0.95: {map50_95:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –û–∫–æ–Ω—á–∞—Ç–µ–ª—å–Ω—ã–π —É–ª—É—á—à–µ–Ω–Ω—ã–π –±–µ–π–∑–ª–∞–π–Ω"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### –°–≤–µ—Ä—Ç–æ—á–Ω–∞—è –º–æ–¥–µ–ª—å yolo11n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ò—Å–ø–æ–ª—å–∑—É–µ–º —É–ª—É—á—à–µ–Ω–∏—è –∏–∑ –æ–±–æ–∏—Ö –≥–∏–ø–æ—Ç–µ–∑ - –æ–±–µ –≥–∏–ø–æ—Ç–µ–∑—ã –ø–æ–∫–∞–∑–∞–ª–∏ –ª—É—á—à–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –∏–∑–Ω–∞—á–∞–ª—å–Ω—ã–º –≤–∞—Ä–∏–∞–Ω—Ç–æ–º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.107 üöÄ Python-3.12.6 torch-2.6.0 CPU (Apple M1 Pro)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolo11n.pt, data=bccd_yolo/data.yaml, epochs=7, time=None, patience=100, batch=4, imgsz=640, save=True, save_period=-1, cache=False, device=cpu, workers=8, project=None, name=train19, exist_ok=False, pretrained=True, optimizer=SGD, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train19\n",
      "Overriding model.yaml nc=80 with nc=3\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
      " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
      " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
      " 23        [16, 19, 22]  1    431257  ultralytics.nn.modules.head.Detect           [3, [64, 128, 256]]           \n",
      "YOLO11n summary: 181 layers, 2,590,425 parameters, 2,590,409 gradients, 6.4 GFLOPs\n",
      "\n",
      "Transferred 448/499 items from pretrained weights\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/daustinov/study/multimedia/bccd_yolo/labels/train.cache... 291 images, 0 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 291/291 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/daustinov/study/multimedia/bccd_yolo/labels/val.cache... 73 images, 0 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 73/73 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs/detect/train19/labels.jpg... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01, momentum=0.937) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005), 87 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train19\u001b[0m\n",
      "Starting training for 7 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/7         0G      1.335      2.829      1.406        124        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 73/73 [01:00<00:00,  1.21it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:03<00:00,  2.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         73        987      0.917      0.183      0.334      0.215\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "        2/7         0G      1.254      1.641      1.262         51        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 73/73 [00:57<00:00,  1.26it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:04<00:00,  2.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         73        987      0.172      0.713       0.54      0.318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        3/7         0G      1.234      1.323      1.232         42        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 73/73 [00:57<00:00,  1.26it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:04<00:00,  2.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         73        987      0.658      0.256      0.418      0.231\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "        4/7         0G      1.178      1.171      1.197         48        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 73/73 [00:57<00:00,  1.27it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:04<00:00,  2.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         73        987      0.803      0.442      0.648      0.358\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "        5/7         0G      1.138      1.064      1.174        104        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 73/73 [00:57<00:00,  1.27it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:04<00:00,  2.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         73        987      0.792      0.851       0.87      0.577\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "        6/7         0G      1.085      1.031      1.167         81        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 73/73 [00:56<00:00,  1.29it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:04<00:00,  2.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         73        987      0.851      0.857      0.895      0.588\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "        7/7         0G      1.088      1.019      1.171         74        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 73/73 [00:57<00:00,  1.27it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:04<00:00,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         73        987      0.846       0.87      0.908      0.607\n",
      "\n",
      "7 epochs completed in 0.121 hours.\n",
      "Optimizer stripped from runs/detect/train19/weights/last.pt, 5.5MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer stripped from runs/detect/train19/weights/best.pt, 5.5MB\n",
      "\n",
      "Validating runs/detect/train19/weights/best.pt...\n",
      "Ultralytics 8.3.107 üöÄ Python-3.12.6 torch-2.6.0 CPU (Apple M1 Pro)\n",
      "YOLO11n summary (fused): 100 layers, 2,582,737 parameters, 0 gradients, 6.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:04<00:00,  2.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         73        987      0.846      0.869      0.908      0.607\n",
      "                   RBC         68        827      0.752      0.765      0.817      0.568\n",
      "                   WBC         71         73      0.966          1      0.987      0.794\n",
      "             Platelets         48         87      0.821      0.841      0.921      0.459\n",
      "Speed: 0.5ms preprocess, 42.1ms inference, 0.0ms loss, 9.5ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/train19\u001b[0m\n",
      "Ultralytics 8.3.107 üöÄ Python-3.12.6 torch-2.6.0 CPU (Apple M1 Pro)\n",
      "YOLO11n summary (fused): 100 layers, 2,582,737 parameters, 0 gradients, 6.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/daustinov/study/multimedia/bccd_yolo/labels/val.cache... 73 images, 0 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 73/73 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 19/19 [00:04<00:00,  4.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         73        987      0.846      0.869      0.908      0.607\n",
      "                   RBC         68        827      0.752      0.765      0.817      0.568\n",
      "                   WBC         71         73      0.966          1      0.987      0.794\n",
      "             Platelets         48         87      0.821      0.841      0.921      0.459\n",
      "Speed: 1.1ms preprocess, 42.4ms inference, 0.0ms loss, 9.7ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/train192\u001b[0m\n",
      "–ú–µ—Ç—Ä–∏–∫–∏ (yolo11n):\n",
      "Precision: 0.8460\n",
      "Recall: 0.8687\n",
      "F1-score: 0.8572\n",
      "mAP@0.5: 0.9083\n",
      "mAP@0.5:0.95: 0.6073\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 7\n",
    "\n",
    "model = YOLO('yolo11n.pt')\n",
    "\n",
    "results = model.train(\n",
    "    data='bccd_yolo/data.yaml',\n",
    "    epochs=EPOCHS,\n",
    "    imgsz=640,\n",
    "    device='cpu',\n",
    "    batch=BATCH_SIZE,\n",
    "    optimizer='SGD',\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "metrics = model.val()\n",
    "precision = metrics.box.p.mean()\n",
    "recall = metrics.box.r.mean()\n",
    "f1_score = 2 * (precision * recall) / (precision + recall + 1e-9)\n",
    "map50 = metrics.box.map50.mean()\n",
    "map50_95 = metrics.box.map.mean()\n",
    "\n",
    "print(f\"–ú–µ—Ç—Ä–∏–∫–∏ (yolo11n):\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-score: {f1_score:.4f}\")\n",
    "print(f\"mAP@0.5: {map50:.4f}\")\n",
    "print(f\"mAP@0.5:0.95: {map50_95:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–Ω–∞—è –º–æ–¥–µ–ª—å rtdetr-l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ò—Å–ø–æ–ª—å–∑—É–µ–º —É–ª—É—á—à–µ–Ω–∏–µ –∏–∑ 1 –≥–∏–ø–æ—Ç–µ–∑—ã - —É–≤–µ–ª–∏—á–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.107 üöÄ Python-3.12.6 torch-2.6.0 CPU (Apple M1 Pro)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=rtdetr-l.pt, data=bccd_yolo/data.yaml, epochs=4, time=None, patience=100, batch=4, imgsz=640, save=True, save_period=-1, cache=False, device=cpu, workers=8, project=None, name=train16, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train16\n",
      "Overriding model.yaml nc=80 with nc=3\n",
      "WARNING ‚ö†Ô∏è no model scale passed. Assuming scale='l'.\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1     25248  ultralytics.nn.modules.block.HGStem          [3, 32, 48]                   \n",
      "  1                  -1  6    155072  ultralytics.nn.modules.block.HGBlock         [48, 48, 128, 3, 6]           \n",
      "  2                  -1  1      1408  ultralytics.nn.modules.conv.DWConv           [128, 128, 3, 2, 1, False]    \n",
      "  3                  -1  6    839296  ultralytics.nn.modules.block.HGBlock         [128, 96, 512, 3, 6]          \n",
      "  4                  -1  1      5632  ultralytics.nn.modules.conv.DWConv           [512, 512, 3, 2, 1, False]    \n",
      "  5                  -1  6   1695360  ultralytics.nn.modules.block.HGBlock         [512, 192, 1024, 5, 6, True, False]\n",
      "  6                  -1  6   2055808  ultralytics.nn.modules.block.HGBlock         [1024, 192, 1024, 5, 6, True, True]\n",
      "  7                  -1  6   2055808  ultralytics.nn.modules.block.HGBlock         [1024, 192, 1024, 5, 6, True, True]\n",
      "  8                  -1  1     11264  ultralytics.nn.modules.conv.DWConv           [1024, 1024, 3, 2, 1, False]  \n",
      "  9                  -1  6   6708480  ultralytics.nn.modules.block.HGBlock         [1024, 384, 2048, 5, 6, True, False]\n",
      " 10                  -1  1    524800  ultralytics.nn.modules.conv.Conv             [2048, 256, 1, 1, None, 1, 1, False]\n",
      " 11                  -1  1    789760  ultralytics.nn.modules.transformer.AIFI      [256, 1024, 8]                \n",
      " 12                  -1  1     66048  ultralytics.nn.modules.conv.Conv             [256, 256, 1, 1]              \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14                   7  1    262656  ultralytics.nn.modules.conv.Conv             [1024, 256, 1, 1, None, 1, 1, False]\n",
      " 15            [-2, -1]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  3   2232320  ultralytics.nn.modules.block.RepC3           [512, 256, 3]                 \n",
      " 17                  -1  1     66048  ultralytics.nn.modules.conv.Conv             [256, 256, 1, 1]              \n",
      " 18                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 19                   3  1    131584  ultralytics.nn.modules.conv.Conv             [512, 256, 1, 1, None, 1, 1, False]\n",
      " 20            [-2, -1]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  3   2232320  ultralytics.nn.modules.block.RepC3           [512, 256, 3]                 \n",
      " 22                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 23            [-1, 17]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 24                  -1  3   2232320  ultralytics.nn.modules.block.RepC3           [512, 256, 3]                 \n",
      " 25                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 26            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 27                  -1  3   2232320  ultralytics.nn.modules.block.RepC3           [512, 256, 3]                 \n",
      " 28        [21, 24, 27]  1   7308017  ultralytics.nn.modules.head.RTDETRDecoder    [3, [256, 256, 256]]          \n",
      "rt-detr-l summary: 457 layers, 32,812,241 parameters, 32,812,241 gradients, 108.0 GFLOPs\n",
      "\n",
      "Transferred 926/941 items from pretrained weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/daustinov/study/multimedia/bccd_yolo/labels/train.cache... 291 images, 0 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 291/291 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/daustinov/study/multimedia/bccd_yolo/labels/val.cache... 73 images, 0 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 73/73 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs/detect/train16/labels.jpg... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001429, momentum=0.9) with parameter groups 143 weight(decay=0.0), 206 weight(decay=0.0005), 226 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train16\u001b[0m\n",
      "Starting training for 4 epochs...\n",
      "\n",
      "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/4         0G     0.8144      2.808     0.4955        112        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 73/73 [07:27<00:00,  6.14s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:29<00:00,  2.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         73        987       0.86      0.234      0.214      0.137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        2/4         0G     0.4833     0.7467     0.2358         46        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 73/73 [07:14<00:00,  5.96s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:29<00:00,  2.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         73        987      0.669      0.794       0.75      0.492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        3/4         0G     0.4105     0.6784     0.1966         36        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 73/73 [07:11<00:00,  5.91s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:29<00:00,  2.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         73        987      0.713      0.845       0.79      0.531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        4/4         0G       0.38     0.6145     0.1774         42        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 73/73 [07:16<00:00,  5.98s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:29<00:00,  2.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         73        987      0.751      0.835      0.831      0.564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4 epochs completed in 0.520 hours.\n",
      "Optimizer stripped from runs/detect/train16/weights/last.pt, 66.1MB\n",
      "Optimizer stripped from runs/detect/train16/weights/best.pt, 66.1MB\n",
      "\n",
      "Validating runs/detect/train16/weights/best.pt...\n",
      "Ultralytics 8.3.107 üöÄ Python-3.12.6 torch-2.6.0 CPU (Apple M1 Pro)\n",
      "rt-detr-l summary: 302 layers, 31,989,905 parameters, 0 gradients, 103.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:31<00:00,  3.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         73        987      0.751      0.837      0.831      0.565\n",
      "                   RBC         68        827      0.585      0.698      0.681      0.472\n",
      "                   WBC         71         73       0.93          1      0.987      0.783\n",
      "             Platelets         48         87      0.739      0.812      0.825      0.439\n",
      "Speed: 0.6ms preprocess, 426.4ms inference, 0.0ms loss, 0.1ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/train16\u001b[0m\n",
      "Ultralytics 8.3.107 üöÄ Python-3.12.6 torch-2.6.0 CPU (Apple M1 Pro)\n",
      "rt-detr-l summary: 302 layers, 31,989,905 parameters, 0 gradients, 103.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/daustinov/study/multimedia/bccd_yolo/labels/val.cache... 73 images, 0 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 73/73 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 19/19 [00:30<00:00,  1.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         73        987      0.751      0.837      0.831      0.565\n",
      "                   RBC         68        827      0.585      0.698      0.681      0.472\n",
      "                   WBC         71         73       0.93          1      0.987      0.783\n",
      "             Platelets         48         87      0.739      0.812      0.825      0.439\n",
      "Speed: 1.0ms preprocess, 412.6ms inference, 0.0ms loss, 0.1ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/train162\u001b[0m\n",
      "–ú–µ—Ç—Ä–∏–∫–∏ (rtdetr-l):\n",
      "Precision: 0.7514\n",
      "Recall: 0.8365\n",
      "F1-score: 0.7917\n",
      "mAP@0.5: 0.8310\n",
      "mAP@0.5:0.95: 0.5646\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import RTDETR\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 4\n",
    "\n",
    "model = RTDETR('rtdetr-l.pt')\n",
    "\n",
    "results = model.train(\n",
    "    data='bccd_yolo/data.yaml',\n",
    "    epochs=EPOCHS,\n",
    "    imgsz=640,\n",
    "    device='cpu',\n",
    "    batch=BATCH_SIZE,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "metrics = model.val()\n",
    "precision = metrics.box.p.mean()\n",
    "recall = metrics.box.r.mean()\n",
    "f1_score = 2 * (precision * recall) / (precision + recall + 1e-9)\n",
    "map50 = metrics.box.map50.mean()\n",
    "map50_95 = metrics.box.map.mean()\n",
    "\n",
    "print(f\"–ú–µ—Ç—Ä–∏–∫–∏ (rtdetr-l):\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-score: {f1_score:.4f}\")\n",
    "print(f\"mAP@0.5: {map50:.4f}\")\n",
    "print(f\"mAP@0.5:0.95: {map50_95:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –í—ã–≤–æ–¥—ã"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü–æ—Å–ª–µ –ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è –±–∞–∑–æ–≤–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π yolo11n –∏ rtdetr-l –±—ã–ª–æ —Ä–µ—à–µ–Ω–æ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –≥–∏–ø–æ—Ç–µ–∑—ã, –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–µ –Ω–∞ —É–ª—É—á—à–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏ –æ–±—â–µ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π. –í –∫–∞—á–µ—Å—Ç–≤–µ –æ—Å–Ω–æ–≤—ã –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å –¥–≤–∞ –ø–æ–¥—Ö–æ–¥–∞: –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (–≤ —Ç–æ–º —á–∏—Å–ª–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ learning rate, batch size –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —ç–ø–æ—Ö) –∏ –∑–∞–º–µ–Ω–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ –Ω–∞ SGD —Å –º–æ–º–µ–Ω—Ç—É–º–æ–º.\n",
    "\n",
    "–î–ª—è —Å–≤–µ—Ä—Ç–æ—á–Ω–æ–π –º–æ–¥–µ–ª–∏ yolo11n –æ–±–∞ –ø–æ–¥—Ö–æ–¥–∞ –æ–∫–∞–∑–∞–ª–∏—Å—å –ø–æ–ª–µ–∑–Ω—ã–º–∏: –ø–æ–¥–±–æ—Ä –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ø–æ–∑–≤–æ–ª–∏–ª –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å –ø—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è –ø–æ–¥ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞ BCCD, –∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ SGD –¥–∞–ª–æ –ø—Ä–∏—Ä–æ—Å—Ç –≤ –∫–∞—á–µ—Å—Ç–≤–µ –æ–±–æ–±—â–µ–Ω–∏—è. –ü–æ—Å–ª–µ –≤–Ω–µ—Å–µ–Ω–∏—è —ç—Ç–∏—Ö –∏–∑–º–µ–Ω–µ–Ω–∏–π —Ç–æ—á–Ω–æ—Å—Ç—å (Precision) –º–æ–¥–µ–ª–∏ —É–≤–µ–ª–∏—á–∏–ª–∞—Å—å —Å 0.8232 –¥–æ 0.8460, –∞ mAP@0.5 –≤—ã—Ä–æ—Å —Å 0.8955 –¥–æ 0.9083. –¢–∞–∫–∂–µ –Ω–∞–±–ª—é–¥–∞–µ—Ç—Å—è —É–ª—É—á—à–µ–Ω–∏–µ F1-–º–µ—Ä—ã –∏ –¥—Ä—É–≥–∏—Ö –º–µ—Ç—Ä–∏–∫, —á—Ç–æ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ —Ç–æ, —á—Ç–æ –º–æ–¥–µ–ª—å —Å—Ç–∞–ª–∞ –Ω–µ —Ç–æ–ª—å–∫–æ —Ç–æ—á–Ω–µ–µ, –Ω–æ –∏ –±–æ–ª–µ–µ —É—Å—Ç–æ–π—á–∏–≤–æ–π –∫ –æ—à–∏–±–∫–∞–º.\n",
    "\n",
    "–î–ª—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–Ω–æ–π –º–æ–¥–µ–ª–∏ rtdetr-l –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ SGD –Ω–µ –¥–∞–ª–æ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–≥–æ —ç—Ñ—Ñ–µ–∫—Ç–∞, –∑–∞—Ç–æ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —ç–ø–æ—Ö –ø–æ–∑–≤–æ–ª–∏–ª–æ —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ —É–ª—É—á—à–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ. –û—Å–æ–±–µ–Ω–Ω–æ –∑–∞–º–µ—Ç–µ–Ω –ø—Ä–∏—Ä–æ—Å—Ç –ø–æ –º–µ—Ç—Ä–∏–∫–µ mAP@0.5: —Å 0.7582 –¥–æ 0.8310. –≠—Ç–æ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–µ—Ç, —á—Ç–æ –º–æ–¥–µ–ª—å –Ω—É–∂–¥–∞–µ—Ç—Å—è –≤ –±–æ–ª—å—à–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ –∏—Ç–µ—Ä–∞—Ü–∏–π –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –¥–∞–Ω–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ.\n",
    "\n",
    "–¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –≥–∏–ø–æ—Ç–µ–∑—ã –æ –≤–ª–∏—è–Ω–∏–∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏ –≤—ã–±–æ—Ä–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ –ø–æ–¥—Ç–≤–µ—Ä–¥–∏–ª–∏—Å—å —á–∞—Å—Ç–∏—á–Ω–æ. –î–ª—è —Å–≤–µ—Ä—Ç–æ—á–Ω–æ–π –º–æ–¥–µ–ª–∏ –ø–æ–ª–µ–∑–Ω—ã–º–∏ –æ–∫–∞–∑–∞–ª–∏—Å—å –æ–±–∞ –ø–æ–¥—Ö–æ–¥–∞, –∞ –¥–ª—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–Ω–æ–π - —Ç–æ–ª—å–∫–æ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —ç–ø–æ—Ö"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. –ò–º–ø–ª–µ–º–µ–Ω—Ç–∞—Ü–∏—è –∞–ª–≥–æ—Ä–∏—Ç–º–∞ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ò–º–ø–ª–µ–º–µ–Ω—Ç–∞—Ü–∏—è —Å–≤–µ—Ä—Ç–æ—á–Ω–æ–π –º–æ–¥–µ–ª–∏. –û–±—É—á–µ–Ω–∏–µ –∏ –ø–æ–¥—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:15<00:00,  2.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1\n",
      "Precision: 0.0042 | Recall: 0.0155 | F1: 0.0066\n",
      "mAP@0.5: 0.0156 | mAP@0.5:0.95: 0.0031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:14<00:00,  2.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2\n",
      "Precision: 0.0291 | Recall: 0.1082 | F1: 0.0458\n",
      "mAP@0.5: 0.0826 | mAP@0.5:0.95: 0.0237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:15<00:00,  2.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3\n",
      "Precision: 0.0261 | Recall: 0.0970 | F1: 0.0411\n",
      "mAP@0.5: 0.0777 | mAP@0.5:0.95: 0.0209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:13<00:00,  2.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4\n",
      "Precision: 0.0380 | Recall: 0.1407 | F1: 0.0599\n",
      "mAP@0.5: 0.1107 | mAP@0.5:0.95: 0.0307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:14<00:00,  2.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5\n",
      "Precision: 0.0610 | Recall: 0.1589 | F1: 0.0882\n",
      "mAP@0.5: 0.1638 | mAP@0.5:0.95: 0.0507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:14<00:00,  2.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6\n",
      "Precision: 0.0342 | Recall: 0.1212 | F1: 0.0534\n",
      "mAP@0.5: 0.1004 | mAP@0.5:0.95: 0.0276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:13<00:00,  2.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7\n",
      "Precision: 0.0558 | Recall: 0.1953 | F1: 0.0868\n",
      "mAP@0.5: 0.1484 | mAP@0.5:0.95: 0.0466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:15<00:00,  2.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8\n",
      "Precision: 0.0699 | Recall: 0.2032 | F1: 0.1040\n",
      "mAP@0.5: 0.1807 | mAP@0.5:0.95: 0.0588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:14<00:00,  2.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9\n",
      "Precision: 0.0224 | Recall: 0.0701 | F1: 0.0339\n",
      "mAP@0.5: 0.0724 | mAP@0.5:0.95: 0.0173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:15<00:00,  2.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10\n",
      "Precision: 0.0790 | Recall: 0.2073 | F1: 0.1144\n",
      "mAP@0.5: 0.2062 | mAP@0.5:0.95: 0.0663\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'precision': [0.004195408215685282,\n",
       "  0.029055258466283287,\n",
       "  0.0260869565210739,\n",
       "  0.038013268781525826,\n",
       "  0.061018676955582385,\n",
       "  0.034217989190587564,\n",
       "  0.05581899471465061,\n",
       "  0.0698849375771738,\n",
       "  0.022357170910832436,\n",
       "  0.07899013977393758],\n",
       " 'recall': [0.015451701581623688,\n",
       "  0.10816191107136582,\n",
       "  0.09697601667485296,\n",
       "  0.14067684139343284,\n",
       "  0.1588776187165724,\n",
       "  0.12124371977237237,\n",
       "  0.19527917336285153,\n",
       "  0.20324201344172507,\n",
       "  0.07014882926626706,\n",
       "  0.20731822919638654],\n",
       " 'f1': [0.006598720836824717,\n",
       "  0.0458054832384776,\n",
       "  0.0411137242669093,\n",
       "  0.059852855332184714,\n",
       "  0.08817299969338657,\n",
       "  0.053372494539331954,\n",
       "  0.08682057554613612,\n",
       "  0.10400660473926575,\n",
       "  0.033907258090701414,\n",
       "  0.1143944116170727],\n",
       " 'map50': [0.015571913927580418,\n",
       "  0.08263305320971526,\n",
       "  0.07769985973664799,\n",
       "  0.11073541841212518,\n",
       "  0.16379655583424177,\n",
       "  0.10035314889635769,\n",
       "  0.14843517136386958,\n",
       "  0.1807099318428272,\n",
       "  0.07244931869849629,\n",
       "  0.20619785454783512],\n",
       " 'map5095': [0.003057757643748822,\n",
       "  0.023697478986116016,\n",
       "  0.02092566619437849,\n",
       "  0.030741053810875097,\n",
       "  0.05074088904983939,\n",
       "  0.02760447321289381,\n",
       "  0.04655737703751547,\n",
       "  0.058802438132030496,\n",
       "  0.017347956127043703,\n",
       "  0.06626936827335739]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.ops import box_iou\n",
    "from PIL import Image\n",
    "import glob\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "class YOLOLike(nn.Module):\n",
    "    def __init__(self, grid_size=7, num_boxes=2, num_classes=3):\n",
    "        super().__init__()\n",
    "        self.grid_size = grid_size\n",
    "        self.num_boxes = num_boxes\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 3, 1, 1), nn.LeakyReLU(0.1), nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(16, 32, 3, 1, 1), nn.LeakyReLU(0.1), nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(32, 64, 3, 1, 1), nn.LeakyReLU(0.1), nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(64, 128, 3, 1, 1), nn.LeakyReLU(0.1), nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(128, 256, 3, 1, 1), nn.LeakyReLU(0.1),\n",
    "            nn.Conv2d(256, 512, 3, 1, 1), nn.LeakyReLU(0.1),\n",
    "            nn.AdaptiveAvgPool2d((grid_size, grid_size))\n",
    "        )\n",
    "        \n",
    "        self.detection = nn.Sequential(\n",
    "            nn.Conv2d(512, (5 * num_boxes + num_classes), 1, 1, 0),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.detection(x)\n",
    "        return x.permute(0, 2, 3, 1)\n",
    "\n",
    "class YOLODataset(Dataset):\n",
    "    def __init__(self, img_dir, label_dir, img_size=416, grid_size=7):\n",
    "        self.img_paths = sorted(glob.glob(f\"{img_dir}/*.jpg\"))\n",
    "        self.label_paths = [os.path.join(label_dir, os.path.basename(p).replace('.jpg', '.txt')) \n",
    "                          for p in self.img_paths]\n",
    "        self.img_size = img_size\n",
    "        self.grid_size = grid_size\n",
    "        self.transform = T.Compose([\n",
    "            T.Resize((img_size, img_size)),\n",
    "            T.ToTensor()\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.img_paths[idx]).convert('RGB')\n",
    "        orig_w, orig_h = image.size\n",
    "        image = self.transform(image)\n",
    "        \n",
    "        label_tensor = torch.zeros((self.grid_size, self.grid_size, 5 + 3))\n",
    "        \n",
    "        try:\n",
    "            with open(self.label_paths[idx], 'r') as f:\n",
    "                for line in f:\n",
    "                    cls, x, y, w, h = map(float, line.strip().split())\n",
    "                    \n",
    "                    grid_x = int(x * self.grid_size)\n",
    "                    grid_y = int(y * self.grid_size)\n",
    "                    \n",
    "                    cell_x = x * self.grid_size - grid_x\n",
    "                    cell_y = y * self.grid_size - grid_y\n",
    "                    \n",
    "                    label_tensor[grid_y, grid_x, 0] = 1.0\n",
    "                    label_tensor[grid_y, grid_x, 1:5] = torch.tensor([cell_x, cell_y, w, h])\n",
    "                    label_tensor[grid_y, grid_x, 5 + int(cls)] = 1.0 \n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "        return image, label_tensor\n",
    "\n",
    "def compute_metrics(detections, annotations, iou_thresholds):\n",
    "    metrics = {\n",
    "        'tp': defaultdict(int),\n",
    "        'fp': defaultdict(int),\n",
    "        'fn': defaultdict(int)\n",
    "    }\n",
    "    \n",
    "    for iou_thresh in iou_thresholds:\n",
    "        for img_dets, img_annots in zip(detections, annotations):\n",
    "            for cls_id in range(3):\n",
    "                gt_boxes = [b[:4] for b in img_annots.get(cls_id, [])]\n",
    "                det_boxes = [b[:5] for b in img_dets.get(cls_id, [])]\n",
    "                \n",
    "                matched_gt = np.zeros(len(gt_boxes), dtype=bool)\n",
    "                for det in det_boxes:\n",
    "                    if len(gt_boxes) == 0:\n",
    "                        metrics['fp'][iou_thresh] += 1\n",
    "                        continue\n",
    "                    \n",
    "                    ious = box_iou(torch.tensor([det[:4]]), torch.tensor(gt_boxes))\n",
    "                    max_iou, max_idx = torch.max(ious, dim=1)\n",
    "                    max_iou = max_iou.item()\n",
    "                    max_idx = max_idx.item()\n",
    "\n",
    "                    if max_iou >= iou_thresh and not matched_gt[max_idx]:\n",
    "                        metrics['tp'][iou_thresh] += 1\n",
    "                        matched_gt[max_idx] = True\n",
    "                    else:\n",
    "                        metrics['fp'][iou_thresh] += 1\n",
    "                \n",
    "                metrics['fn'][iou_thresh] += np.sum(~matched_gt)\n",
    "    \n",
    "    results = {}\n",
    "    iou_5095 = np.arange(0.5, 1.0, 0.05)\n",
    "    \n",
    "    aps = []\n",
    "    for thresh in iou_5095:\n",
    "        tp = metrics['tp'][thresh]\n",
    "        fp = metrics['fp'][thresh]\n",
    "        fn = metrics['fn'][thresh]\n",
    "        ap = tp / (tp + fp + 1e-6)\n",
    "        aps.append(ap)\n",
    "    results['map5095'] = np.mean(aps)\n",
    "    \n",
    "    results['map50'] = metrics['tp'][0.5] / (metrics['tp'][0.5] + metrics['fp'][0.5] + 1e-6)\n",
    "    \n",
    "    total_tp = sum(metrics['tp'].values())\n",
    "    total_fp = sum(metrics['fp'].values())\n",
    "    total_fn = sum(metrics['fn'].values())\n",
    "    \n",
    "    results['precision'] = total_tp / (total_tp + total_fp + 1e-6)\n",
    "    results['recall'] = total_tp / (total_tp + total_fn + 1e-6)\n",
    "    results['f1'] = 2 * (results['precision'] * results['recall']) / (results['precision'] + results['recall'] + 1e-6)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def compute_map(model, dataloader, grid_size=7, num_classes=3):\n",
    "    model.eval()\n",
    "    all_detections = []\n",
    "    all_annotations = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, targets in dataloader:\n",
    "            annotations = convert_targets_to_annotations(targets, grid_size)\n",
    "            all_annotations.extend(annotations)\n",
    "            \n",
    "            preds = model(images)\n",
    "            detections = process_predictions(preds, grid_size, num_classes)\n",
    "            all_detections.extend(detections)\n",
    "    \n",
    "    return compute_metrics(all_detections, all_annotations, iou_thresholds=[0.5] + list(np.arange(0.5, 1.0, 0.05)))\n",
    "\n",
    "def convert_targets_to_annotations(targets, grid_size):\n",
    "    annotations = []\n",
    "    for batch_idx in range(targets.size(0)):\n",
    "        image_annotations = defaultdict(list)\n",
    "        for i in range(grid_size):\n",
    "            for j in range(grid_size):\n",
    "                if targets[batch_idx, i, j, 0] == 1:\n",
    "                    x_center = (j + targets[batch_idx, i, j, 1]) / grid_size\n",
    "                    y_center = (i + targets[batch_idx, i, j, 2]) / grid_size\n",
    "                    width = targets[batch_idx, i, j, 3]\n",
    "                    height = targets[batch_idx, i, j, 4]\n",
    "                    class_id = torch.argmax(targets[batch_idx, i, j, 5:])\n",
    "                    \n",
    "                    box = [\n",
    "                        x_center - width/2,\n",
    "                        y_center - height/2,\n",
    "                        x_center + width/2,\n",
    "                        y_center + height/2,\n",
    "                        1.0,\n",
    "                        class_id.item()\n",
    "                    ]\n",
    "                    image_annotations[class_id.item()].append(box)\n",
    "        annotations.append(image_annotations)\n",
    "    return annotations\n",
    "\n",
    "def process_predictions(preds, grid_size, num_classes, confidence_threshold=0.5):\n",
    "    detections = []\n",
    "    for batch_idx in range(preds.size(0)):\n",
    "        image_detections = defaultdict(list)\n",
    "        \n",
    "        for i in range(grid_size):\n",
    "            for j in range(grid_size):\n",
    "                confidence = preds[batch_idx, i, j, 0]\n",
    "                if confidence < confidence_threshold:\n",
    "                    continue\n",
    "                \n",
    "                x_center = (j + preds[batch_idx, i, j, 1]) / grid_size\n",
    "                y_center = (i + preds[batch_idx, i, j, 2]) / grid_size\n",
    "                width = preds[batch_idx, i, j, 3]\n",
    "                height = preds[batch_idx, i, j, 4]\n",
    "                \n",
    "                class_probs = preds[batch_idx, i, j, 5:5+num_classes]\n",
    "                class_id = torch.argmax(class_probs)\n",
    "                class_confidence = class_probs[class_id]\n",
    "                \n",
    "                box = [\n",
    "                    x_center - width/2,\n",
    "                    y_center - height/2,\n",
    "                    x_center + width/2,\n",
    "                    y_center + height/2,\n",
    "                    confidence * class_confidence,\n",
    "                    class_id.item()\n",
    "                ]\n",
    "                image_detections[class_id.item()].append(box)\n",
    "        \n",
    "        for class_id, boxes in image_detections.items():\n",
    "            boxes = sorted(boxes, key=lambda x: x[4], reverse=True)\n",
    "            filtered_boxes = []\n",
    "            while boxes:\n",
    "                best = boxes.pop(0)\n",
    "                filtered_boxes.append(best)\n",
    "                boxes = [box for box in boxes if \n",
    "                        box_iou(torch.tensor([best[:4]]), \n",
    "                                 torch.tensor([box[:4]]))[0][0] < 0.5]\n",
    "            image_detections[class_id] = filtered_boxes\n",
    "        \n",
    "        detections.append(image_detections)\n",
    "    return detections\n",
    "\n",
    "def compute_ap(detections, annotations, iou_threshold):\n",
    "    aps = {}\n",
    "    for class_id in range(3):\n",
    "        class_detections = []\n",
    "        class_annotations = []\n",
    "        \n",
    "        for img_idx, (img_dets, img_annots) in enumerate(zip(detections, annotations)):\n",
    "            gt_boxes = [box[:4] for box in img_annots.get(class_id, [])]\n",
    "            det_boxes = [box[:5] for box in img_dets.get(class_id, [])]\n",
    "            \n",
    "            tp = np.zeros(len(det_boxes))\n",
    "            fp = np.zeros(len(det_boxes))\n",
    "            \n",
    "            if len(gt_boxes) == 0:\n",
    "                fp = np.ones(len(det_boxes))\n",
    "            else:\n",
    "                gt_tensor = torch.tensor(gt_boxes).view(-1, 4)\n",
    "                \n",
    "                for i, det in enumerate(det_boxes):\n",
    "                    det_tensor = torch.tensor([det[:4]]).view(-1, 4)\n",
    "                    \n",
    "                    ious = box_iou(det_tensor, gt_tensor)\n",
    "                    max_iou = torch.max(ious).item() if gt_tensor.size(0) > 0 else 0.0\n",
    "                    \n",
    "                    if max_iou >= iou_threshold:\n",
    "                        tp[i] = 1\n",
    "                        gt_tensor = gt_tensor[torch.argmax(ious) != torch.arange(gt_tensor.size(0))]\n",
    "                    else:\n",
    "                        fp[i] = 1\n",
    "\n",
    "            scores = np.array([det[4] for det in det_boxes])\n",
    "            sort_idx = np.argsort(-scores)\n",
    "            tp = tp[sort_idx]\n",
    "            fp = fp[sort_idx]\n",
    "            \n",
    "            class_detections.extend(zip(tp, fp))\n",
    "            class_annotations.extend([1]*len(img_annots.get(class_id, [])))\n",
    "        \n",
    "        tp_fp = np.array(class_detections)\n",
    "        if tp_fp.size == 0:\n",
    "            ap = 0\n",
    "        else:\n",
    "            tp_cumsum = np.cumsum(tp_fp[:, 0])\n",
    "            fp_cumsum = np.cumsum(tp_fp[:, 1])\n",
    "            \n",
    "            recalls = tp_cumsum / (len(class_annotations) + 1e-6)\n",
    "            precisions = tp_cumsum / (tp_cumsum + fp_cumsum + 1e-6)\n",
    "            \n",
    "            ap = 0\n",
    "            for t in np.linspace(0, 1, 11):\n",
    "                mask = recalls >= t\n",
    "                if np.any(mask):\n",
    "                    ap += np.max(precisions[mask]) / 11\n",
    "        \n",
    "        aps[class_id] = ap\n",
    "    return aps\n",
    "\n",
    "def compute_iou(box1, box2):\n",
    "    box1 = torch.tensor([box1[0] - box1[2]/2, box1[1] - box1[3]/2,\n",
    "                         box1[0] + box1[2]/2, box1[1] + box1[3]/2])\n",
    "    box2 = torch.tensor([box2[0] - box2[2]/2, box2[1] - box2[3]/2,\n",
    "                         box2[0] + box2[2]/2, box2[1] + box2[3]/2])\n",
    "    return box_iou(box1.unsqueeze(0), box2.unsqueeze(0)).item()\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images = torch.stack([item[0] for item in batch])\n",
    "    labels = torch.stack([item[1] for item in batch])\n",
    "    return images, labels\n",
    "\n",
    "def train(model, train_dataloader, val_dataloader, optimizer, epochs=10, grid_size=7, num_classes=3):\n",
    "    best_map = 0.0\n",
    "    history = {'precision': [], 'recall': [], 'f1': [], 'map50': [], 'map5095': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        total_objects = 0\n",
    "        correct_boxes = 0\n",
    "        \n",
    "        for imgs, targets in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(imgs)\n",
    "            \n",
    "            batch_size = preds.size(0)\n",
    "            preds = preds.view(batch_size, grid_size * grid_size, -1)\n",
    "            targets = targets.view(batch_size, grid_size * grid_size, -1)\n",
    "            \n",
    "            pred_obj = preds[..., 0]\n",
    "            pred_box = preds[..., 1:5]\n",
    "            pred_cls = preds[..., 5:5+num_classes]\n",
    "            \n",
    "            target_obj = targets[..., 0]\n",
    "            target_box = targets[..., 1:5]\n",
    "            target_cls = targets[..., 5:5+num_classes]\n",
    "            \n",
    "            obj_mask = target_obj == 1\n",
    "            no_obj_mask = target_obj == 0\n",
    "            \n",
    "            obj_loss = nn.BCELoss()(pred_obj[obj_mask], target_obj[obj_mask])\n",
    "            no_obj_loss = nn.BCELoss()(pred_obj[no_obj_mask], target_obj[no_obj_mask])\n",
    "            coord_loss = nn.MSELoss()(pred_box[obj_mask], target_box[obj_mask])\n",
    "            class_loss = nn.BCELoss()(pred_cls[obj_mask], target_cls[obj_mask])\n",
    "            \n",
    "            total_loss = 5*coord_loss + obj_loss + 0.5*no_obj_loss + class_loss\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += total_loss.item()\n",
    "            total_objects += obj_mask.sum().item()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for i in range(batch_size):\n",
    "                    for j in range(grid_size * grid_size):\n",
    "                        if obj_mask[i, j]:\n",
    "                            pred_b = pred_box[i, j]\n",
    "                            true_b = target_box[i, j]\n",
    "                            if compute_iou(pred_b, true_b) > 0.5:\n",
    "                                correct_boxes += 1\n",
    "        \n",
    "        model.eval()\n",
    "        metrics = compute_map(model, val_dataloader, grid_size, num_classes)\n",
    "                \n",
    "        for key in history.keys():\n",
    "            history[key].append(metrics[key])\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch+1}\")\n",
    "        print(f\"Precision: {metrics['precision']:.4f} | Recall: {metrics['recall']:.4f} | F1: {metrics['f1']:.4f}\")\n",
    "        print(f\"mAP@0.5: {metrics['map50']:.4f} | mAP@0.5:0.95: {metrics['map5095']:.4f}\")\n",
    "        \n",
    "        if metrics['map5095'] > best_map:\n",
    "            best_map = metrics['map5095']\n",
    "    \n",
    "    return history\n",
    "\n",
    "grid_size = 7\n",
    "num_classes = 3\n",
    "img_size = 416\n",
    "batch_size = 8\n",
    "lr = 0.001\n",
    "epochs = 10\n",
    "\n",
    "train_ds = YOLODataset('bccd_yolo/images/train', 'bccd_yolo/labels/train', img_size, grid_size)\n",
    "val_ds = YOLODataset('bccd_yolo/images/val', 'bccd_yolo/labels/val', img_size, grid_size)\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_dl = DataLoader(val_ds, batch_size=batch_size, collate_fn=collate_fn)\n",
    "\n",
    "model = YOLOLike(grid_size=grid_size, num_classes=num_classes)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "\n",
    "train(model, train_dl, val_dl, optimizer, epochs, grid_size, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ò–º–ø–ª–µ–º–µ–Ω—Ç–∞—Ü–∏—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–Ω–æ–π –º–æ–¥–µ–ª–∏ –º–æ–¥–µ–ª–∏. –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏ –ø–æ–¥—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:21<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.0942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:17<00:00,  2.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 0.7241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:16<00:00,  2.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 0.6747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:17<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 0.6466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:20<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 0.6138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:18<00:00,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Loss: 0.5966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:18<00:00,  2.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Loss: 0.5924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:19<00:00,  1.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Loss: 0.5412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:19<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Loss: 0.5099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:17<00:00,  2.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 0.4865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:19<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Loss: 0.4419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:19<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Loss: 0.4302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:18<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Loss: 0.4230\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:18<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Loss: 0.4223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:17<00:00,  2.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Loss: 0.4188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:19<00:00,  1.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Loss: 0.3849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:17<00:00,  2.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, Loss: 0.3896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:20<00:00,  1.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, Loss: 0.3985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:21<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, Loss: 0.4073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:20<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, Loss: 0.3709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 19/19 [00:01<00:00, 10.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision@0.5: 0.0892\n",
      "Recall@0.5: 0.1317\n",
      "F1@0.5: 0.1064\n",
      "mAP@0.5: 0.0892, mAP@0.5:0.95: 0.0232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torchvision.models import resnet18\n",
    "from torchvision.ops import box_iou\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "CLASSES = ['RBC', 'WBC', 'Platelets']\n",
    "NUM_CLASSES = len(CLASSES)\n",
    "IMG_SIZE = 256\n",
    "MAX_DETECTIONS = 20\n",
    "\n",
    "class YOLODetectionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, img_dir, label_dir, transform=None):\n",
    "        self.img_dir = Path(img_dir)\n",
    "        self.label_dir = Path(label_dir)\n",
    "        self.transform = transform or T.Compose([\n",
    "            T.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        self.images = sorted(self.img_dir.glob('*.jpg'))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        label_path = self.label_dir / f\"{img_path.stem}.txt\"\n",
    "\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        orig_w, orig_h = img.size\n",
    "        img_tensor = self.transform(img)\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        if label_path.exists():\n",
    "            with open(label_path) as f:\n",
    "                for line in f:\n",
    "                    cls, cx, cy, bw, bh = map(float, line.strip().split())\n",
    "                    \n",
    "                    scale_x = IMG_SIZE / orig_w\n",
    "                    scale_y = IMG_SIZE / orig_h\n",
    "                    \n",
    "                    x1 = (cx - bw/2) * scale_x\n",
    "                    y1 = (cy - bh/2) * scale_y\n",
    "                    x2 = (cx + bw/2) * scale_x\n",
    "                    y2 = (cy + bh/2) * scale_y\n",
    "                    \n",
    "                    boxes.append([x1, y1, x2, y2])\n",
    "                    labels.append(int(cls))\n",
    "\n",
    "        return {\n",
    "            'image': img_tensor,\n",
    "            'boxes': torch.tensor(boxes, dtype=torch.float32) if boxes else torch.zeros((0, 4)),\n",
    "            'labels': torch.tensor(labels, dtype=torch.long) if labels else torch.zeros(0, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'image': torch.stack([x['image'] for x in batch]),\n",
    "        'boxes': [x['boxes'] for x in batch],\n",
    "        'labels': [x['labels'] for x in batch]\n",
    "    }\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for _ in range(num_layers-1):\n",
    "            layers.extend([\n",
    "                nn.Linear(input_dim, hidden_dim),\n",
    "                nn.ReLU()\n",
    "            ])\n",
    "            input_dim = hidden_dim\n",
    "        layers.append(nn.Linear(hidden_dim, output_dim))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class DETRLike(nn.Module):\n",
    "    def __init__(self, num_classes, num_queries=MAX_DETECTIONS, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.backbone = nn.Sequential(*list(resnet18(pretrained=False).children())[:-2])\n",
    "        self.conv = nn.Conv2d(512, hidden_dim, 1)\n",
    "        \n",
    "        self.encoder_pos = nn.Parameter(torch.randn(1, hidden_dim, IMG_SIZE//32, IMG_SIZE//32))\n",
    "        self.decoder_pos = nn.Parameter(torch.randn(num_queries, hidden_dim))\n",
    "        \n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=hidden_dim,\n",
    "            nhead=8,\n",
    "            num_encoder_layers=3,\n",
    "            num_decoder_layers=3,\n",
    "            dim_feedforward=2048,\n",
    "            dropout=0.1\n",
    "        )\n",
    "        \n",
    "        self.class_embed = nn.Linear(hidden_dim, num_classes + 1)\n",
    "        self.bbox_embed = MLP(hidden_dim, hidden_dim, 4, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        src = self.conv(features)\n",
    "        \n",
    "        bs, c, h, w = src.shape\n",
    "        src = src.flatten(2).permute(2, 0, 1)\n",
    "        pos_enc = self.encoder_pos.expand(bs, -1, h, w).flatten(2).permute(2, 0, 1)\n",
    "        src = src + pos_enc\n",
    "        \n",
    "        query_embed = self.decoder_pos.unsqueeze(1).repeat(1, bs, 1)\n",
    "        \n",
    "        hs = self.transformer(\n",
    "            src=src,\n",
    "            tgt=query_embed,\n",
    "            src_key_padding_mask=None,\n",
    "            tgt_key_padding_mask=None,\n",
    "            memory_key_padding_mask=None\n",
    "        )\n",
    "        \n",
    "        outputs_class = self.class_embed(hs)\n",
    "        outputs_coord = self.bbox_embed(hs).sigmoid()\n",
    "        \n",
    "        return outputs_class.permute(1, 0, 2), outputs_coord.permute(1, 0, 2)\n",
    "\n",
    "class HungarianMatcher(nn.Module):\n",
    "    def __init__(self, cost_class=1, cost_bbox=5, cost_giou=2):\n",
    "        super().__init__()\n",
    "        self.cost_class = cost_class\n",
    "        self.cost_bbox = cost_bbox\n",
    "        self.cost_giou = cost_giou\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, outputs, targets):\n",
    "        bs, num_queries = outputs[\"pred_logits\"].shape[:2]\n",
    "        \n",
    "        indices = []\n",
    "        for i in range(bs):\n",
    "            out_prob = outputs[\"pred_logits\"][i].softmax(-1)\n",
    "            out_bbox = outputs[\"pred_boxes\"][i]\n",
    "\n",
    "            tgt_bbox = targets[i][\"boxes\"]\n",
    "            tgt_ids = targets[i][\"labels\"]\n",
    "            \n",
    "            cost_class = -out_prob[:, tgt_ids]\n",
    "            \n",
    "            cost_bbox = torch.cdist(out_bbox, tgt_bbox, p=1)\n",
    "            \n",
    "            C = self.cost_bbox * cost_bbox + self.cost_class * cost_class\n",
    "            C = C.reshape(num_queries, -1).cpu()\n",
    "            \n",
    "            indices.append(linear_sum_assignment(C))\n",
    "        \n",
    "        return [(torch.as_tensor(i, dtype=torch.int64), torch.as_tensor(j, dtype=torch.int64)) for i, j in indices]\n",
    "\n",
    "def loss_fn(outputs, targets):\n",
    "    pred_logits, pred_boxes = outputs\n",
    "    \n",
    "    matcher = HungarianMatcher()\n",
    "    indices = matcher({\"pred_logits\": pred_logits, \"pred_boxes\": pred_boxes}, targets)\n",
    "    \n",
    "    src_logits = torch.cat([pred_logits[i][idx] for i, (idx, _) in enumerate(indices)])\n",
    "    target_classes = torch.cat([t[\"labels\"][j] for t, (_, j) in zip(targets, indices)])\n",
    "    loss_cls = F.cross_entropy(src_logits, target_classes)\n",
    "    \n",
    "    src_boxes = torch.cat([pred_boxes[i][idx] for i, (idx, _) in enumerate(indices)])\n",
    "    target_boxes = torch.cat([t[\"boxes\"][j] for t, (_, j) in zip(targets, indices)])\n",
    "    loss_bbox = F.l1_loss(src_boxes, target_boxes)\n",
    "    \n",
    "    return loss_cls + 5 * loss_bbox\n",
    "\n",
    "def train(model, dataloader, optimizer, epochs=50):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(dataloader, desc=f\"Epoch {epoch+1}\"):\n",
    "            images = batch['image'].to(device)\n",
    "            targets = [\n",
    "                {\"boxes\": b.to(device), \"labels\": l.to(device)}\n",
    "                for b, l in zip(batch['boxes'], batch['labels'])\n",
    "            ]\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss / len(dataloader):.4f}\")\n",
    "\n",
    "def evaluate(model, dataloader, conf_thresh=0.5):\n",
    "    model.eval()\n",
    "    results = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            images = batch['image'].to(device)\n",
    "            outputs = model(images)\n",
    "            pred_logits, pred_boxes = outputs\n",
    "            \n",
    "            probs = F.softmax(pred_logits, dim=-1)\n",
    "            scores, labels = torch.max(probs, dim=-1)\n",
    "            \n",
    "            for i in range(images.size(0)):\n",
    "                keep = scores[i] > conf_thresh\n",
    "                pred = {\n",
    "                    'boxes': pred_boxes[i][keep].cpu(),\n",
    "                    'scores': scores[i][keep].cpu(),\n",
    "                    'labels': labels[i][keep].cpu()\n",
    "                }\n",
    "                results.append((pred, batch['boxes'][i], batch['labels'][i]))\n",
    "    \n",
    "    aps = []\n",
    "    tp_total = fp_total = fn_total = 0\n",
    "    \n",
    "    for iou_threshold in np.linspace(0.5, 0.95, 10):\n",
    "        tp = fp = fn = 0\n",
    "        \n",
    "        for pred, gt_boxes, gt_labels in results:\n",
    "            pred_boxes = pred['boxes']\n",
    "            pred_labels = pred['labels']\n",
    "            \n",
    "            if len(pred_boxes) == 0:\n",
    "                fn += len(gt_labels)\n",
    "                continue\n",
    "                \n",
    "            ious = box_iou(pred_boxes, gt_boxes)\n",
    "            \n",
    "            matches = (ious > iou_threshold) & (pred_labels.unsqueeze(1) == gt_labels)\n",
    "            matched_gt = set()\n",
    "            matched_pred = set()\n",
    "            \n",
    "            for pred_idx, gt_idx in zip(*torch.where(matches)):\n",
    "                if gt_idx not in matched_gt and pred_idx not in matched_pred:\n",
    "                    matched_gt.add(gt_idx.item())\n",
    "                    matched_pred.add(pred_idx.item())\n",
    "            \n",
    "            cur_tp = len(matched_gt)\n",
    "            cur_fp = len(pred_boxes) - len(matched_pred)\n",
    "            cur_fn = len(gt_labels) - len(matched_gt)\n",
    "            \n",
    "            if iou_threshold == 0.5:\n",
    "                tp_total += cur_tp\n",
    "                fp_total += cur_fp\n",
    "                fn_total += cur_fn\n",
    "            \n",
    "            tp += cur_tp\n",
    "            fp += cur_fp\n",
    "            fn += cur_fn\n",
    "        \n",
    "        precision = tp / (tp + fp + 1e-6)\n",
    "        recall = tp / (tp + fn + 1e-6)\n",
    "        aps.append(precision)\n",
    "    \n",
    "    precision = tp_total / (tp_total + fp_total + 1e-6)\n",
    "    recall = tp_total / (tp_total + fn_total + 1e-6)\n",
    "    f1 = 2 * precision * recall / (precision + recall + 1e-6)\n",
    "    \n",
    "    map50 = np.mean(aps[:1])\n",
    "    map5095 = np.mean(aps)\n",
    "    \n",
    "    print(f\"Precision@0.5: {precision:.4f}\")\n",
    "    print(f\"Recall@0.5: {recall:.4f}\")\n",
    "    print(f\"F1@0.5: {f1:.4f}\")\n",
    "    print(f\"mAP@0.5: {map50:.4f}, mAP@0.5:0.95: {map5095:.4f}\")\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, nn.Conv2d):\n",
    "        nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "\n",
    "model = DETRLike(NUM_CLASSES).to(device)\n",
    "model.apply(init_weights)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "\n",
    "train_dataset = YOLODetectionDataset(\"bccd_yolo/images/train\", \"bccd_yolo/labels/train\")\n",
    "val_dataset = YOLODetectionDataset(\"bccd_yolo/images/val\", \"bccd_yolo/labels/val\")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=4, collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "train(model, train_loader, optimizer, epochs=20)\n",
    "evaluate(model, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å –ø.2. –í—ã–≤–æ–¥—ã"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í –ø—Ä–æ—Ü–µ—Å—Å–µ —Ä–∞–±–æ—Ç—ã –±—ã–ª–∏ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω—ã –¥–≤–µ –º–æ–¥–µ–ª–∏ –¥–µ—Ç–µ–∫—Ü–∏–∏ –æ–±—ä–µ–∫—Ç–æ–≤: —Å–≤–µ—Ä—Ç–æ—á–Ω–∞—è –º–æ–¥–µ–ª—å, –ø–æ—Å—Ç—Ä–æ–µ–Ω–Ω–∞—è –ø–æ –º–æ—Ç–∏–≤–∞–º yolo, –∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–Ω–∞—è –º–æ–¥–µ–ª—å, —É–ø—Ä–æ—â—ë–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è rtdetr. –û–±–µ –º–æ–¥–µ–ª–∏ –±—ã–ª–∏ –æ–±—É—á–µ–Ω—ã –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ BCCD —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º yolo-—Ñ–æ—Ä–º–∞—Ç–∞ —Ä–∞–∑–º–µ—Ç–∫–∏ –∏ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω—ã –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –ü–æ–ª—É—á–µ–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –±—ã–ª–∏ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω—ã —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∞–Ω–∞–ª–æ–≥–∏—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∏–∑ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ ultralytics, —Ç–∞–∫–∏—Ö –∫–∞–∫ yolo11n –∏ rtdetr-l.\n",
    "\n",
    "–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –æ—â—É—Ç–∏–º—É—é —Ä–∞–∑–Ω–∏—Ü—É –≤ –∫–∞—á–µ—Å—Ç–≤–µ –¥–µ—Ç–µ–∫—Ü–∏–∏ –º–µ–∂–¥—É –≥–æ—Ç–æ–≤—ã–º–∏ –∏ —Å–∞–º–æ–ø–∏—Å–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏. –ù–∞–ø—Ä–∏–º–µ—Ä, —Å–≤–µ—Ä—Ç–æ—á–Ω–∞—è –º–æ–¥–µ–ª—å –∏–∑ ultralytics –ø–æ–∫–∞–∑–∞–ª–∞ –≤—ã—Å–æ–∫–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫: —Ç–æ—á–Ω–æ—Å—Ç—å (Precision) —Å–æ—Å—Ç–∞–≤–∏–ª–∞ 0.8232, –ø–æ–ª–Ω–æ—Ç–∞ (Recall) - 0.8832, F1-–º–µ—Ä–∞ - 0.8521, –∞ –∑–Ω–∞—á–µ–Ω–∏–µ mAP@0.5 –¥–æ—Å—Ç–∏–≥–ª–æ 0.8955. –í —Ç–æ –≤—Ä–µ–º—è –∫–∞–∫ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–∞—è —Å–≤–µ—Ä—Ç–æ—á–Ω–∞—è –º–æ–¥–µ–ª—å –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É—Å—Ç—É–ø–∞–µ—Ç –ø–æ –≤—Å–µ–º –ø–æ–∫–∞–∑–∞—Ç–µ–ª—è–º, –≤—ã–¥–∞–≤ Precision 0.0790, Recall 0.2073, F1-score 0.1144 –∏ mAP@0.5 —Ä–∞–≤–Ω—ã–π 0.2062. –ê–Ω–∞–ª–æ–≥–∏—á–Ω–∞—è —Å–∏—Ç—É–∞—Ü–∏—è –Ω–∞–±–ª—é–¥–∞–µ—Ç—Å—è –∏ —Å —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–Ω–æ–π –º–æ–¥–µ–ª—å—é: rtdetr-l –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –≤—ã—Å–æ–∫–∏–π —É—Ä–æ–≤–µ–Ω—å Recall (0.8813) –∏ mAP@0.5 (0.7582), –≤ —Ç–æ –≤—Ä–µ–º—è –∫–∞–∫ –ø—Ä–æ—Å—Ç–∞—è —Å–∞–º–æ–ø–∏—Å–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–∞ –±–∞–∑–µ resnet18 –∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ –ø–æ–∫–∞–∑–∞–ª–∞ –≤—Å–µ–≥–æ 0.0892 –ø–æ Precision –∏ 0.0892 –ø–æ mAP@0.5.\n",
    "\n",
    "–¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –º–æ–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å –≤—ã–≤–æ–¥, —á—Ç–æ –Ω–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—É—é —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä –∏ —É—Å–ø–µ—à–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π, —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–∞–º–æ–ø–∏—Å–Ω—ã—Ö –≤–µ—Ä—Å–∏–π —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ –æ—Ç—Å—Ç–∞—é—Ç –æ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω—ã—Ö –∏ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ä–µ—à–µ–Ω–∏–π, –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –≤ –±–∏–±–ª–∏–æ—Ç–µ–∫–µ ultralytics. –≠—Ç–æ —Å–≤—è–∑–∞–Ω–æ, –≤ –ø–µ—Ä–≤—É—é –æ—á–µ—Ä–µ–¥—å, —Å —É–ø—Ä–æ—â–µ–Ω–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π, –∞ —Ç–∞–∫–∂–µ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–π –≥–ª—É–±–∏–Ω–æ–π –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–µ–π."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –£–ª—É—á—à–µ–Ω–∏–µ –±–µ–π–∑–ª–∞–π–Ω–∞. –î–æ–±–∞–≤–ª–µ–Ω–∏–π —Ç–µ—Ö–Ω–∏–∫ –¥–ª—è –∫–∞–∂–¥–æ–π –∏–∑ –º–æ–¥–µ–ª–µ–π –∏–∑ –ø—É–Ω–∫—Ç–∞ 3c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### –°–≤–µ—Ä—Ç–æ—á–Ω–∞—è –º–æ–¥–µ–ª—å"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ò—Å–ø–æ–ª—å–∑—É–µ–º —É–ª—É—á—à–µ–Ω–∏—è –∏–∑ –æ–±–æ–∏—Ö –≥–∏–ø–æ—Ç–µ–∑ - –æ–±–µ –≥–∏–ø–æ—Ç–µ–∑—ã –ø–æ–∫–∞–∑–∞–ª–∏ –ª—É—á—à–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –∏–∑–Ω–∞—á–∞–ª—å–Ω—ã–º –≤–∞—Ä–∏–∞–Ω—Ç–æ–º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:15<00:00,  2.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1\n",
      "Precision: 0.0204 | Recall: 0.0759 | F1: 0.0321\n",
      "mAP@0.5: 0.0654 | mAP@0.5:0.95: 0.0159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:13<00:00,  2.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2\n",
      "Precision: 0.0188 | Recall: 0.0700 | F1: 0.0296\n",
      "mAP@0.5: 0.0604 | mAP@0.5:0.95: 0.0146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:14<00:00,  2.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3\n",
      "Precision: 0.0441 | Recall: 0.1480 | F1: 0.0679\n",
      "mAP@0.5: 0.1267 | mAP@0.5:0.95: 0.0358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:13<00:00,  2.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4\n",
      "Precision: 0.0629 | Recall: 0.1941 | F1: 0.0950\n",
      "mAP@0.5: 0.1685 | mAP@0.5:0.95: 0.0523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:14<00:00,  2.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5\n",
      "Precision: 0.0455 | Recall: 0.1680 | F1: 0.0716\n",
      "mAP@0.5: 0.1238 | mAP@0.5:0.95: 0.0377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:14<00:00,  2.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6\n",
      "Precision: 0.0463 | Recall: 0.1685 | F1: 0.0726\n",
      "mAP@0.5: 0.1286 | mAP@0.5:0.95: 0.0380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:13<00:00,  2.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7\n",
      "Precision: 0.0490 | Recall: 0.1400 | F1: 0.0725\n",
      "mAP@0.5: 0.1458 | mAP@0.5:0.95: 0.0393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:14<00:00,  2.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8\n",
      "Precision: 0.0364 | Recall: 0.1142 | F1: 0.0552\n",
      "mAP@0.5: 0.1047 | mAP@0.5:0.95: 0.0296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:13<00:00,  2.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9\n",
      "Precision: 0.0373 | Recall: 0.1025 | F1: 0.0546\n",
      "mAP@0.5: 0.1058 | mAP@0.5:0.95: 0.0304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:15<00:00,  2.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10\n",
      "Precision: 0.0387 | Recall: 0.1121 | F1: 0.0575\n",
      "mAP@0.5: 0.1082 | mAP@0.5:0.95: 0.0317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:16<00:00,  2.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11\n",
      "Precision: 0.0954 | Recall: 0.2054 | F1: 0.1303\n",
      "mAP@0.5: 0.2437 | mAP@0.5:0.95: 0.0806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:15<00:00,  2.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12\n",
      "Precision: 0.1076 | Recall: 0.1965 | F1: 0.1391\n",
      "mAP@0.5: 0.2678 | mAP@0.5:0.95: 0.0916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:16<00:00,  2.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13\n",
      "Precision: 0.1174 | Recall: 0.2595 | F1: 0.1617\n",
      "mAP@0.5: 0.2836 | mAP@0.5:0.95: 0.1008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:16<00:00,  2.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14\n",
      "Precision: 0.1244 | Recall: 0.2758 | F1: 0.1715\n",
      "mAP@0.5: 0.2819 | mAP@0.5:0.95: 0.1087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:14<00:00,  2.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15\n",
      "Precision: 0.1396 | Recall: 0.2581 | F1: 0.1812\n",
      "mAP@0.5: 0.3215 | mAP@0.5:0.95: 0.1214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:14<00:00,  2.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16\n",
      "Precision: 0.1235 | Recall: 0.2584 | F1: 0.1672\n",
      "mAP@0.5: 0.2941 | mAP@0.5:0.95: 0.1065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:14<00:00,  2.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17\n",
      "Precision: 0.1306 | Recall: 0.2725 | F1: 0.1766\n",
      "mAP@0.5: 0.3048 | mAP@0.5:0.95: 0.1132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:15<00:00,  2.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18\n",
      "Precision: 0.1487 | Recall: 0.2489 | F1: 0.1862\n",
      "mAP@0.5: 0.3421 | mAP@0.5:0.95: 0.1294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:15<00:00,  2.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 19\n",
      "Precision: 0.1240 | Recall: 0.2768 | F1: 0.1713\n",
      "mAP@0.5: 0.2938 | mAP@0.5:0.95: 0.1070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:16<00:00,  2.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20\n",
      "Precision: 0.1414 | Recall: 0.2160 | F1: 0.1709\n",
      "mAP@0.5: 0.3406 | mAP@0.5:0.95: 0.1215\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'precision': [0.020357333468362077,\n",
       "  0.018756194880962813,\n",
       "  0.04408483718704045,\n",
       "  0.06287802032289834,\n",
       "  0.045518764929859006,\n",
       "  0.04626158492017438,\n",
       "  0.048951048949426604,\n",
       "  0.03641803674938292,\n",
       "  0.03725273967753626,\n",
       "  0.03865760407690159,\n",
       "  0.09544573642990462,\n",
       "  0.10762681064806465,\n",
       "  0.11742245484073008,\n",
       "  0.12444919785563853,\n",
       "  0.1396195457037574,\n",
       "  0.12353847547704438,\n",
       "  0.1306165099209206,\n",
       "  0.14873973377803795,\n",
       "  0.12398624261712937,\n",
       "  0.14142103629280664],\n",
       " 'recall': [0.07593136789497285,\n",
       "  0.06995923783581769,\n",
       "  0.1479761114657336,\n",
       "  0.19414162478015531,\n",
       "  0.1679780073781422,\n",
       "  0.1684519859542656,\n",
       "  0.14001327138686004,\n",
       "  0.11422883684574567,\n",
       "  0.10247416815788471,\n",
       "  0.1121433311108026,\n",
       "  0.20542231489189283,\n",
       "  0.19651151766077243,\n",
       "  0.2594558725699634,\n",
       "  0.27576073558860925,\n",
       "  0.25812873255681784,\n",
       "  0.25841311970249187,\n",
       "  0.27253768127096994,\n",
       "  0.24893354818002336,\n",
       "  0.2768034884560808,\n",
       "  0.21603943499705758],\n",
       " 'f1': [0.03210644866607587,\n",
       "  0.029581196141220182,\n",
       "  0.06793123601314656,\n",
       "  0.09499035397372503,\n",
       "  0.0716274586715697,\n",
       "  0.07258804860292745,\n",
       "  0.07254025753540785,\n",
       "  0.05522799464488625,\n",
       "  0.054641103115265204,\n",
       "  0.05749518384562166,\n",
       "  0.13033367199354748,\n",
       "  0.13908038805138134,\n",
       "  0.16167480285703945,\n",
       "  0.17150057370366184,\n",
       "  0.18121876439488616,\n",
       "  0.16716191095026867,\n",
       "  0.17659661354880452,\n",
       "  0.18621425304351893,\n",
       "  0.17126056974962528,\n",
       "  0.17094161598535687],\n",
       " 'map50': [0.06541794799197401,\n",
       "  0.06038579814643754,\n",
       "  0.12674743707257727,\n",
       "  0.1685241472190942,\n",
       "  0.12376377506021986,\n",
       "  0.12857961051996283,\n",
       "  0.14582573821621841,\n",
       "  0.10472074466344401,\n",
       "  0.10576194084424527,\n",
       "  0.10819554275553638,\n",
       "  0.24370155032856064,\n",
       "  0.26784694452659996,\n",
       "  0.28362435104208955,\n",
       "  0.2818823528748512,\n",
       "  0.32148900160138494,\n",
       "  0.29411764698551407,\n",
       "  0.3048475761357203,\n",
       "  0.34205607465979565,\n",
       "  0.2937879494876721,\n",
       "  0.34061433435473915],\n",
       " 'map5095': [0.01585127201213856,\n",
       "  0.014593234550859955,\n",
       "  0.035818577188729044,\n",
       "  0.05231340761773483,\n",
       "  0.03769426390745802,\n",
       "  0.03802978235062978,\n",
       "  0.039263580010217895,\n",
       "  0.029587765949351152,\n",
       "  0.030401819550752947,\n",
       "  0.03170381019903162,\n",
       "  0.08062015500560317,\n",
       "  0.091604797214042,\n",
       "  0.10080226517856487,\n",
       "  0.108705882308418,\n",
       "  0.12143260005337948,\n",
       "  0.10648055827927486,\n",
       "  0.11319340324939979,\n",
       "  0.12940809961850136,\n",
       "  0.10700607188588666,\n",
       "  0.1215017064133304]}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.ops import box_iou\n",
    "from PIL import Image\n",
    "import glob\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "class YOLOLike(nn.Module):\n",
    "    def __init__(self, grid_size=7, num_boxes=2, num_classes=3):\n",
    "        super().__init__()\n",
    "        self.grid_size = grid_size\n",
    "        self.num_boxes = num_boxes\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 3, 1, 1), nn.LeakyReLU(0.1), nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(16, 32, 3, 1, 1), nn.LeakyReLU(0.1), nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(32, 64, 3, 1, 1), nn.LeakyReLU(0.1), nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(64, 128, 3, 1, 1), nn.LeakyReLU(0.1), nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(128, 256, 3, 1, 1), nn.LeakyReLU(0.1),\n",
    "            nn.Conv2d(256, 512, 3, 1, 1), nn.LeakyReLU(0.1),\n",
    "            nn.AdaptiveAvgPool2d((grid_size, grid_size))\n",
    "        )\n",
    "        \n",
    "        self.detection = nn.Sequential(\n",
    "            nn.Conv2d(512, (5 * num_boxes + num_classes), 1, 1, 0),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.detection(x)\n",
    "        return x.permute(0, 2, 3, 1)\n",
    "\n",
    "class YOLODataset(Dataset):\n",
    "    def __init__(self, img_dir, label_dir, img_size=416, grid_size=7):\n",
    "        self.img_paths = sorted(glob.glob(f\"{img_dir}/*.jpg\"))\n",
    "        self.label_paths = [os.path.join(label_dir, os.path.basename(p).replace('.jpg', '.txt')) \n",
    "                          for p in self.img_paths]\n",
    "        self.img_size = img_size\n",
    "        self.grid_size = grid_size\n",
    "        self.transform = T.Compose([\n",
    "            T.Resize((img_size, img_size)),\n",
    "            T.ToTensor()\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.img_paths[idx]).convert('RGB')\n",
    "        orig_w, orig_h = image.size\n",
    "        image = self.transform(image)\n",
    "        \n",
    "        label_tensor = torch.zeros((self.grid_size, self.grid_size, 5 + 3))\n",
    "        \n",
    "        try:\n",
    "            with open(self.label_paths[idx], 'r') as f:\n",
    "                for line in f:\n",
    "                    cls, x, y, w, h = map(float, line.strip().split())\n",
    "                    \n",
    "                    grid_x = int(x * self.grid_size)\n",
    "                    grid_y = int(y * self.grid_size)\n",
    "                    \n",
    "                    cell_x = x * self.grid_size - grid_x\n",
    "                    cell_y = y * self.grid_size - grid_y\n",
    "                    \n",
    "                    label_tensor[grid_y, grid_x, 0] = 1.0\n",
    "                    label_tensor[grid_y, grid_x, 1:5] = torch.tensor([cell_x, cell_y, w, h])\n",
    "                    label_tensor[grid_y, grid_x, 5 + int(cls)] = 1.0 \n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "        return image, label_tensor\n",
    "\n",
    "def compute_metrics(detections, annotations, iou_thresholds):\n",
    "    metrics = {\n",
    "        'tp': defaultdict(int),\n",
    "        'fp': defaultdict(int),\n",
    "        'fn': defaultdict(int)\n",
    "    }\n",
    "    \n",
    "    for iou_thresh in iou_thresholds:\n",
    "        for img_dets, img_annots in zip(detections, annotations):\n",
    "            for cls_id in range(3):\n",
    "                gt_boxes = [b[:4] for b in img_annots.get(cls_id, [])]\n",
    "                det_boxes = [b[:5] for b in img_dets.get(cls_id, [])]\n",
    "                \n",
    "                matched_gt = np.zeros(len(gt_boxes), dtype=bool)\n",
    "                for det in det_boxes:\n",
    "                    if len(gt_boxes) == 0:\n",
    "                        metrics['fp'][iou_thresh] += 1\n",
    "                        continue\n",
    "                    \n",
    "                    ious = box_iou(torch.tensor([det[:4]]), torch.tensor(gt_boxes))\n",
    "                    max_iou, max_idx = torch.max(ious, dim=1)\n",
    "                    max_iou = max_iou.item()\n",
    "                    max_idx = max_idx.item()\n",
    "\n",
    "                    if max_iou >= iou_thresh and not matched_gt[max_idx]:\n",
    "                        metrics['tp'][iou_thresh] += 1\n",
    "                        matched_gt[max_idx] = True\n",
    "                    else:\n",
    "                        metrics['fp'][iou_thresh] += 1\n",
    "                \n",
    "                metrics['fn'][iou_thresh] += np.sum(~matched_gt)\n",
    "    \n",
    "    results = {}\n",
    "    iou_5095 = np.arange(0.5, 1.0, 0.05)\n",
    "    \n",
    "    aps = []\n",
    "    for thresh in iou_5095:\n",
    "        tp = metrics['tp'][thresh]\n",
    "        fp = metrics['fp'][thresh]\n",
    "        fn = metrics['fn'][thresh]\n",
    "        ap = tp / (tp + fp + 1e-6)\n",
    "        aps.append(ap)\n",
    "    results['map5095'] = np.mean(aps)\n",
    "    \n",
    "    results['map50'] = metrics['tp'][0.5] / (metrics['tp'][0.5] + metrics['fp'][0.5] + 1e-6)\n",
    "    \n",
    "    total_tp = sum(metrics['tp'].values())\n",
    "    total_fp = sum(metrics['fp'].values())\n",
    "    total_fn = sum(metrics['fn'].values())\n",
    "    \n",
    "    results['precision'] = total_tp / (total_tp + total_fp + 1e-6)\n",
    "    results['recall'] = total_tp / (total_tp + total_fn + 1e-6)\n",
    "    results['f1'] = 2 * (results['precision'] * results['recall']) / (results['precision'] + results['recall'] + 1e-6)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def compute_map(model, dataloader, grid_size=7, num_classes=3):\n",
    "    model.eval()\n",
    "    all_detections = []\n",
    "    all_annotations = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, targets in dataloader:\n",
    "            annotations = convert_targets_to_annotations(targets, grid_size)\n",
    "            all_annotations.extend(annotations)\n",
    "            \n",
    "            preds = model(images)\n",
    "            detections = process_predictions(preds, grid_size, num_classes)\n",
    "            all_detections.extend(detections)\n",
    "    \n",
    "    return compute_metrics(all_detections, all_annotations, iou_thresholds=[0.5] + list(np.arange(0.5, 1.0, 0.05)))\n",
    "\n",
    "def convert_targets_to_annotations(targets, grid_size):\n",
    "    annotations = []\n",
    "    for batch_idx in range(targets.size(0)):\n",
    "        image_annotations = defaultdict(list)\n",
    "        for i in range(grid_size):\n",
    "            for j in range(grid_size):\n",
    "                if targets[batch_idx, i, j, 0] == 1:\n",
    "                    x_center = (j + targets[batch_idx, i, j, 1]) / grid_size\n",
    "                    y_center = (i + targets[batch_idx, i, j, 2]) / grid_size\n",
    "                    width = targets[batch_idx, i, j, 3]\n",
    "                    height = targets[batch_idx, i, j, 4]\n",
    "                    class_id = torch.argmax(targets[batch_idx, i, j, 5:])\n",
    "                    \n",
    "                    box = [\n",
    "                        x_center - width/2,\n",
    "                        y_center - height/2,\n",
    "                        x_center + width/2,\n",
    "                        y_center + height/2,\n",
    "                        1.0,\n",
    "                        class_id.item()\n",
    "                    ]\n",
    "                    image_annotations[class_id.item()].append(box)\n",
    "        annotations.append(image_annotations)\n",
    "    return annotations\n",
    "\n",
    "def process_predictions(preds, grid_size, num_classes, confidence_threshold=0.5):\n",
    "    detections = []\n",
    "    for batch_idx in range(preds.size(0)):\n",
    "        image_detections = defaultdict(list)\n",
    "        \n",
    "        for i in range(grid_size):\n",
    "            for j in range(grid_size):\n",
    "                confidence = preds[batch_idx, i, j, 0]\n",
    "                if confidence < confidence_threshold:\n",
    "                    continue\n",
    "                \n",
    "                x_center = (j + preds[batch_idx, i, j, 1]) / grid_size\n",
    "                y_center = (i + preds[batch_idx, i, j, 2]) / grid_size\n",
    "                width = preds[batch_idx, i, j, 3]\n",
    "                height = preds[batch_idx, i, j, 4]\n",
    "                \n",
    "                class_probs = preds[batch_idx, i, j, 5:5+num_classes]\n",
    "                class_id = torch.argmax(class_probs)\n",
    "                class_confidence = class_probs[class_id]\n",
    "                \n",
    "                box = [\n",
    "                    x_center - width/2,\n",
    "                    y_center - height/2,\n",
    "                    x_center + width/2,\n",
    "                    y_center + height/2,\n",
    "                    confidence * class_confidence,\n",
    "                    class_id.item()\n",
    "                ]\n",
    "                image_detections[class_id.item()].append(box)\n",
    "        \n",
    "        for class_id, boxes in image_detections.items():\n",
    "            boxes = sorted(boxes, key=lambda x: x[4], reverse=True)\n",
    "            filtered_boxes = []\n",
    "            while boxes:\n",
    "                best = boxes.pop(0)\n",
    "                filtered_boxes.append(best)\n",
    "                boxes = [box for box in boxes if \n",
    "                        box_iou(torch.tensor([best[:4]]), \n",
    "                                 torch.tensor([box[:4]]))[0][0] < 0.5]\n",
    "            image_detections[class_id] = filtered_boxes\n",
    "        \n",
    "        detections.append(image_detections)\n",
    "    return detections\n",
    "\n",
    "def compute_ap(detections, annotations, iou_threshold):\n",
    "    aps = {}\n",
    "    for class_id in range(3):\n",
    "        class_detections = []\n",
    "        class_annotations = []\n",
    "        \n",
    "        for img_idx, (img_dets, img_annots) in enumerate(zip(detections, annotations)):\n",
    "            gt_boxes = [box[:4] for box in img_annots.get(class_id, [])]\n",
    "            det_boxes = [box[:5] for box in img_dets.get(class_id, [])]\n",
    "            \n",
    "            tp = np.zeros(len(det_boxes))\n",
    "            fp = np.zeros(len(det_boxes))\n",
    "            \n",
    "            if len(gt_boxes) == 0:\n",
    "                fp = np.ones(len(det_boxes))\n",
    "            else:\n",
    "                gt_tensor = torch.tensor(gt_boxes).view(-1, 4)\n",
    "                \n",
    "                for i, det in enumerate(det_boxes):\n",
    "                    det_tensor = torch.tensor([det[:4]]).view(-1, 4)\n",
    "                    \n",
    "                    ious = box_iou(det_tensor, gt_tensor)\n",
    "                    max_iou = torch.max(ious).item() if gt_tensor.size(0) > 0 else 0.0\n",
    "                    \n",
    "                    if max_iou >= iou_threshold:\n",
    "                        tp[i] = 1\n",
    "                        gt_tensor = gt_tensor[torch.argmax(ious) != torch.arange(gt_tensor.size(0))]\n",
    "                    else:\n",
    "                        fp[i] = 1\n",
    "\n",
    "            scores = np.array([det[4] for det in det_boxes])\n",
    "            sort_idx = np.argsort(-scores)\n",
    "            tp = tp[sort_idx]\n",
    "            fp = fp[sort_idx]\n",
    "            \n",
    "            class_detections.extend(zip(tp, fp))\n",
    "            class_annotations.extend([1]*len(img_annots.get(class_id, [])))\n",
    "        \n",
    "        tp_fp = np.array(class_detections)\n",
    "        if tp_fp.size == 0:\n",
    "            ap = 0\n",
    "        else:\n",
    "            tp_cumsum = np.cumsum(tp_fp[:, 0])\n",
    "            fp_cumsum = np.cumsum(tp_fp[:, 1])\n",
    "            \n",
    "            recalls = tp_cumsum / (len(class_annotations) + 1e-6)\n",
    "            precisions = tp_cumsum / (tp_cumsum + fp_cumsum + 1e-6)\n",
    "            \n",
    "            ap = 0\n",
    "            for t in np.linspace(0, 1, 11):\n",
    "                mask = recalls >= t\n",
    "                if np.any(mask):\n",
    "                    ap += np.max(precisions[mask]) / 11\n",
    "        \n",
    "        aps[class_id] = ap\n",
    "    return aps\n",
    "\n",
    "def compute_iou(box1, box2):\n",
    "    box1 = torch.tensor([box1[0] - box1[2]/2, box1[1] - box1[3]/2,\n",
    "                         box1[0] + box1[2]/2, box1[1] + box1[3]/2])\n",
    "    box2 = torch.tensor([box2[0] - box2[2]/2, box2[1] - box2[3]/2,\n",
    "                         box2[0] + box2[2]/2, box2[1] + box2[3]/2])\n",
    "    return box_iou(box1.unsqueeze(0), box2.unsqueeze(0)).item()\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images = torch.stack([item[0] for item in batch])\n",
    "    labels = torch.stack([item[1] for item in batch])\n",
    "    return images, labels\n",
    "\n",
    "def train(model, train_dataloader, val_dataloader, optimizer, epochs=10, grid_size=7, num_classes=3):\n",
    "    best_map = 0.0\n",
    "    history = {'precision': [], 'recall': [], 'f1': [], 'map50': [], 'map5095': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        total_objects = 0\n",
    "        correct_boxes = 0\n",
    "        \n",
    "        for imgs, targets in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(imgs)\n",
    "            \n",
    "            batch_size = preds.size(0)\n",
    "            preds = preds.view(batch_size, grid_size * grid_size, -1)\n",
    "            targets = targets.view(batch_size, grid_size * grid_size, -1)\n",
    "            \n",
    "            pred_obj = preds[..., 0]\n",
    "            pred_box = preds[..., 1:5]\n",
    "            pred_cls = preds[..., 5:5+num_classes]\n",
    "            \n",
    "            target_obj = targets[..., 0]\n",
    "            target_box = targets[..., 1:5]\n",
    "            target_cls = targets[..., 5:5+num_classes]\n",
    "            \n",
    "            obj_mask = target_obj == 1\n",
    "            no_obj_mask = target_obj == 0\n",
    "            \n",
    "            obj_loss = nn.BCELoss()(pred_obj[obj_mask], target_obj[obj_mask])\n",
    "            no_obj_loss = nn.BCELoss()(pred_obj[no_obj_mask], target_obj[no_obj_mask])\n",
    "            coord_loss = nn.MSELoss()(pred_box[obj_mask], target_box[obj_mask])\n",
    "            class_loss = nn.BCELoss()(pred_cls[obj_mask], target_cls[obj_mask])\n",
    "            \n",
    "            total_loss = 5*coord_loss + obj_loss + 0.5*no_obj_loss + class_loss\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += total_loss.item()\n",
    "            total_objects += obj_mask.sum().item()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for i in range(batch_size):\n",
    "                    for j in range(grid_size * grid_size):\n",
    "                        if obj_mask[i, j]:\n",
    "                            pred_b = pred_box[i, j]\n",
    "                            true_b = target_box[i, j]\n",
    "                            if compute_iou(pred_b, true_b) > 0.5:\n",
    "                                correct_boxes += 1\n",
    "        \n",
    "        model.eval()\n",
    "        metrics = compute_map(model, val_dataloader, grid_size, num_classes)\n",
    "                \n",
    "        for key in history.keys():\n",
    "            history[key].append(metrics[key])\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch+1}\")\n",
    "        print(f\"Precision: {metrics['precision']:.4f} | Recall: {metrics['recall']:.4f} | F1: {metrics['f1']:.4f}\")\n",
    "        print(f\"mAP@0.5: {metrics['map50']:.4f} | mAP@0.5:0.95: {metrics['map5095']:.4f}\")\n",
    "        \n",
    "        if metrics['map5095'] > best_map:\n",
    "            best_map = metrics['map5095']\n",
    "    \n",
    "    return history\n",
    "\n",
    "grid_size = 7\n",
    "num_classes = 3\n",
    "img_size = 416\n",
    "batch_size = 8\n",
    "lr = 0.001\n",
    "epochs = 20\n",
    "\n",
    "train_ds = YOLODataset('bccd_yolo/images/train', 'bccd_yolo/labels/train', img_size, grid_size)\n",
    "val_ds = YOLODataset('bccd_yolo/images/val', 'bccd_yolo/labels/val', img_size, grid_size)\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_dl = DataLoader(val_ds, batch_size=batch_size, collate_fn=collate_fn)\n",
    "\n",
    "model = YOLOLike(grid_size=grid_size, num_classes=num_classes)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "\n",
    "train(model, train_dl, val_dl, optimizer, epochs, grid_size, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–Ω–∞—è –º–æ–¥–µ–ª—å"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ò—Å–ø–æ–ª—å–∑—É–µ–º —É–ª—É—á—à–µ–Ω–∏–µ –∏–∑ 1 –≥–∏–ø–æ—Ç–µ–∑—ã - —É–≤–µ–ª–∏—á–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daustinov/study/multimedia/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/daustinov/study/multimedia/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "/Users/daustinov/study/multimedia/lib/python3.12/site-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n",
      "Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:20<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.0418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:19<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 0.7015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:20<00:00,  1.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 0.6439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:19<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 0.6309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:18<00:00,  1.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 0.5579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:18<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Loss: 0.5485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:18<00:00,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Loss: 0.5133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:19<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Loss: 0.4926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:19<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Loss: 0.4747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:19<00:00,  1.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 0.4699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:19<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Loss: 0.4424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:20<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Loss: 0.4510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:18<00:00,  1.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Loss: 0.4054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:19<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Loss: 0.4107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:19<00:00,  1.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Loss: 0.3830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:19<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Loss: 0.3795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:18<00:00,  2.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, Loss: 0.3672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:19<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, Loss: 0.3592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:19<00:00,  1.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, Loss: 0.3653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:18<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, Loss: 0.3534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:18<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21, Loss: 0.3368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:19<00:00,  1.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22, Loss: 0.3414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:18<00:00,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23, Loss: 0.3520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:19<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24, Loss: 0.3177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:18<00:00,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25, Loss: 0.3405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:19<00:00,  1.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26, Loss: 0.3132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:20<00:00,  1.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27, Loss: 0.2952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:19<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28, Loss: 0.2721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:21<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29, Loss: 0.2689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:20<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30, Loss: 0.2624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:17<00:00,  2.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31, Loss: 0.2661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:19<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32, Loss: 0.2702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:19<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33, Loss: 0.2662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:18<00:00,  2.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34, Loss: 0.2644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:18<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35, Loss: 0.2595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:20<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36, Loss: 0.2310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:20<00:00,  1.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37, Loss: 0.2460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:18<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38, Loss: 0.2423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:19<00:00,  1.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39, Loss: 0.2321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:20<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40, Loss: 0.2215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 19/19 [00:01<00:00, 10.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision@0.5: 0.1272\n",
      "Recall@0.5: 0.1884\n",
      "F1@0.5: 0.1519\n",
      "mAP@0.5: 0.1272, mAP@0.5:0.95: 0.0358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torchvision.models import resnet18\n",
    "from torchvision.ops import box_iou\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "CLASSES = ['RBC', 'WBC', 'Platelets']\n",
    "NUM_CLASSES = len(CLASSES)\n",
    "IMG_SIZE = 256\n",
    "MAX_DETECTIONS = 20\n",
    "\n",
    "class YOLODetectionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, img_dir, label_dir, transform=None):\n",
    "        self.img_dir = Path(img_dir)\n",
    "        self.label_dir = Path(label_dir)\n",
    "        self.transform = transform or T.Compose([\n",
    "            T.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        self.images = sorted(self.img_dir.glob('*.jpg'))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        label_path = self.label_dir / f\"{img_path.stem}.txt\"\n",
    "\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        orig_w, orig_h = img.size\n",
    "        img_tensor = self.transform(img)\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        if label_path.exists():\n",
    "            with open(label_path) as f:\n",
    "                for line in f:\n",
    "                    cls, cx, cy, bw, bh = map(float, line.strip().split())\n",
    "                    \n",
    "                    scale_x = IMG_SIZE / orig_w\n",
    "                    scale_y = IMG_SIZE / orig_h\n",
    "                    \n",
    "                    x1 = (cx - bw/2) * scale_x\n",
    "                    y1 = (cy - bh/2) * scale_y\n",
    "                    x2 = (cx + bw/2) * scale_x\n",
    "                    y2 = (cy + bh/2) * scale_y\n",
    "                    \n",
    "                    boxes.append([x1, y1, x2, y2])\n",
    "                    labels.append(int(cls))\n",
    "\n",
    "        return {\n",
    "            'image': img_tensor,\n",
    "            'boxes': torch.tensor(boxes, dtype=torch.float32) if boxes else torch.zeros((0, 4)),\n",
    "            'labels': torch.tensor(labels, dtype=torch.long) if labels else torch.zeros(0, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'image': torch.stack([x['image'] for x in batch]),\n",
    "        'boxes': [x['boxes'] for x in batch],\n",
    "        'labels': [x['labels'] for x in batch]\n",
    "    }\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for _ in range(num_layers-1):\n",
    "            layers.extend([\n",
    "                nn.Linear(input_dim, hidden_dim),\n",
    "                nn.ReLU()\n",
    "            ])\n",
    "            input_dim = hidden_dim\n",
    "        layers.append(nn.Linear(hidden_dim, output_dim))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class DETRLike(nn.Module):\n",
    "    def __init__(self, num_classes, num_queries=MAX_DETECTIONS, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.backbone = nn.Sequential(*list(resnet18(pretrained=False).children())[:-2])\n",
    "        self.conv = nn.Conv2d(512, hidden_dim, 1)\n",
    "        \n",
    "        self.encoder_pos = nn.Parameter(torch.randn(1, hidden_dim, IMG_SIZE//32, IMG_SIZE//32))\n",
    "        self.decoder_pos = nn.Parameter(torch.randn(num_queries, hidden_dim))\n",
    "        \n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=hidden_dim,\n",
    "            nhead=8,\n",
    "            num_encoder_layers=3,\n",
    "            num_decoder_layers=3,\n",
    "            dim_feedforward=2048,\n",
    "            dropout=0.1\n",
    "        )\n",
    "        \n",
    "        self.class_embed = nn.Linear(hidden_dim, num_classes + 1)\n",
    "        self.bbox_embed = MLP(hidden_dim, hidden_dim, 4, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        src = self.conv(features)\n",
    "        \n",
    "        bs, c, h, w = src.shape\n",
    "        src = src.flatten(2).permute(2, 0, 1)\n",
    "        pos_enc = self.encoder_pos.expand(bs, -1, h, w).flatten(2).permute(2, 0, 1)\n",
    "        src = src + pos_enc\n",
    "        \n",
    "        query_embed = self.decoder_pos.unsqueeze(1).repeat(1, bs, 1)\n",
    "        \n",
    "        hs = self.transformer(\n",
    "            src=src,\n",
    "            tgt=query_embed,\n",
    "            src_key_padding_mask=None,\n",
    "            tgt_key_padding_mask=None,\n",
    "            memory_key_padding_mask=None\n",
    "        )\n",
    "        \n",
    "        outputs_class = self.class_embed(hs)\n",
    "        outputs_coord = self.bbox_embed(hs).sigmoid()\n",
    "        \n",
    "        return outputs_class.permute(1, 0, 2), outputs_coord.permute(1, 0, 2)\n",
    "\n",
    "class HungarianMatcher(nn.Module):\n",
    "    def __init__(self, cost_class=1, cost_bbox=5, cost_giou=2):\n",
    "        super().__init__()\n",
    "        self.cost_class = cost_class\n",
    "        self.cost_bbox = cost_bbox\n",
    "        self.cost_giou = cost_giou\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, outputs, targets):\n",
    "        bs, num_queries = outputs[\"pred_logits\"].shape[:2]\n",
    "        \n",
    "        indices = []\n",
    "        for i in range(bs):\n",
    "            out_prob = outputs[\"pred_logits\"][i].softmax(-1)\n",
    "            out_bbox = outputs[\"pred_boxes\"][i]\n",
    "\n",
    "            tgt_bbox = targets[i][\"boxes\"]\n",
    "            tgt_ids = targets[i][\"labels\"]\n",
    "            \n",
    "            cost_class = -out_prob[:, tgt_ids]\n",
    "            \n",
    "            cost_bbox = torch.cdist(out_bbox, tgt_bbox, p=1)\n",
    "            \n",
    "            C = self.cost_bbox * cost_bbox + self.cost_class * cost_class\n",
    "            C = C.reshape(num_queries, -1).cpu()\n",
    "            \n",
    "            indices.append(linear_sum_assignment(C))\n",
    "        \n",
    "        return [(torch.as_tensor(i, dtype=torch.int64), torch.as_tensor(j, dtype=torch.int64)) for i, j in indices]\n",
    "\n",
    "def loss_fn(outputs, targets):\n",
    "    pred_logits, pred_boxes = outputs\n",
    "    \n",
    "    matcher = HungarianMatcher()\n",
    "    indices = matcher({\"pred_logits\": pred_logits, \"pred_boxes\": pred_boxes}, targets)\n",
    "    \n",
    "    src_logits = torch.cat([pred_logits[i][idx] for i, (idx, _) in enumerate(indices)])\n",
    "    target_classes = torch.cat([t[\"labels\"][j] for t, (_, j) in zip(targets, indices)])\n",
    "    loss_cls = F.cross_entropy(src_logits, target_classes)\n",
    "    \n",
    "    src_boxes = torch.cat([pred_boxes[i][idx] for i, (idx, _) in enumerate(indices)])\n",
    "    target_boxes = torch.cat([t[\"boxes\"][j] for t, (_, j) in zip(targets, indices)])\n",
    "    loss_bbox = F.l1_loss(src_boxes, target_boxes)\n",
    "    \n",
    "    return loss_cls + 5 * loss_bbox\n",
    "\n",
    "def train(model, dataloader, optimizer, epochs=50):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(dataloader, desc=f\"Epoch {epoch+1}\"):\n",
    "            images = batch['image'].to(device)\n",
    "            targets = [\n",
    "                {\"boxes\": b.to(device), \"labels\": l.to(device)}\n",
    "                for b, l in zip(batch['boxes'], batch['labels'])\n",
    "            ]\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss / len(dataloader):.4f}\")\n",
    "\n",
    "def evaluate(model, dataloader, conf_thresh=0.5):\n",
    "    model.eval()\n",
    "    results = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            images = batch['image'].to(device)\n",
    "            outputs = model(images)\n",
    "            pred_logits, pred_boxes = outputs\n",
    "            \n",
    "            probs = F.softmax(pred_logits, dim=-1)\n",
    "            scores, labels = torch.max(probs, dim=-1)\n",
    "            \n",
    "            for i in range(images.size(0)):\n",
    "                keep = scores[i] > conf_thresh\n",
    "                pred = {\n",
    "                    'boxes': pred_boxes[i][keep].cpu(),\n",
    "                    'scores': scores[i][keep].cpu(),\n",
    "                    'labels': labels[i][keep].cpu()\n",
    "                }\n",
    "                results.append((pred, batch['boxes'][i], batch['labels'][i]))\n",
    "    \n",
    "    aps = []\n",
    "    tp_total = fp_total = fn_total = 0\n",
    "    \n",
    "    for iou_threshold in np.linspace(0.5, 0.95, 10):\n",
    "        tp = fp = fn = 0\n",
    "        \n",
    "        for pred, gt_boxes, gt_labels in results:\n",
    "            pred_boxes = pred['boxes']\n",
    "            pred_labels = pred['labels']\n",
    "            \n",
    "            if len(pred_boxes) == 0:\n",
    "                fn += len(gt_labels)\n",
    "                continue\n",
    "                \n",
    "            ious = box_iou(pred_boxes, gt_boxes)\n",
    "            \n",
    "            matches = (ious > iou_threshold) & (pred_labels.unsqueeze(1) == gt_labels)\n",
    "            matched_gt = set()\n",
    "            matched_pred = set()\n",
    "            \n",
    "            for pred_idx, gt_idx in zip(*torch.where(matches)):\n",
    "                if gt_idx not in matched_gt and pred_idx not in matched_pred:\n",
    "                    matched_gt.add(gt_idx.item())\n",
    "                    matched_pred.add(pred_idx.item())\n",
    "            \n",
    "            cur_tp = len(matched_gt)\n",
    "            cur_fp = len(pred_boxes) - len(matched_pred)\n",
    "            cur_fn = len(gt_labels) - len(matched_gt)\n",
    "            \n",
    "            if iou_threshold == 0.5:\n",
    "                tp_total += cur_tp\n",
    "                fp_total += cur_fp\n",
    "                fn_total += cur_fn\n",
    "            \n",
    "            tp += cur_tp\n",
    "            fp += cur_fp\n",
    "            fn += cur_fn\n",
    "        \n",
    "        precision = tp / (tp + fp + 1e-6)\n",
    "        recall = tp / (tp + fn + 1e-6)\n",
    "        aps.append(precision)\n",
    "    \n",
    "    precision = tp_total / (tp_total + fp_total + 1e-6)\n",
    "    recall = tp_total / (tp_total + fn_total + 1e-6)\n",
    "    f1 = 2 * precision * recall / (precision + recall + 1e-6)\n",
    "    \n",
    "    map50 = np.mean(aps[:1])\n",
    "    map5095 = np.mean(aps)\n",
    "    \n",
    "    print(f\"Precision@0.5: {precision:.4f}\")\n",
    "    print(f\"Recall@0.5: {recall:.4f}\")\n",
    "    print(f\"F1@0.5: {f1:.4f}\")\n",
    "    print(f\"mAP@0.5: {map50:.4f}, mAP@0.5:0.95: {map5095:.4f}\")\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, nn.Conv2d):\n",
    "        nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "\n",
    "model = DETRLike(NUM_CLASSES).to(device)\n",
    "model.apply(init_weights)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "\n",
    "train_dataset = YOLODetectionDataset(\"bccd_yolo/images/train\", \"bccd_yolo/labels/train\")\n",
    "val_dataset = YOLODetectionDataset(\"bccd_yolo/images/val\", \"bccd_yolo/labels/val\")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=4, collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "train(model, train_loader, optimizer, epochs=40)\n",
    "evaluate(model, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –í—ã–≤–æ–¥—ã"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü–æ—Å–ª–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è —É–ª—É—á—à–µ–Ω–∏–π, –∞–Ω–∞–ª–æ–≥–∏—á–Ω—ã—Ö —Ç–µ–º, —á—Ç–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –±–µ–π–∑–ª–∞–π–Ω–∞ –º–æ–¥–µ–ª–µ–π –∏–∑ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ ultralytics, —É–¥–∞–ª–æ—Å—å –¥–æ–±–∏—Ç—å—Å—è –∑–∞–º–µ—Ç–Ω–æ–≥–æ –ø—Ä–∏—Ä–æ—Å—Ç–∞ –∫–∞—á–µ—Å—Ç–≤–∞ —É —Å–∞–º–æ–ø–∏—Å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–µ—Ç–µ–∫—Ü–∏–∏ –æ–±—ä–µ–∫—Ç–æ–≤. –£–ª—É—á—à–µ–Ω–∏—è –≤–∫–ª—é—á–∞–ª–∏ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ —á–∏—Å–ª–∞ —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è, –∞ —Ç–∞–∫–∂–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –º–æ–¥–µ–ª–∏ –Ω–∞ —É—Ä–æ–≤–Ω–µ –¥–∞–Ω–Ω—ã—Ö –∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã, –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ –º–æ–¥–µ–ª–∏.\n",
    "\n",
    "–°–≤–µ—Ä—Ç–æ—á–Ω–∞—è –º–æ–¥–µ–ª—å, –ø–æ—Å—Ç—Ä–æ–µ–Ω–Ω–∞—è –ø–æ –∞–Ω–∞–ª–æ–≥–∏–∏ —Å yolo, –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞–ª–∞ –Ω–∞–∏–ª—É—á—à–∏–π –ø—Ä–∏—Ä–æ—Å—Ç –ø–æ—Å–ª–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –æ–±–µ–∏—Ö –≥–∏–ø–æ—Ç–µ–∑ —É–ª—É—á—à–µ–Ω–∏—è. –ü–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –∏–∑–Ω–∞—á–∞–ª—å–Ω—ã–º–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ (Precision: 0.0790, Recall: 0.2073, F1: 0.1144, mAP@0.5: 0.2062, mAP@0.5:0.95: 0.0663), —É–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –º–æ–¥–µ–ª–∏ –ø–æ–∫–∞–∑–∞–ª–∞ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π —Ä–æ—Å—Ç –ø–æ –≤—Å–µ–º –∫–ª—é—á–µ–≤—ã–º –º–µ—Ç—Ä–∏–∫–∞–º: Precision —É–≤–µ–ª–∏—á–∏–ª—Å—è –¥–æ 0.1414, Recall - –¥–æ 0.2160, F1 - –¥–æ 0.1709, –∞ –∑–Ω–∞—á–µ–Ω–∏–µ mAP@0.5 –¥–æ—Å—Ç–∏–≥–ª–æ 0.3406, —á—Ç–æ –ø–æ—á—Ç–∏ –≤ –¥–≤–∞ —Ä–∞–∑–∞ –ø—Ä–µ–≤—ã—à–∞–µ—Ç –∏—Å—Ö–æ–¥–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç. –¢–∞–∫–∂–µ –∑–∞–º–µ—Ç–Ω–æ –≤—ã—Ä–æ—Å–ª–∞ –∏ —Å–ª–æ–∂–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞ mAP@0.5:0.95 - —Å 0.0663 –¥–æ 0.1215.\n",
    "\n",
    "–ê–Ω–∞–ª–æ–≥–∏—á–Ω–∞—è –¥–∏–Ω–∞–º–∏–∫–∞ –Ω–∞–±–ª—é–¥–∞–µ—Ç—Å—è –∏ –≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–Ω–æ–π –º–æ–¥–µ–ª–∏. –ò–∑–Ω–∞—á–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫ (Precision: 0.0892, Recall: 0.1317, F1: 0.1064, mAP@0.5: 0.0892, mAP@0.5:0.95: 0.0232) –≤—ã—Ä–æ—Å–ª–∏ –¥–æ Precision: 0.1272, Recall: 0.1884, F1: 0.1519, mAP@0.5: 0.1272 –∏ mAP@0.5:0.95: 0.0358 –ø–æ—Å–ª–µ —É–≤–µ–ª–∏—á–µ–Ω–∏—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —ç–ø–æ—Ö. –•–æ—Ç—è –∞–±—Å–æ–ª—é—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –æ—Å—Ç–∞—é—Ç—Å—è –Ω–∏–∂–µ, —á–µ–º —É —Å–≤–µ—Ä—Ç–æ—á–Ω–æ–π –º–æ–¥–µ–ª–∏, –ø—Ä–∏—Ä–æ—Å—Ç —É —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–Ω–æ–π –º–æ–¥–µ–ª–∏ —Ç–∞–∫–∂–µ –∑–∞–º–µ—Ç–µ–Ω –∏ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –≥–∏–ø–æ—Ç–µ–∑—ã.\n",
    "\n",
    "–¢–µ–º –Ω–µ –º–µ–Ω–µ–µ, –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Ñ–∏–Ω–∞–ª—å–Ω—ã–º–∏ —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ –∏–∑ ultralytics (–≥–¥–µ yolo11n –¥–æ—Å—Ç–∏–≥–∞–ª mAP@0.5: 0.9083, –∞ rtdetr-l - mAP@0.5: 0.8310), —Å–∞–º–æ–ø–∏—Å–Ω—ã–µ –≤–µ—Ä—Å–∏–∏ –≤—Å—ë –µ—â—ë —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ –æ—Ç—Å—Ç–∞—é—Ç. –û–¥–Ω–∞–∫–æ –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –ø—Ä–∏–º–µ–Ω—è–µ–º—ã–µ —Ç–µ—Ö–Ω–∏–∫–∏ —É–ª—É—á—à–µ–Ω–∏—è –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ —Ä–∞–±–æ—Ç–∞—é—Ç, –∞ –º–æ–¥–µ–ª–∏ —Ä–µ–∞–≥–∏—Ä—É—é—Ç –Ω–∞ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º–æ.\n",
    "\n",
    "–¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –ø—É—Å—Ç—å –∞–±—Å–æ–ª—é—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ–∫–∞ –æ—Å—Ç–∞—é—Ç—Å—è –Ω–∏–∑–∫–∏–º–∏, –∑–∞–º–µ—Ç–Ω—ã–π —Ä–æ—Å—Ç –º–µ—Ç—Ä–∏–∫ –ø–æ—Å–ª–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è —É–ª—É—á—à–µ–Ω–∏–π –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–µ—Ç —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multimedia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
