{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лабораторная работа 7, студент Устинов Денис Александрович М8О-406Б-21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Выбор начальных условий"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Набор данных для задачи сегментации\n",
    "В качестве датасета был выбран CamVid (https://github.com/alexgkendall/SegNet-Tutorial).\n",
    "\n",
    "CamVid - это небольшой, но хорошо структурированный датасет для семантической сегментации дорожных сцен. Он содержит видеозаписи и соответствующие им изображения с детальной разметкой, где каждый пиксель отнесен к одному из 32 классов, включая дороги, здания, автомобили, пешеходов и другие элементы городской среды. Данные были собраны в реальных условиях движения по городу, что обеспечивает разнообразие сцен с различным освещением, погодными условиями и углами обзора.\n",
    "\n",
    "Обоснование выбора:\n",
    "\n",
    "С практической точки зрения, задачи, которые можно решать с помощью CamVid, имеют прямое отношение к разработке систем автономного вождения и интеллектуального анализа городской инфраструктуры. Например, точная сегментация дорожного полотна, тротуаров и препятствий критически важна для навигации беспилотных автомобилей. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Выбор метрик качества"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) IoU (Intersection over Union) — отношение площади пересечения предсказанной и истинной маски к их объединению (основная метрика в сегментации).\n",
    "2) Pixel Accuracy — доля правильно классифицированных пикселей (простая, но чувствительна к дисбалансу классов)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Создание бейзлайна и оценка качества"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Обучить сверточную модель (resnet34) из segmentation_models_pytorch для выбранного набора данных и оценить качество моделей по выбранным метрикам на выбранном наборе данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daustinov/study/multimedia/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IoU (Jaccard Index): 0.3292\n",
      "Pixel Accuracy: 0.7772\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import wget\n",
    "import zipfile\n",
    "import segmentation_models_pytorch as smp\n",
    "from torchmetrics import JaccardIndex, Accuracy\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 0.001\n",
    "IMAGE_SIZE = (128, 128)\n",
    "\n",
    "if not os.path.exists(\"./CamVid\"):\n",
    "    os.makedirs(\"./CamVid\", exist_ok=True)\n",
    "    url = \"https://github.com/alexgkendall/SegNet-Tutorial/archive/master.zip\"\n",
    "    wget.download(url, out=\"./CamVid/master.zip\")\n",
    "    \n",
    "    with zipfile.ZipFile(\"./CamVid/master.zip\", 'r') as zip_ref:\n",
    "        zip_ref.extractall(\"./CamVid\")\n",
    "    \n",
    "    os.rename(\"./CamVid/SegNet-Tutorial-master/CamVid\", \"./CamVid/data\")\n",
    "    os.remove(\"./CamVid/master.zip\")\n",
    "\n",
    "class CamVidDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_dir, image_size=(128, 128)):\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.image_size = image_size\n",
    "        \n",
    "        self.image_files = sorted([f for f in os.listdir(image_dir) if f.endswith('.png')])\n",
    "        self.label_files = sorted([f for f in os.listdir(label_dir) if f.endswith('.png')])\n",
    "                \n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(image_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, self.image_files[idx])\n",
    "        label_path = os.path.join(self.label_dir, self.label_files[idx])\n",
    "        \n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        label = Image.open(label_path)\n",
    "        \n",
    "        img = self.transform(img)\n",
    "        label = transforms.Resize(self.image_size, Image.NEAREST)(label)\n",
    "        label = torch.from_numpy(np.array(label)).long()\n",
    "        \n",
    "        return img, label\n",
    "\n",
    "train_data = CamVidDataset(\n",
    "    image_dir=\"./CamVid/data/train\",\n",
    "    label_dir=\"./CamVid/data/trainannot\",\n",
    "    image_size=IMAGE_SIZE\n",
    ")\n",
    "test_data = CamVidDataset(\n",
    "    image_dir=\"./CamVid/data/test\",\n",
    "    label_dir=\"./CamVid/data/testannot\",\n",
    "    image_size=IMAGE_SIZE\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model = smp.Unet(\n",
    "    encoder_name=\"resnet34\",\n",
    "    encoder_weights=\"imagenet\",\n",
    "    in_channels=3,\n",
    "    classes=32,\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "iou = JaccardIndex(task=\"multiclass\", num_classes=32).to(device)\n",
    "pixel_acc = Accuracy(task=\"multiclass\", num_classes=32).to(device)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    for images, targets in train_loader:\n",
    "        images, targets = images.to(device), targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, targets in test_loader:\n",
    "        images, targets = images.to(device), targets.to(device)\n",
    "        outputs = model(images)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        \n",
    "        iou.update(preds, targets)\n",
    "        pixel_acc.update(preds, targets)\n",
    "\n",
    "print(f\"IoU (Jaccard Index): {iou.compute().item():.4f}\")\n",
    "print(f\"Pixel Accuracy: {pixel_acc.compute().item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Обучить трансформерную модель (mit_b0) из segmentation_models_pytorch для выбранного набора данных и оценить качество моделей по выбранным метрикам на выбранном наборе данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daustinov/study/multimedia/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IoU (Jaccard Index): 0.3003\n",
      "Pixel Accuracy: 0.7670\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import segmentation_models_pytorch as smp\n",
    "from torchmetrics import JaccardIndex, Accuracy\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "BATCH_SIZE = 2\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 0.001\n",
    "IMAGE_SIZE = (224, 224)\n",
    "\n",
    "class CamVidDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_dir, image_size=(224, 224)):\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.image_size = image_size\n",
    "        \n",
    "        self.image_files = sorted([f for f in os.listdir(image_dir) if f.endswith('.png')])\n",
    "        self.label_files = sorted([f for f in os.listdir(label_dir) if f.endswith('.png')])\n",
    "                \n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(image_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, self.image_files[idx])\n",
    "        label_path = os.path.join(self.label_dir, self.label_files[idx])\n",
    "        \n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        label = Image.open(label_path)\n",
    "        \n",
    "        img = self.transform(img)\n",
    "        label = transforms.Resize(self.image_size, Image.NEAREST)(label)\n",
    "        label = torch.from_numpy(np.array(label)).long()\n",
    "        \n",
    "        return img, label\n",
    "\n",
    "train_data = CamVidDataset(\n",
    "    image_dir=\"./CamVid/data/train\",\n",
    "    label_dir=\"./CamVid/data/trainannot\",\n",
    "    image_size=IMAGE_SIZE\n",
    ")\n",
    "test_data = CamVidDataset(\n",
    "    image_dir=\"./CamVid/data/test\",\n",
    "    label_dir=\"./CamVid/data/testannot\",\n",
    "    image_size=IMAGE_SIZE\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model = smp.Unet(\n",
    "    encoder_name=\"mit_b0\",\n",
    "    encoder_weights=\"imagenet\",\n",
    "    in_channels=3,\n",
    "    classes=32,\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "iou = JaccardIndex(task=\"multiclass\", num_classes=32).to(device)\n",
    "pixel_acc = Accuracy(task=\"multiclass\", num_classes=32).to(device)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    for images, targets in train_loader:\n",
    "        images, targets = images.to(device), targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, targets in test_loader:\n",
    "        images, targets = images.to(device), targets.to(device)\n",
    "        outputs = model(images)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        \n",
    "        iou.update(preds, targets)\n",
    "        pixel_acc.update(preds, targets)\n",
    "\n",
    "print(f\"IoU (Jaccard Index): {iou.compute().item():.4f}\")\n",
    "print(f\"Pixel Accuracy: {pixel_acc.compute().item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Улучшение бейзлайна"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Сформулировать гипотезы (аугментации данных, подбор моделей, подбор гиперпараметров и т.д)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Аугментация данных**. Добавим небольшие сдвиги, повороты и изменения яркости/контраста\n",
    "2. **Оптимизация гиперпараметров + AdamW**. Подбор learning rate, batch size и использование AdamW с косинусным расписанием ускоряют сходимость.\n",
    "3. **Комбинированная функция потерь (Dice + CE)**. Сочетание кросс-энтропии и Dice Loss улучшает качество сегментации за счет баланса между точностью классификации и геометрией областей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение моделей, оценка качества обучения моделей по метрикам"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Аугментация данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Сверточная модель resnet34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IoU (Jaccard Index): 0.2267\n",
      "Pixel Accuracy: 0.6731\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import segmentation_models_pytorch as smp\n",
    "import random\n",
    "from torchmetrics import JaccardIndex, Accuracy\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 0.001\n",
    "IMAGE_SIZE = (128, 128)\n",
    "\n",
    "class CamVidDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_dir, image_size=(128, 128), is_train=True):\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.image_size = image_size\n",
    "        self.is_train = is_train\n",
    "        \n",
    "        self.image_files = sorted([f for f in os.listdir(image_dir) if f.endswith('.png')])\n",
    "        self.label_files = sorted([f for f in os.listdir(label_dir) if f.endswith('.png')])\n",
    "        \n",
    "        self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, self.image_files[idx])\n",
    "        label_path = os.path.join(self.label_dir, self.label_files[idx])\n",
    "        \n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        label = Image.open(label_path)\n",
    "        \n",
    "        img = TF.resize(img, self.image_size)\n",
    "        label = TF.resize(label, self.image_size, interpolation=Image.NEAREST)\n",
    "        \n",
    "        if self.is_train:\n",
    "            if random.random() > 0.5:\n",
    "                img = TF.hflip(img)\n",
    "                label = TF.hflip(label)\n",
    "            \n",
    "            if random.random() > 0.5:\n",
    "                img = TF.vflip(img)\n",
    "                label = TF.vflip(label)\n",
    "            \n",
    "            angle = random.uniform(-10, 10)\n",
    "            img = TF.rotate(img, angle)\n",
    "            label = TF.rotate(label, angle)\n",
    "            \n",
    "            brightness = random.uniform(0.8, 1.2)\n",
    "            contrast = random.uniform(0.8, 1.2)\n",
    "            img = TF.adjust_brightness(img, brightness)\n",
    "            img = TF.adjust_contrast(img, contrast)\n",
    "        \n",
    "        img = TF.to_tensor(img)\n",
    "        label = torch.from_numpy(np.array(label)).long()\n",
    "        \n",
    "        img = self.normalize(img)\n",
    "        \n",
    "        return img, label\n",
    "\n",
    "train_data = CamVidDataset(\n",
    "    image_dir=\"./CamVid/data/train\",\n",
    "    label_dir=\"./CamVid/data/trainannot\",\n",
    "    image_size=IMAGE_SIZE,\n",
    "    is_train=True\n",
    ")\n",
    "test_data = CamVidDataset(\n",
    "    image_dir=\"./CamVid/data/test\",\n",
    "    label_dir=\"./CamVid/data/testannot\",\n",
    "    image_size=IMAGE_SIZE,\n",
    "    is_train=False\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model = smp.Unet(\n",
    "    encoder_name=\"resnet34\",\n",
    "    encoder_weights=\"imagenet\",\n",
    "    in_channels=3,\n",
    "    classes=32,\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "iou = JaccardIndex(task=\"multiclass\", num_classes=32).to(device)\n",
    "pixel_acc = Accuracy(task=\"multiclass\", num_classes=32).to(device)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    for images, targets in train_loader:\n",
    "        images, targets = images.to(device), targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, targets in test_loader:\n",
    "        images, targets = images.to(device), targets.to(device)\n",
    "        outputs = model(images)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        \n",
    "        iou.update(preds, targets)\n",
    "        pixel_acc.update(preds, targets)\n",
    "\n",
    "print(f\"IoU (Jaccard Index): {iou.compute().item():.4f}\")\n",
    "print(f\"Pixel Accuracy: {pixel_acc.compute().item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Трансформерная модель mit_b0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IoU (Jaccard Index): 0.1834\n",
      "Pixel Accuracy: 0.6261\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import segmentation_models_pytorch as smp\n",
    "import random\n",
    "from torchmetrics import JaccardIndex, Accuracy\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "BATCH_SIZE = 2\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 0.001\n",
    "IMAGE_SIZE = (224, 224)\n",
    "\n",
    "class CamVidDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_dir, image_size=(128, 128), is_train=True):\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.image_size = image_size\n",
    "        self.is_train = is_train\n",
    "        \n",
    "        self.image_files = sorted([f for f in os.listdir(image_dir) if f.endswith('.png')])\n",
    "        self.label_files = sorted([f for f in os.listdir(label_dir) if f.endswith('.png')])\n",
    "        \n",
    "        self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, self.image_files[idx])\n",
    "        label_path = os.path.join(self.label_dir, self.label_files[idx])\n",
    "        \n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        label = Image.open(label_path)\n",
    "        \n",
    "        img = TF.resize(img, self.image_size)\n",
    "        label = TF.resize(label, self.image_size, interpolation=Image.NEAREST)\n",
    "        \n",
    "        if self.is_train:\n",
    "            if random.random() > 0.5:\n",
    "                img = TF.hflip(img)\n",
    "                label = TF.hflip(label)\n",
    "            \n",
    "            if random.random() > 0.5:\n",
    "                img = TF.vflip(img)\n",
    "                label = TF.vflip(label)\n",
    "            \n",
    "            angle = random.uniform(-10, 10)\n",
    "            img = TF.rotate(img, angle)\n",
    "            label = TF.rotate(label, angle)\n",
    "            \n",
    "            brightness = random.uniform(0.8, 1.2)\n",
    "            contrast = random.uniform(0.8, 1.2)\n",
    "            img = TF.adjust_brightness(img, brightness)\n",
    "            img = TF.adjust_contrast(img, contrast)\n",
    "        \n",
    "        img = TF.to_tensor(img)\n",
    "        label = torch.from_numpy(np.array(label)).long()\n",
    "        \n",
    "        img = self.normalize(img)\n",
    "        \n",
    "        return img, label\n",
    "\n",
    "train_data = CamVidDataset(\n",
    "    image_dir=\"./CamVid/data/train\",\n",
    "    label_dir=\"./CamVid/data/trainannot\",\n",
    "    image_size=IMAGE_SIZE,\n",
    "    is_train=True\n",
    ")\n",
    "test_data = CamVidDataset(\n",
    "    image_dir=\"./CamVid/data/test\",\n",
    "    label_dir=\"./CamVid/data/testannot\",\n",
    "    image_size=IMAGE_SIZE,\n",
    "    is_train=False\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model = smp.Unet(\n",
    "    encoder_name=\"mit_b0\",\n",
    "    encoder_weights=\"imagenet\",\n",
    "    in_channels=3,\n",
    "    classes=32,\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "iou = JaccardIndex(task=\"multiclass\", num_classes=32).to(device)\n",
    "pixel_acc = Accuracy(task=\"multiclass\", num_classes=32).to(device)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    for images, targets in train_loader:\n",
    "        images, targets = images.to(device), targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, targets in test_loader:\n",
    "        images, targets = images.to(device), targets.to(device)\n",
    "        outputs = model(images)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        \n",
    "        iou.update(preds, targets)\n",
    "        pixel_acc.update(preds, targets)\n",
    "\n",
    "print(f\"IoU (Jaccard Index): {iou.compute().item():.4f}\")\n",
    "print(f\"Pixel Accuracy: {pixel_acc.compute().item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Оптимизация гиперпараметров + AdamW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Сверточная модель resnet34"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Уменьшаем LEARNING_RATE и увеличиваем количество эпох и размер батча. Используем AdamW и CosineAnnealingLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IoU (Jaccard Index): 0.3560\n",
      "Pixel Accuracy: 0.8121\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import segmentation_models_pytorch as smp\n",
    "from torchmetrics import JaccardIndex, Accuracy\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 5\n",
    "LEARNING_RATE = 0.0005\n",
    "IMAGE_SIZE = (128, 128)\n",
    "\n",
    "class CamVidDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_dir, image_size=(224, 224)):\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.image_size = image_size\n",
    "        \n",
    "        self.image_files = sorted([f for f in os.listdir(image_dir) if f.endswith('.png')])\n",
    "        self.label_files = sorted([f for f in os.listdir(label_dir) if f.endswith('.png')])\n",
    "                \n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(image_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, self.image_files[idx])\n",
    "        label_path = os.path.join(self.label_dir, self.label_files[idx])\n",
    "        \n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        label = Image.open(label_path)\n",
    "        \n",
    "        img = self.transform(img)\n",
    "        label = transforms.Resize(self.image_size, Image.NEAREST)(label)\n",
    "        label = torch.from_numpy(np.array(label)).long()\n",
    "        \n",
    "        return img, label\n",
    "\n",
    "train_data = CamVidDataset(\n",
    "    image_dir=\"./CamVid/data/train\",\n",
    "    label_dir=\"./CamVid/data/trainannot\",\n",
    "    image_size=IMAGE_SIZE\n",
    ")\n",
    "test_data = CamVidDataset(\n",
    "    image_dir=\"./CamVid/data/test\",\n",
    "    label_dir=\"./CamVid/data/testannot\",\n",
    "    image_size=IMAGE_SIZE\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model = smp.Unet(\n",
    "    encoder_name=\"resnet34\",\n",
    "    encoder_weights=\"imagenet\",\n",
    "    in_channels=3,\n",
    "    classes=32,\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "\n",
    "iou = JaccardIndex(task=\"multiclass\", num_classes=32).to(device)\n",
    "pixel_acc = Accuracy(task=\"multiclass\", num_classes=32).to(device)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    for images, targets in train_loader:\n",
    "        images, targets = images.to(device), targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    scheduler.step()\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, targets in test_loader:\n",
    "        images, targets = images.to(device), targets.to(device)\n",
    "        outputs = model(images)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        \n",
    "        iou.update(preds, targets)\n",
    "        pixel_acc.update(preds, targets)\n",
    "\n",
    "print(f\"IoU (Jaccard Index): {iou.compute().item():.4f}\")\n",
    "print(f\"Pixel Accuracy: {pixel_acc.compute().item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Трансформерная модель mit_b0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Уменьшаем LEARNING_RATE и увеличиваем количество эпох и размер батча. Используем AdamW и CosineAnnealingLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IoU (Jaccard Index): 0.3920\n",
      "Pixel Accuracy: 0.8366\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import segmentation_models_pytorch as smp\n",
    "from torchmetrics import JaccardIndex, Accuracy\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 5\n",
    "LEARNING_RATE = 0.0005\n",
    "IMAGE_SIZE = (224, 224)\n",
    "\n",
    "class CamVidDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_dir, image_size=(224, 224)):\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.image_size = image_size\n",
    "        \n",
    "        self.image_files = sorted([f for f in os.listdir(image_dir) if f.endswith('.png')])\n",
    "        self.label_files = sorted([f for f in os.listdir(label_dir) if f.endswith('.png')])\n",
    "                \n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(image_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, self.image_files[idx])\n",
    "        label_path = os.path.join(self.label_dir, self.label_files[idx])\n",
    "        \n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        label = Image.open(label_path)\n",
    "        \n",
    "        img = self.transform(img)\n",
    "        label = transforms.Resize(self.image_size, Image.NEAREST)(label)\n",
    "        label = torch.from_numpy(np.array(label)).long()\n",
    "        \n",
    "        return img, label\n",
    "\n",
    "train_data = CamVidDataset(\n",
    "    image_dir=\"./CamVid/data/train\",\n",
    "    label_dir=\"./CamVid/data/trainannot\",\n",
    "    image_size=IMAGE_SIZE\n",
    ")\n",
    "test_data = CamVidDataset(\n",
    "    image_dir=\"./CamVid/data/test\",\n",
    "    label_dir=\"./CamVid/data/testannot\",\n",
    "    image_size=IMAGE_SIZE\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "model = smp.Unet(\n",
    "    encoder_name=\"mit_b0\",\n",
    "    encoder_weights=\"imagenet\",\n",
    "    in_channels=3,\n",
    "    classes=32,\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "\n",
    "iou = JaccardIndex(task=\"multiclass\", num_classes=32).to(device)\n",
    "pixel_acc = Accuracy(task=\"multiclass\", num_classes=32).to(device)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    for images, targets in train_loader:\n",
    "        images, targets = images.to(device), targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    scheduler.step()\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, targets in test_loader:\n",
    "        images, targets = images.to(device), targets.to(device)\n",
    "        outputs = model(images)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        \n",
    "        iou.update(preds, targets)\n",
    "        pixel_acc.update(preds, targets)\n",
    "\n",
    "print(f\"IoU (Jaccard Index): {iou.compute().item():.4f}\")\n",
    "print(f\"Pixel Accuracy: {pixel_acc.compute().item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Комбинированная функция потерь (Dice + CE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Сверточная модель resnet34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IoU (Jaccard Index): 0.3633\n",
      "Pixel Accuracy: 0.8088\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import segmentation_models_pytorch as smp\n",
    "from torchmetrics import JaccardIndex, Accuracy\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 0.001\n",
    "IMAGE_SIZE = (128, 128)\n",
    "\n",
    "class CamVidDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_dir, image_size=(224, 224)):\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.image_size = image_size\n",
    "        \n",
    "        self.image_files = sorted([f for f in os.listdir(image_dir) if f.endswith('.png')])\n",
    "        self.label_files = sorted([f for f in os.listdir(label_dir) if f.endswith('.png')])\n",
    "                \n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(image_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, self.image_files[idx])\n",
    "        label_path = os.path.join(self.label_dir, self.label_files[idx])\n",
    "        \n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        label = Image.open(label_path)\n",
    "        \n",
    "        img = self.transform(img)\n",
    "        label = transforms.Resize(self.image_size, Image.NEAREST)(label)\n",
    "        label = torch.from_numpy(np.array(label)).long()\n",
    "        \n",
    "        return img, label\n",
    "\n",
    "train_data = CamVidDataset(\n",
    "    image_dir=\"./CamVid/data/train\",\n",
    "    label_dir=\"./CamVid/data/trainannot\",\n",
    "    image_size=IMAGE_SIZE\n",
    ")\n",
    "test_data = CamVidDataset(\n",
    "    image_dir=\"./CamVid/data/test\",\n",
    "    label_dir=\"./CamVid/data/testannot\",\n",
    "    image_size=IMAGE_SIZE\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model = smp.Unet(\n",
    "    encoder_name=\"resnet34\",\n",
    "    encoder_weights=\"imagenet\",\n",
    "    in_channels=3,\n",
    "    classes=32,\n",
    ").to(device)\n",
    "\n",
    "class DiceCELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ce = nn.CrossEntropyLoss()\n",
    "        self.dice = smp.losses.DiceLoss(mode='multiclass')\n",
    "    \n",
    "    def forward(self, outputs, targets):\n",
    "        return 0.5 * self.ce(outputs, targets) + 0.5 * self.dice(outputs, targets)\n",
    "\n",
    "criterion = DiceCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "iou = JaccardIndex(task=\"multiclass\", num_classes=32).to(device)\n",
    "pixel_acc = Accuracy(task=\"multiclass\", num_classes=32).to(device)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    for images, targets in train_loader:\n",
    "        images, targets = images.to(device), targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, targets in test_loader:\n",
    "        images, targets = images.to(device), targets.to(device)\n",
    "        outputs = model(images)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        \n",
    "        iou.update(preds, targets)\n",
    "        pixel_acc.update(preds, targets)\n",
    "\n",
    "print(f\"IoU (Jaccard Index): {iou.compute().item():.4f}\")\n",
    "print(f\"Pixel Accuracy: {pixel_acc.compute().item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Трансформерная модель mit_b0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IoU (Jaccard Index): 0.3341\n",
      "Pixel Accuracy: 0.7872\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import segmentation_models_pytorch as smp\n",
    "from torchmetrics import JaccardIndex, Accuracy\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "BATCH_SIZE = 2\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 0.001\n",
    "IMAGE_SIZE = (224, 224)\n",
    "\n",
    "class CamVidDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_dir, image_size=(224, 224)):\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.image_size = image_size\n",
    "        \n",
    "        self.image_files = sorted([f for f in os.listdir(image_dir) if f.endswith('.png')])\n",
    "        self.label_files = sorted([f for f in os.listdir(label_dir) if f.endswith('.png')])\n",
    "                \n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(image_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, self.image_files[idx])\n",
    "        label_path = os.path.join(self.label_dir, self.label_files[idx])\n",
    "        \n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        label = Image.open(label_path)\n",
    "        \n",
    "        img = self.transform(img)\n",
    "        label = transforms.Resize(self.image_size, Image.NEAREST)(label)\n",
    "        label = torch.from_numpy(np.array(label)).long()\n",
    "        \n",
    "        return img, label\n",
    "\n",
    "train_data = CamVidDataset(\n",
    "    image_dir=\"./CamVid/data/train\",\n",
    "    label_dir=\"./CamVid/data/trainannot\",\n",
    "    image_size=IMAGE_SIZE\n",
    ")\n",
    "test_data = CamVidDataset(\n",
    "    image_dir=\"./CamVid/data/test\",\n",
    "    label_dir=\"./CamVid/data/testannot\",\n",
    "    image_size=IMAGE_SIZE\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model = smp.Unet(\n",
    "    encoder_name=\"mit_b0\",\n",
    "    encoder_weights=\"imagenet\",\n",
    "    in_channels=3,\n",
    "    classes=32,\n",
    ").to(device)\n",
    "\n",
    "class DiceCELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ce = nn.CrossEntropyLoss()\n",
    "        self.dice = smp.losses.DiceLoss(mode='multiclass')\n",
    "    \n",
    "    def forward(self, outputs, targets):\n",
    "        return 0.5 * self.ce(outputs, targets) + 0.5 * self.dice(outputs, targets)\n",
    "\n",
    "criterion = DiceCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "iou = JaccardIndex(task=\"multiclass\", num_classes=32).to(device)\n",
    "pixel_acc = Accuracy(task=\"multiclass\", num_classes=32).to(device)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    for images, targets in train_loader:\n",
    "        images, targets = images.to(device), targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, targets in test_loader:\n",
    "        images, targets = images.to(device), targets.to(device)\n",
    "        outputs = model(images)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        \n",
    "        iou.update(preds, targets)\n",
    "        pixel_acc.update(preds, targets)\n",
    "\n",
    "print(f\"IoU (Jaccard Index): {iou.compute().item():.4f}\")\n",
    "print(f\"Pixel Accuracy: {pixel_acc.compute().item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Окончательный улучшенный бейзлайн"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Сверточная модель resnet34"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используем улучшения из 2 и 3 гипотезы. Не используем аугментацию (1 гипотеза) - так как она не дала улучшений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IoU (Jaccard Index): 0.3640\n",
      "Pixel Accuracy: 0.8116\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import segmentation_models_pytorch as smp\n",
    "from torchmetrics import JaccardIndex, Accuracy\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 5\n",
    "LEARNING_RATE = 0.0005\n",
    "IMAGE_SIZE = (128, 128)\n",
    "\n",
    "class CamVidDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_dir, image_size=(224, 224)):\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.image_size = image_size\n",
    "        \n",
    "        self.image_files = sorted([f for f in os.listdir(image_dir) if f.endswith('.png')])\n",
    "        self.label_files = sorted([f for f in os.listdir(label_dir) if f.endswith('.png')])\n",
    "                \n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(image_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, self.image_files[idx])\n",
    "        label_path = os.path.join(self.label_dir, self.label_files[idx])\n",
    "        \n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        label = Image.open(label_path)\n",
    "        \n",
    "        img = self.transform(img)\n",
    "        label = transforms.Resize(self.image_size, Image.NEAREST)(label)\n",
    "        label = torch.from_numpy(np.array(label)).long()\n",
    "        \n",
    "        return img, label\n",
    "\n",
    "train_data = CamVidDataset(\n",
    "    image_dir=\"./CamVid/data/train\",\n",
    "    label_dir=\"./CamVid/data/trainannot\",\n",
    "    image_size=IMAGE_SIZE\n",
    ")\n",
    "test_data = CamVidDataset(\n",
    "    image_dir=\"./CamVid/data/test\",\n",
    "    label_dir=\"./CamVid/data/testannot\",\n",
    "    image_size=IMAGE_SIZE\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model = smp.Unet(\n",
    "    encoder_name=\"resnet34\",\n",
    "    encoder_weights=\"imagenet\",\n",
    "    in_channels=3,\n",
    "    classes=32,\n",
    ").to(device)\n",
    "\n",
    "class DiceCELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ce = nn.CrossEntropyLoss()\n",
    "        self.dice = smp.losses.DiceLoss(mode='multiclass')\n",
    "    \n",
    "    def forward(self, outputs, targets):\n",
    "        return 0.5 * self.ce(outputs, targets) + 0.5 * self.dice(outputs, targets)\n",
    "\n",
    "criterion = DiceCELoss()\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "\n",
    "iou = JaccardIndex(task=\"multiclass\", num_classes=32).to(device)\n",
    "pixel_acc = Accuracy(task=\"multiclass\", num_classes=32).to(device)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    for images, targets in train_loader:\n",
    "        images, targets = images.to(device), targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    scheduler.step()\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, targets in test_loader:\n",
    "        images, targets = images.to(device), targets.to(device)\n",
    "        outputs = model(images)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        \n",
    "        iou.update(preds, targets)\n",
    "        pixel_acc.update(preds, targets)\n",
    "\n",
    "print(f\"IoU (Jaccard Index): {iou.compute().item():.4f}\")\n",
    "print(f\"Pixel Accuracy: {pixel_acc.compute().item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Трансформерная модель mit_b0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используем улучшения из 2 и 3 гипотезы. Не используем аугментацию (1 гипотеза) - так как она не дала улучшений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IoU (Jaccard Index): 0.3942\n",
      "Pixel Accuracy: 0.8342\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import segmentation_models_pytorch as smp\n",
    "from torchmetrics import JaccardIndex, Accuracy\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 5\n",
    "LEARNING_RATE = 0.0005\n",
    "IMAGE_SIZE = (224, 224)\n",
    "\n",
    "class CamVidDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_dir, image_size=(224, 224)):\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.image_size = image_size\n",
    "        \n",
    "        self.image_files = sorted([f for f in os.listdir(image_dir) if f.endswith('.png')])\n",
    "        self.label_files = sorted([f for f in os.listdir(label_dir) if f.endswith('.png')])\n",
    "                \n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(image_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, self.image_files[idx])\n",
    "        label_path = os.path.join(self.label_dir, self.label_files[idx])\n",
    "        \n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        label = Image.open(label_path)\n",
    "        \n",
    "        img = self.transform(img)\n",
    "        label = transforms.Resize(self.image_size, Image.NEAREST)(label)\n",
    "        label = torch.from_numpy(np.array(label)).long()\n",
    "        \n",
    "        return img, label\n",
    "\n",
    "train_data = CamVidDataset(\n",
    "    image_dir=\"./CamVid/data/train\",\n",
    "    label_dir=\"./CamVid/data/trainannot\",\n",
    "    image_size=IMAGE_SIZE\n",
    ")\n",
    "test_data = CamVidDataset(\n",
    "    image_dir=\"./CamVid/data/test\",\n",
    "    label_dir=\"./CamVid/data/testannot\",\n",
    "    image_size=IMAGE_SIZE\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "model = smp.Unet(\n",
    "    encoder_name=\"mit_b0\",\n",
    "    encoder_weights=\"imagenet\",\n",
    "    in_channels=3,\n",
    "    classes=32,\n",
    ").to(device)\n",
    "\n",
    "class DiceCELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ce = nn.CrossEntropyLoss()\n",
    "        self.dice = smp.losses.DiceLoss(mode='multiclass')\n",
    "    \n",
    "    def forward(self, outputs, targets):\n",
    "        return 0.5 * self.ce(outputs, targets) + 0.5 * self.dice(outputs, targets)\n",
    "\n",
    "criterion = DiceCELoss()\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "\n",
    "iou = JaccardIndex(task=\"multiclass\", num_classes=32).to(device)\n",
    "pixel_acc = Accuracy(task=\"multiclass\", num_classes=32).to(device)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    for images, targets in train_loader:\n",
    "        images, targets = images.to(device), targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    scheduler.step()\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, targets in test_loader:\n",
    "        images, targets = images.to(device), targets.to(device)\n",
    "        outputs = model(images)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        \n",
    "        iou.update(preds, targets)\n",
    "        pixel_acc.update(preds, targets)\n",
    "\n",
    "print(f\"IoU (Jaccard Index): {iou.compute().item():.4f}\")\n",
    "print(f\"Pixel Accuracy: {pixel_acc.compute().item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В ходе экспериментального исследования были последовательно проверены три гипотезы по улучшению базовых моделей семантической сегментации на датасете CamVid. Исходные показатели для сверточной модели resnet34 демонстрировали уровень IoU 0.329 и точность пикселей 0.777, в то время как трансформерная модель mit_b0 показывала несколько более скромные результаты - IoU 0.300 при точности 0.767.\n",
    "\n",
    "Применение аугментации данных, вопреки ожиданиям, привело к ухудшению метрик обеих моделей, что может объясняться недостаточно точной настройкой параметров преобразований или особенностями самого датасета. Наиболее значимый положительный эффект был достигнут при оптимизации гиперпараметров и использовании усовершенствованного оптимизатора AdamW. Для resnet34 это позволило повысить IoU до 0.356 при точности 0.812, а mit_b0 показала еще более впечатляющий рост - IoU 0.392 при точности 0.837.\n",
    "\n",
    "Введение комбинированной функции потерь Dice + CrossEntropy также дало положительный результат, хотя и менее выраженный. Наилучшие итоговые показатели были получены при совместном применении техник из второй и третьей гипотез. Финальные метрики улучшенной resnet34 составили IoU 0.364 и точность 0.812, а mit_b0 достигла IoU 0.394 при точности 0.834. \n",
    "\n",
    "Считаю важным отметить, что трансформерная архитектура в конечном итоге превзошла сверточную по основному показателю IoU, демонстрируя потенциал подобных моделей для задач сегментации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Имплементация алгоритма машинного обучения "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Имплементация сверточной модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class UpBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_channels, out_channels, 2, stride=2)\n",
    "        self.conv = ConvBlock(in_channels, out_channels)\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "class CustomUNet(nn.Module):\n",
    "    def __init__(self, n_classes=32):\n",
    "        super().__init__()\n",
    "        self.down1 = ConvBlock(3, 64)\n",
    "        self.down2 = ConvBlock(64, 128)\n",
    "        self.down3 = ConvBlock(128, 256)\n",
    "        self.down4 = ConvBlock(256, 512)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.bottleneck = ConvBlock(512, 1024)\n",
    "        \n",
    "        self.up1 = UpBlock(1024, 512)\n",
    "        self.up2 = UpBlock(512, 256)\n",
    "        self.up3 = UpBlock(256, 128)\n",
    "        self.up4 = UpBlock(128, 64)\n",
    "        \n",
    "        self.out = nn.Conv2d(64, n_classes, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.down1(x)\n",
    "        x2 = self.pool(x1)\n",
    "        x2 = self.down2(x2)\n",
    "        x3 = self.pool(x2)\n",
    "        x3 = self.down3(x3)\n",
    "        x4 = self.pool(x3)\n",
    "        x4 = self.down4(x4)\n",
    "        x5 = self.pool(x4)\n",
    "        \n",
    "        x5 = self.bottleneck(x5)\n",
    "        \n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        \n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Имплементация трансформерной модели модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange, repeat\n",
    "from torch import einsum\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, patch_size=16, in_chans=3, embed_dim=384):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "        x = rearrange(x, 'b e h w -> b (h w) e')\n",
    "        return x\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=6):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.scale = (dim // num_heads) ** -0.5\n",
    "        \n",
    "        self.to_qkv = nn.Linear(dim, dim * 3)\n",
    "        self.to_out = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=self.num_heads), qkv)\n",
    "        \n",
    "        dots = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n",
    "        attn = dots.softmax(dim=-1)\n",
    "        \n",
    "        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return self.to_out(out)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, dim, num_heads=6, mlp_dim=1536):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn = Attention(dim, num_heads)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, mlp_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(mlp_dim, dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "class SimpleSegmenter(nn.Module):\n",
    "    def __init__(self, n_classes=32, img_size=224, patch_size=16):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.embed_dim = 384\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        \n",
    "        self.patch_embed = PatchEmbedding(patch_size, 3, self.embed_dim)\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, self.num_patches, self.embed_dim))\n",
    "        \n",
    "        self.blocks = nn.Sequential(\n",
    "            *[TransformerBlock(self.embed_dim) for _ in range(4)]\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.LayerNorm(self.embed_dim),\n",
    "            nn.Linear(self.embed_dim, n_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        \n",
    "        x = self.patch_embed(x)\n",
    "        x = x + self.pos_embed\n",
    "        \n",
    "        x = self.blocks(x)\n",
    "        \n",
    "        x = rearrange(x, 'b (h w) c -> b c h w', h=H//self.patch_size, w=W//self.patch_size)\n",
    "        x = F.interpolate(x, size=(H, W), mode='bilinear', align_corners=False)\n",
    "        \n",
    "        x = x.permute(0, 2, 3, 1)\n",
    "        x = self.decoder(x)\n",
    "        return x.permute(0, 3, 1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение моделей на выбранных датасетах и вывод метрик"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Сверточная модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "IoU: 0.1848 | Pixel Acc: 0.6246\n",
      "Epoch 2/3\n",
      "IoU: 0.2243 | Pixel Acc: 0.6359\n",
      "Epoch 3/3\n",
      "IoU: 0.2291 | Pixel Acc: 0.6649\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchmetrics import JaccardIndex, Accuracy\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 0.001\n",
    "IMAGE_SIZE = (128, 128)\n",
    "\n",
    "class CamVidDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_dir, image_size=(224, 224)):\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.image_size = image_size\n",
    "        \n",
    "        self.image_files = sorted([f for f in os.listdir(image_dir) if f.endswith('.png')])\n",
    "        self.label_files = sorted([f for f in os.listdir(label_dir) if f.endswith('.png')])\n",
    "                \n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(image_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, self.image_files[idx])\n",
    "        label_path = os.path.join(self.label_dir, self.label_files[idx])\n",
    "        \n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        label = Image.open(label_path)\n",
    "        \n",
    "        img = self.transform(img)\n",
    "        label = transforms.Resize(self.image_size, Image.NEAREST)(label)\n",
    "        label = torch.from_numpy(np.array(label)).long()\n",
    "        \n",
    "        return img, label\n",
    "\n",
    "train_data = CamVidDataset(\n",
    "    image_dir=\"./CamVid/data/train\",\n",
    "    label_dir=\"./CamVid/data/trainannot\",\n",
    "    image_size=IMAGE_SIZE\n",
    ")\n",
    "test_data = CamVidDataset(\n",
    "    image_dir=\"./CamVid/data/test\",\n",
    "    label_dir=\"./CamVid/data/testannot\",\n",
    "    image_size=IMAGE_SIZE\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model = CustomUNet(n_classes=32).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "iou = JaccardIndex(task=\"multiclass\", num_classes=32).to(device)\n",
    "pixel_acc = Accuracy(task=\"multiclass\", num_classes=32).to(device)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    for images, targets in train_loader:\n",
    "        images, targets = images.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, targets in test_loader:\n",
    "            images, targets = images.to(device), targets.to(device)\n",
    "            outputs = model(images)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            iou.update(preds, targets)\n",
    "            pixel_acc.update(preds, targets)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    print(f\"IoU: {iou.compute().item():.4f} | Pixel Acc: {pixel_acc.compute().item():.4f}\")\n",
    "    iou.reset()\n",
    "    pixel_acc.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Трансформерная модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "IoU: 0.1460 | Pixel Acc: 0.5720\n",
      "Epoch 2/3\n",
      "IoU: 0.1558 | Pixel Acc: 0.5685\n",
      "Epoch 3/3\n",
      "IoU: 0.1713 | Pixel Acc: 0.6463\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchmetrics import JaccardIndex, Accuracy\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "BATCH_SIZE = 2\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 0.001\n",
    "IMAGE_SIZE = (224, 224)\n",
    "\n",
    "class CamVidDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_dir, image_size=(224, 224)):\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.image_size = image_size\n",
    "        \n",
    "        self.image_files = sorted([f for f in os.listdir(image_dir) if f.endswith('.png')])\n",
    "        self.label_files = sorted([f for f in os.listdir(label_dir) if f.endswith('.png')])\n",
    "                \n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(image_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, self.image_files[idx])\n",
    "        label_path = os.path.join(self.label_dir, self.label_files[idx])\n",
    "        \n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        label = Image.open(label_path)\n",
    "        \n",
    "        img = self.transform(img)\n",
    "        label = transforms.Resize(self.image_size, Image.NEAREST)(label)\n",
    "        label = torch.from_numpy(np.array(label)).long()\n",
    "        \n",
    "        return img, label\n",
    "\n",
    "train_data = CamVidDataset(\n",
    "    image_dir=\"./CamVid/data/train\",\n",
    "    label_dir=\"./CamVid/data/trainannot\",\n",
    "    image_size=IMAGE_SIZE\n",
    ")\n",
    "test_data = CamVidDataset(\n",
    "    image_dir=\"./CamVid/data/test\",\n",
    "    label_dir=\"./CamVid/data/testannot\",\n",
    "    image_size=IMAGE_SIZE\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model = SimpleSegmenter(n_classes=32).to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "iou = JaccardIndex(task=\"multiclass\", num_classes=32).to(device)\n",
    "pixel_acc = Accuracy(task=\"multiclass\", num_classes=32).to(device)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    for images, targets in train_loader:\n",
    "        images, targets = images.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, targets in test_loader:\n",
    "            images, targets = images.to(device), targets.to(device)\n",
    "            outputs = model(images)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            iou.update(preds, targets)\n",
    "            pixel_acc.update(preds, targets)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    print(f\"IoU: {iou.compute().item():.4f} | Pixel Acc: {pixel_acc.compute().item():.4f}\")\n",
    "    iou.reset()\n",
    "    pixel_acc.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сравнение результатов с п.2. Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При сравнении результатов самостоятельно реализованных моделей с базовыми версиями из библиотеки segmentation_models.pytorch наблюдаются заметные различия в качестве сегментации. Сверточная модель на основе кастомной U-Net архитектуры показала значение IoU 0.229, что примерно на 30% ниже аналогичного показателя у стандартной ResNet34 (0.329), при этом точность пиксельной классификации также оказалась существенно ниже - 0.665 против 0.777. Еще более значительный разрыв наблюдается у трансформерной модели: самостоятельно реализованный сегментер достиг IoU всего 0.171, что почти в два раза хуже показателя MIT-B0 из библиотеки (0.300), с аналогичным отставанием по pixel accuracy (0.646 против 0.767).\n",
    "\n",
    "Такое существенное расхождение в метриках объясняется несколькими ключевыми факторами. Во-первых, библиотечные модели используют предобученные на ImageNet энкодеры, что дает им значительное преимущество в качестве извлечения признаков. Во-вторых, в реализациях segmentation_models.pytorch применяются дополнительные оптимизации и тонкая настройка архитектуры, наработанные за годы исследований. В-третьих, самостоятельно созданные модели имеют упрощенную структуру.\n",
    "\n",
    "Считаю важным заметить, что обе самостоятельно реализованные модели демонстрируют схожее относительное отставание от своих библиотечных аналогов, что показывает важность использования отработанных архитектурных решений и предобученных компонентов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Улучшение бейзлайна. Добавлений техник для каждой из моделей из пункта 3c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Сверточная модель"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используем улучшения из 2 и 3 гипотезы. Не используем аугментацию (1 гипотеза) - так как она не дала улучшений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "IoU: 0.1393 | Pixel Acc: 0.6040\n",
      "Epoch 2/5\n",
      "IoU: 0.2434 | Pixel Acc: 0.6776\n",
      "Epoch 3/5\n",
      "IoU: 0.2425 | Pixel Acc: 0.6892\n",
      "Epoch 4/5\n",
      "IoU: 0.3018 | Pixel Acc: 0.7618\n",
      "Epoch 5/5\n",
      "IoU: 0.3093 | Pixel Acc: 0.7620\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import segmentation_models_pytorch as smp\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchmetrics import JaccardIndex, Accuracy\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 5\n",
    "LEARNING_RATE = 0.0005\n",
    "IMAGE_SIZE = (128, 128)\n",
    "\n",
    "class CamVidDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_dir, image_size=(224, 224)):\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.image_size = image_size\n",
    "        \n",
    "        self.image_files = sorted([f for f in os.listdir(image_dir) if f.endswith('.png')])\n",
    "        self.label_files = sorted([f for f in os.listdir(label_dir) if f.endswith('.png')])\n",
    "                \n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(image_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, self.image_files[idx])\n",
    "        label_path = os.path.join(self.label_dir, self.label_files[idx])\n",
    "        \n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        label = Image.open(label_path)\n",
    "        \n",
    "        img = self.transform(img)\n",
    "        label = transforms.Resize(self.image_size, Image.NEAREST)(label)\n",
    "        label = torch.from_numpy(np.array(label)).long()\n",
    "        \n",
    "        return img, label\n",
    "\n",
    "train_data = CamVidDataset(\n",
    "    image_dir=\"./CamVid/data/train\",\n",
    "    label_dir=\"./CamVid/data/trainannot\",\n",
    "    image_size=IMAGE_SIZE\n",
    ")\n",
    "test_data = CamVidDataset(\n",
    "    image_dir=\"./CamVid/data/test\",\n",
    "    label_dir=\"./CamVid/data/testannot\",\n",
    "    image_size=IMAGE_SIZE\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model = CustomUNet(n_classes=32).to(device)\n",
    "\n",
    "class DiceCELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ce = nn.CrossEntropyLoss()\n",
    "        self.dice = smp.losses.DiceLoss(mode='multiclass')\n",
    "    \n",
    "    def forward(self, outputs, targets):\n",
    "        return 0.5 * self.ce(outputs, targets) + 0.5 * self.dice(outputs, targets)\n",
    "\n",
    "criterion = DiceCELoss()\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "\n",
    "iou = JaccardIndex(task=\"multiclass\", num_classes=32).to(device)\n",
    "pixel_acc = Accuracy(task=\"multiclass\", num_classes=32).to(device)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    for images, targets in train_loader:\n",
    "        images, targets = images.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, targets in test_loader:\n",
    "            images, targets = images.to(device), targets.to(device)\n",
    "            outputs = model(images)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            iou.update(preds, targets)\n",
    "            pixel_acc.update(preds, targets)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    print(f\"IoU: {iou.compute().item():.4f} | Pixel Acc: {pixel_acc.compute().item():.4f}\")\n",
    "    iou.reset()\n",
    "    pixel_acc.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Трансформерная модель"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используем улучшения из 2 и 3 гипотезы. Не используем аугментацию (1 гипотеза) - так как она не дала улучшений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "IoU: 0.1966 | Pixel Acc: 0.6321\n",
      "Epoch 2/5\n",
      "IoU: 0.2016 | Pixel Acc: 0.6304\n",
      "Epoch 3/5\n",
      "IoU: 0.2585 | Pixel Acc: 0.6794\n",
      "Epoch 4/5\n",
      "IoU: 0.2606 | Pixel Acc: 0.6947\n",
      "Epoch 5/5\n",
      "IoU: 0.2858 | Pixel Acc: 0.7148\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchmetrics import JaccardIndex, Accuracy\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import segmentation_models_pytorch as smp\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 5\n",
    "LEARNING_RATE = 0.0005\n",
    "IMAGE_SIZE = (224, 224)\n",
    "\n",
    "class CamVidDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_dir, image_size=(224, 224)):\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.image_size = image_size\n",
    "        \n",
    "        self.image_files = sorted([f for f in os.listdir(image_dir) if f.endswith('.png')])\n",
    "        self.label_files = sorted([f for f in os.listdir(label_dir) if f.endswith('.png')])\n",
    "                \n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(image_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, self.image_files[idx])\n",
    "        label_path = os.path.join(self.label_dir, self.label_files[idx])\n",
    "        \n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        label = Image.open(label_path)\n",
    "        \n",
    "        img = self.transform(img)\n",
    "        label = transforms.Resize(self.image_size, Image.NEAREST)(label)\n",
    "        label = torch.from_numpy(np.array(label)).long()\n",
    "        \n",
    "        return img, label\n",
    "\n",
    "train_data = CamVidDataset(\n",
    "    image_dir=\"./CamVid/data/train\",\n",
    "    label_dir=\"./CamVid/data/trainannot\",\n",
    "    image_size=IMAGE_SIZE\n",
    ")\n",
    "test_data = CamVidDataset(\n",
    "    image_dir=\"./CamVid/data/test\",\n",
    "    label_dir=\"./CamVid/data/testannot\",\n",
    "    image_size=IMAGE_SIZE\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model = SimpleSegmenter(n_classes=32).to(device)\n",
    "\n",
    "class DiceCELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ce = nn.CrossEntropyLoss()\n",
    "        self.dice = smp.losses.DiceLoss(mode='multiclass')\n",
    "    \n",
    "    def forward(self, outputs, targets):\n",
    "        return 0.5 * self.ce(outputs, targets) + 0.5 * self.dice(outputs, targets)\n",
    "\n",
    "criterion = DiceCELoss()\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "\n",
    "iou = JaccardIndex(task=\"multiclass\", num_classes=32).to(device)\n",
    "pixel_acc = Accuracy(task=\"multiclass\", num_classes=32).to(device)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    for images, targets in train_loader:\n",
    "        images, targets = images.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, targets in test_loader:\n",
    "            images, targets = images.to(device), targets.to(device)\n",
    "            outputs = model(images)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            iou.update(preds, targets)\n",
    "            pixel_acc.update(preds, targets)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    print(f\"IoU: {iou.compute().item():.4f} | Pixel Acc: {pixel_acc.compute().item():.4f}\")\n",
    "    iou.reset()\n",
    "    pixel_acc.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После внесения оптимизаций в самостоятельно реализованные модели удалось добиться значительного улучшения их показателей, хотя они по-прежнему несколько уступают решениям из библиотеки segmentation_models.pytorch. Улучшенная версия кастомной сверточной модели достигла IoU 0.309, что примерно на 15% ниже аналогичного показателя оптимизированной resnet34 (0.364), при этом разрыв в точности пиксельной классификации сократился до 5% (0.762 против 0.812). Еще более показательные изменения произошли с трансформерной архитектурой: после применения тех же методов оптимизации ее IoU вырос до 0.286, что хотя и меньше показателя улучшенного mit_b0 (0.394), но демонстрирует принципиальную возможность эффективного обучения самописных трансформерных моделей для задач сегментации.\n",
    "\n",
    "Сравнительный анализ показывает, что даже после всех улучшений сохраняется устойчивый разрыв около 20-27% между библиотечными и самописными реализациями, что особенно заметно на трансформерной архитектуре. Это объясняется несколькими фундаментальными факторами: во-первых, в библиотечных моделях используются более сложные и тонко настроенные механизмы внимания и нормализации; во-вторых, их энкодеры предварительно обучались на огромных датасетах; в-третьих, они содержат дополнительные архитектурные оптимизации, которые сложно полностью воспроизвести в кастомных реализациях. \n",
    "\n",
    "При этом важно отметить, что относительный прирост метрик после оптимизации у самописных моделей оказался сопоставим с библиотечными (примерно +25-30% по IoU), что подтверждает правильность выбранных направлений улучшения. Особенно показательно, что после всех доработок кастомная сверточная модель практически догнала по точности пиксельной классификации неоптимизированную версию resnet34 из библиотеки, что свидетельствует о перспективности дальнейшей работы по улучшению самостоятельно созданных архитектур."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multimedia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
