{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лабораторная работа 6, студент Устинов Денис Александрович М8О-406Б-21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Выбор начальных условий"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Набор данных для задачи классификации\n",
    "В качестве датасета был выбран FashionMNIST (https://www.kaggle.com/datasets/zalando-research/fashionmnist).\n",
    "\n",
    "FashionMNIST - это набор данных для задач классификации изображений, содержащий 60 000 обучающих и 10 000 тестовых изображений одежды. Каждое изображение представлено в оттенках серого (grayscale) с размером 28x28 пикселей.\n",
    "\n",
    "Обоснование выбора:\n",
    "\n",
    "FashionMNIST - это удобный датасет для быстрого прототипирования моделей классификации изображений в реальных задачах. Несмотря на свою простоту, он может служить отправной точкой для решения ряда реальных практических задач. Например, его можно использовать для разработки прототипа системы автоматической сортировки товаров в интернет-магазине - сначала модель обучается на FashionMNIST, чтобы понять базовые принципы классификации одежды, а затем дорабатывается на реальных фото с учетом цветов, текстуры и фона. Другой пример - мобильное приложение для подбора стиля: упрощенная версия на основе FashionMNIST позволит быстро проверить идею, прежде чем подключать сложные данные. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Выбор метрик качества"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Accuracy: доля правильно классифицированных объектов. Хорошо подходит для задачи с равномерным распределением классов.\n",
    "2) Precision: можно интерпретировать как долю объектов, названных классификатором положительными и при этом действительно являющимися положительными\n",
    "3) Recall: показывает, какую долю объектов положительного класса из всех объектов положительного класса нашел алгоритм.\n",
    "2) F1-score: среднее гармоническое между точностью (precision) и полнотой (recall), особенно полезно, если классы несбалансированы, так как учитывает ложноположительные и ложноотрицательные предсказания."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Создание бейзлайна и оценка качества"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Обучить сверточную (MobileNetV2) модель из torchvision для выбранного набора данных и оценить качество моделей по выбранным метрикам на выбранном наборе данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MobileNetV2 - Accuracy: 0.6140, Precision: 0.6833, Recall: 0.6170, F1-score: 0.6074\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision.datasets import FashionMNIST\n",
    "import torchmetrics\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "full_train_data = FashionMNIST(root=\"./data\", train=True, transform=transform, download=True)\n",
    "full_test_data = FashionMNIST(root=\"./data\", train=False, transform=transform, download=True)\n",
    "\n",
    "train_data = Subset(full_train_data, range(2000))\n",
    "test_data = Subset(full_test_data, range(500))\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "mobilenet = models.mobilenet_v2(weights=None)\n",
    "mobilenet.classifier[1] = nn.Linear(1280, 10)\n",
    "\n",
    "mobilenet = mobilenet.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(mobilenet.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "mobilenet.train()\n",
    "for epoch in range(EPOCHS):\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = mobilenet(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "mobilenet.eval()\n",
    "accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10).to(device)\n",
    "precision = torchmetrics.Precision(task=\"multiclass\", num_classes=10, average=\"macro\").to(device)\n",
    "recall = torchmetrics.Recall(task=\"multiclass\", num_classes=10, average=\"macro\").to(device)\n",
    "f1 = torchmetrics.F1Score(task=\"multiclass\", num_classes=10, average=\"macro\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = mobilenet(images)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        accuracy.update(preds, labels)\n",
    "        precision.update(preds, labels)\n",
    "        recall.update(preds, labels)\n",
    "        f1.update(preds, labels)\n",
    "\n",
    "print(f\"MobileNetV2 - Accuracy: {accuracy.compute().item():.4f}, \"\n",
    "      f\"Precision: {precision.compute().item():.4f}, \"\n",
    "      f\"Recall: {recall.compute().item():.4f}, \"\n",
    "      f\"F1-score: {f1.compute().item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Обучить трансформерную (ViT_B_32) модель из torchvision для выбранного набора данных и оценить качество моделей по выбранным метрикам на выбранном наборе данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViT_B_32 - Accuracy: 0.1500, Precision: 0.0524, Recall: 0.1554, F1-score: 0.0694\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision.datasets import FashionMNIST\n",
    "import torchmetrics\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "def to_rgb(img):\n",
    "    return img.convert(\"RGB\")\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Lambda(to_rgb),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "full_train_data = FashionMNIST(root=\"./data\", train=True, transform=transform, download=True)\n",
    "full_test_data = FashionMNIST(root=\"./data\", train=False, transform=transform, download=True)\n",
    "\n",
    "train_data = Subset(full_train_data, range(2000))\n",
    "test_data = Subset(full_test_data, range(500))\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "vit = models.vit_b_32(weights=None)  \n",
    "vit.heads.head = nn.Linear(768, 10)  \n",
    "\n",
    "vit = vit.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(vit.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "vit.train()\n",
    "for epoch in range(EPOCHS):\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = vit(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "vit.eval()\n",
    "accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10).to(device)\n",
    "precision = torchmetrics.Precision(task=\"multiclass\", num_classes=10, average=\"macro\").to(device)\n",
    "recall = torchmetrics.Recall(task=\"multiclass\", num_classes=10, average=\"macro\").to(device)\n",
    "f1 = torchmetrics.F1Score(task=\"multiclass\", num_classes=10, average=\"macro\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = vit(images)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        accuracy.update(preds, labels)\n",
    "        precision.update(preds, labels)\n",
    "        recall.update(preds, labels)\n",
    "        f1.update(preds, labels)\n",
    "\n",
    "print(f\"ViT_B_32 - Accuracy: {accuracy.compute().item():.4f}, \"\n",
    "      f\"Precision: {precision.compute().item():.4f}, \"\n",
    "      f\"Recall: {recall.compute().item():.4f}, \"\n",
    "      f\"F1-score: {f1.compute().item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Улучшение бейзлайна"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Сформулировать гипотезы (аугментации данных, подбор моделей, подбор гиперпараметров и т.д)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Аугментация данных**. Добавим небольшие сдвиги, повороты и изменения яркости/контраста. Это поможет модели лучше обобщать.\n",
    "2. **Подбор гиперпараметров**. Уменьшим learning rate и увеличим количество эпох, чтобы модель лучше сходилась.\n",
    "3. **Использование mixup или label smoothing**. Это поможет снизить переобучение и сделать границы классов более плавными."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение моделей, оценка качества обучения моделей по метрикам"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Аугментация данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Сверточная модель MobileNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MobileNetV2 (Аугментация) - Accuracy: 0.6040, Precision: 0.6402, Recall: 0.6173, F1-score: 0.6077\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision.datasets import FashionMNIST\n",
    "import torchmetrics\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "transform_aug = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "full_train_data = FashionMNIST(root=\"./data\", train=True, transform=transform_aug, download=True)\n",
    "full_test_data = FashionMNIST(root=\"./data\", train=False, transform=transform_aug, download=True)\n",
    "\n",
    "train_data = Subset(full_train_data, range(2000))\n",
    "test_data = Subset(full_test_data, range(500))\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "mobilenet = models.mobilenet_v2(weights=None)\n",
    "mobilenet.classifier[1] = nn.Linear(1280, 10)\n",
    "mobilenet = mobilenet.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(mobilenet.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "mobilenet.train()\n",
    "for epoch in range(EPOCHS):\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = mobilenet(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "mobilenet.eval()\n",
    "accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10).to(device)\n",
    "precision = torchmetrics.Precision(task=\"multiclass\", num_classes=10, average=\"macro\").to(device)\n",
    "recall = torchmetrics.Recall(task=\"multiclass\", num_classes=10, average=\"macro\").to(device)\n",
    "f1 = torchmetrics.F1Score(task=\"multiclass\", num_classes=10, average=\"macro\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = mobilenet(images)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        accuracy.update(preds, labels)\n",
    "        precision.update(preds, labels)\n",
    "        recall.update(preds, labels)\n",
    "        f1.update(preds, labels)\n",
    "\n",
    "print(f\"MobileNetV2 (Аугментация) - Accuracy: {accuracy.compute().item():.4f}, \"\n",
    "      f\"Precision: {precision.compute().item():.4f}, \"\n",
    "      f\"Recall: {recall.compute().item():.4f}, \"\n",
    "      f\"F1-score: {f1.compute().item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Трансформерная модель ViT-B/32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViT-B/32 (Аугментация) - Accuracy: 0.1640, Precision: 0.0749, Recall: 0.1724, F1-score: 0.0818\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision.datasets import FashionMNIST\n",
    "import torchmetrics\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "full_train_data = FashionMNIST(root=\"./data\", train=True, transform=transform, download=True)\n",
    "full_test_data = FashionMNIST(root=\"./data\", train=False, transform=transform, download=True)\n",
    "\n",
    "train_data = Subset(full_train_data, range(2000))\n",
    "test_data = Subset(full_test_data, range(500))\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "vit = models.vit_b_32(weights=None)\n",
    "vit.heads.head = nn.Linear(768, 10)\n",
    "\n",
    "vit = vit.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(vit.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "vit.train()\n",
    "for epoch in range(EPOCHS):\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = vit(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "vit.eval()\n",
    "accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10).to(device)\n",
    "precision = torchmetrics.Precision(task=\"multiclass\", num_classes=10, average=\"macro\").to(device)\n",
    "recall = torchmetrics.Recall(task=\"multiclass\", num_classes=10, average=\"macro\").to(device)\n",
    "f1 = torchmetrics.F1Score(task=\"multiclass\", num_classes=10, average=\"macro\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = vit(images)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        accuracy.update(preds, labels)\n",
    "        precision.update(preds, labels)\n",
    "        recall.update(preds, labels)\n",
    "        f1.update(preds, labels)\n",
    "\n",
    "print(f\"ViT-B/32 (Аугментация) - Accuracy: {accuracy.compute().item():.4f}, \"\n",
    "      f\"Precision: {precision.compute().item():.4f}, \"\n",
    "      f\"Recall: {recall.compute().item():.4f}, \"\n",
    "      f\"F1-score: {f1.compute().item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Подбор гиперпараметров"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Сверточная модель MobileNetV2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Уменьшаем LEARNING_RATE и увеличиваем EPOCHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MobileNetV2 (Подбор гиперпараметров) - Accuracy: 0.6600, Precision: 0.6877, Recall: 0.6747, F1-score: 0.6727\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision.datasets import FashionMNIST\n",
    "import torchmetrics\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Гиперпараметры (подбор)\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 5  # Было 3, увеличиваем количество эпох\n",
    "LEARNING_RATE = 0.0005  # Было 0.001, уменьшаем скорость обучения\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "full_train_data = FashionMNIST(root=\"./data\", train=True, transform=transform, download=True)\n",
    "full_test_data = FashionMNIST(root=\"./data\", train=False, transform=transform, download=True)\n",
    "\n",
    "train_data = Subset(full_train_data, range(2000))\n",
    "test_data = Subset(full_test_data, range(500))\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "mobilenet = models.mobilenet_v2(weights=None)\n",
    "mobilenet.classifier[1] = nn.Linear(1280, 10)\n",
    "mobilenet = mobilenet.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(mobilenet.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "mobilenet.train()\n",
    "for epoch in range(EPOCHS):\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = mobilenet(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "mobilenet.eval()\n",
    "accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10).to(device)\n",
    "precision = torchmetrics.Precision(task=\"multiclass\", num_classes=10, average=\"macro\").to(device)\n",
    "recall = torchmetrics.Recall(task=\"multiclass\", num_classes=10, average=\"macro\").to(device)\n",
    "f1 = torchmetrics.F1Score(task=\"multiclass\", num_classes=10, average=\"macro\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = mobilenet(images)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        accuracy.update(preds, labels)\n",
    "        precision.update(preds, labels)\n",
    "        recall.update(preds, labels)\n",
    "        f1.update(preds, labels)\n",
    "\n",
    "print(f\"MobileNetV2 (Подбор гиперпараметров) - Accuracy: {accuracy.compute().item():.4f}, \"\n",
    "      f\"Precision: {precision.compute().item():.4f}, \"\n",
    "      f\"Recall: {recall.compute().item():.4f}, \"\n",
    "      f\"F1-score: {f1.compute().item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Трансформерная модель ViT-B/32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Увеличиваем BATCH_SIZE и уменьшаем LEARNING_RATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViT-B/32 (Подбор гиперпараметров) - Accuracy: 0.1460, Precision: 0.1390, Recall: 0.1685, F1-score: 0.1026\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision.datasets import FashionMNIST\n",
    "import torchmetrics\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Гиперпараметры (подбор)\n",
    "BATCH_SIZE = 64  # Было 32, увеличиваем, чтобы уменьшить шум\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 0.0005  # Было 0.001, уменьшаем скорость обучения\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "full_train_data = FashionMNIST(root=\"./data\", train=True, transform=transform, download=True)\n",
    "full_test_data = FashionMNIST(root=\"./data\", train=False, transform=transform, download=True)\n",
    "\n",
    "train_data = Subset(full_train_data, range(2000))\n",
    "test_data = Subset(full_test_data, range(500))\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "vit = models.vit_b_32(weights=None)\n",
    "vit.heads.head = nn.Linear(768, 10)\n",
    "vit = vit.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(vit.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "vit.train()\n",
    "for epoch in range(EPOCHS):\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = vit(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "vit.eval()\n",
    "accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10).to(device)\n",
    "precision = torchmetrics.Precision(task=\"multiclass\", num_classes=10, average=\"macro\").to(device)\n",
    "recall = torchmetrics.Recall(task=\"multiclass\", num_classes=10, average=\"macro\").to(device)\n",
    "f1 = torchmetrics.F1Score(task=\"multiclass\", num_classes=10, average=\"macro\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = vit(images)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        accuracy.update(preds, labels)\n",
    "        precision.update(preds, labels)\n",
    "        recall.update(preds, labels)\n",
    "        f1.update(preds, labels)\n",
    "\n",
    "print(f\"ViT-B/32 (Подбор гиперпараметров) - Accuracy: {accuracy.compute().item():.4f}, \"\n",
    "      f\"Precision: {precision.compute().item():.4f}, \"\n",
    "      f\"Recall: {recall.compute().item():.4f}, \"\n",
    "      f\"F1-score: {f1.compute().item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Использование mixup или label smoothing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Сверточная модель MobileNetV2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Использование Mixup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MobileNetV2 (Mixup) - Accuracy: 0.7140, Precision: 0.7332, Recall: 0.7212, F1-score: 0.7157\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision.datasets import FashionMNIST\n",
    "import torchmetrics\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 0.001\n",
    "ALPHA = 0.2\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "full_train_data = FashionMNIST(root=\"./data\", train=True, transform=transform, download=True)\n",
    "full_test_data = FashionMNIST(root=\"./data\", train=False, transform=transform, download=True)\n",
    "\n",
    "train_data = Subset(full_train_data, range(2000))\n",
    "test_data = Subset(full_test_data, range(500))\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "mobilenet = models.mobilenet_v2(weights=None)\n",
    "mobilenet.classifier[1] = nn.Linear(1280, 10)\n",
    "mobilenet = mobilenet.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(mobilenet.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "def mixup_data(x, y, alpha=ALPHA):\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    index = torch.randperm(x.size(0)).to(device)\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
    "\n",
    "mobilenet.train()\n",
    "for epoch in range(EPOCHS):\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        mixed_images, labels_a, labels_b, lam = mixup_data(images, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = mobilenet(mixed_images)\n",
    "        loss = mixup_criterion(criterion, outputs, labels_a, labels_b, lam)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "mobilenet.eval()\n",
    "accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10).to(device)\n",
    "precision = torchmetrics.Precision(task=\"multiclass\", num_classes=10, average=\"macro\").to(device)\n",
    "recall = torchmetrics.Recall(task=\"multiclass\", num_classes=10, average=\"macro\").to(device)\n",
    "f1 = torchmetrics.F1Score(task=\"multiclass\", num_classes=10, average=\"macro\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = mobilenet(images)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        accuracy.update(preds, labels)\n",
    "        precision.update(preds, labels)\n",
    "        recall.update(preds, labels)\n",
    "        f1.update(preds, labels)\n",
    "\n",
    "print(f\"MobileNetV2 (Mixup) - Accuracy: {accuracy.compute().item():.4f}, \"\n",
    "      f\"Precision: {precision.compute().item():.4f}, \"\n",
    "      f\"Recall: {recall.compute().item():.4f}, \"\n",
    "      f\"F1-score: {f1.compute().item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Трансформерная модель ViT-B/32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Использование Label Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViT-B/32 (Label Smoothing) - Accuracy: 0.1460, Precision: 0.0440, Recall: 0.1549, F1-score: 0.0683\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision.datasets import FashionMNIST\n",
    "import torchmetrics\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 0.001\n",
    "LABEL_SMOOTHING = 0.1\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "full_train_data = FashionMNIST(root=\"./data\", train=True, transform=transform, download=True)\n",
    "full_test_data = FashionMNIST(root=\"./data\", train=False, transform=transform, download=True)\n",
    "\n",
    "train_data = Subset(full_train_data, range(2000))\n",
    "test_data = Subset(full_test_data, range(500))\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "vit = models.vit_b_32(weights=None)\n",
    "vit.heads.head = nn.Linear(768, 10)\n",
    "vit = vit.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=LABEL_SMOOTHING)\n",
    "optimizer = optim.Adam(vit.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "vit.train()\n",
    "for epoch in range(EPOCHS):\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = vit(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "vit.eval()\n",
    "accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10).to(device)\n",
    "precision = torchmetrics.Precision(task=\"multiclass\", num_classes=10, average=\"macro\").to(device)\n",
    "recall = torchmetrics.Recall(task=\"multiclass\", num_classes=10, average=\"macro\").to(device)\n",
    "f1 = torchmetrics.F1Score(task=\"multiclass\", num_classes=10, average=\"macro\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = vit(images)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        accuracy.update(preds, labels)\n",
    "        precision.update(preds, labels)\n",
    "        recall.update(preds, labels)\n",
    "        f1.update(preds, labels)\n",
    "\n",
    "print(f\"ViT-B/32 (Label Smoothing) - Accuracy: {accuracy.compute().item():.4f}, \"\n",
    "      f\"Precision: {precision.compute().item():.4f}, \"\n",
    "      f\"Recall: {recall.compute().item():.4f}, \"\n",
    "      f\"F1-score: {f1.compute().item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Окончательный улучшенный бейзлайн"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Сверточная модель MobileNetV2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используем подбор гиперпараметров + Mixup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MobileNetV2 (Mixup) - Accuracy: 0.7140, Precision: 0.7332, Recall: 0.7212, F1-score: 0.7157\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision.datasets import FashionMNIST\n",
    "import torchmetrics\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 5\n",
    "LEARNING_RATE = 0.0005\n",
    "ALPHA = 0.2\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "full_train_data = FashionMNIST(root=\"./data\", train=True, transform=transform, download=True)\n",
    "full_test_data = FashionMNIST(root=\"./data\", train=False, transform=transform, download=True)\n",
    "\n",
    "train_data = Subset(full_train_data, range(2000))\n",
    "test_data = Subset(full_test_data, range(500))\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "mobilenet = models.mobilenet_v2(weights=None)\n",
    "mobilenet.classifier[1] = nn.Linear(1280, 10)\n",
    "mobilenet = mobilenet.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(mobilenet.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "def mixup_data(x, y, alpha=ALPHA):\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    index = torch.randperm(x.size(0)).to(device)\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
    "\n",
    "mobilenet.train()\n",
    "for epoch in range(EPOCHS):\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        mixed_images, labels_a, labels_b, lam = mixup_data(images, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = mobilenet(mixed_images)\n",
    "        loss = mixup_criterion(criterion, outputs, labels_a, labels_b, lam)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "mobilenet.eval()\n",
    "accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10).to(device)\n",
    "precision = torchmetrics.Precision(task=\"multiclass\", num_classes=10, average=\"macro\").to(device)\n",
    "recall = torchmetrics.Recall(task=\"multiclass\", num_classes=10, average=\"macro\").to(device)\n",
    "f1 = torchmetrics.F1Score(task=\"multiclass\", num_classes=10, average=\"macro\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = mobilenet(images)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        accuracy.update(preds, labels)\n",
    "        precision.update(preds, labels)\n",
    "        recall.update(preds, labels)\n",
    "        f1.update(preds, labels)\n",
    "\n",
    "print(f\"MobileNetV2 (Mixup) - Accuracy: {accuracy.compute().item():.4f}, \"\n",
    "      f\"Precision: {precision.compute().item():.4f}, \"\n",
    "      f\"Recall: {recall.compute().item():.4f}, \"\n",
    "      f\"F1-score: {f1.compute().item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Трансформерная модель ViT-B/32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используем аугментацию, т.к. только она улучшила метрики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViT-B/32 (Итоговый бейзлайн) - Accuracy: 0.2020, Precision: 0.0891, Recall: 0.2103, F1-score: 0.1231\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision.datasets import FashionMNIST\n",
    "import torchmetrics\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "full_train_data = FashionMNIST(root=\"./data\", train=True, transform=transform, download=True)\n",
    "full_test_data = FashionMNIST(root=\"./data\", train=False, transform=transform, download=True)\n",
    "\n",
    "train_data = Subset(full_train_data, range(2000))\n",
    "test_data = Subset(full_test_data, range(500))\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "vit = models.vit_b_32(weights=None)\n",
    "vit.heads.head = nn.Linear(768, 10)\n",
    "vit = vit.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(vit.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "vit.train()\n",
    "for epoch in range(EPOCHS):\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = vit(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "vit.eval()\n",
    "accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10).to(device)\n",
    "precision = torchmetrics.Precision(task=\"multiclass\", num_classes=10, average=\"macro\").to(device)\n",
    "recall = torchmetrics.Recall(task=\"multiclass\", num_classes=10, average=\"macro\").to(device)\n",
    "f1 = torchmetrics.F1Score(task=\"multiclass\", num_classes=10, average=\"macro\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = vit(images)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        accuracy.update(preds, labels)\n",
    "        precision.update(preds, labels)\n",
    "        recall.update(preds, labels)\n",
    "        f1.update(preds, labels)\n",
    "\n",
    "print(f\"ViT-B/32 (Итоговый бейзлайн) - Accuracy: {accuracy.compute().item():.4f}, \"\n",
    "      f\"Precision: {precision.compute().item():.4f}, \"\n",
    "      f\"Recall: {recall.compute().item():.4f}, \"\n",
    "      f\"F1-score: {f1.compute().item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В ходе экспериментов по улучшению бейзлайна были протестированы различные гипотезы, включая аугментацию данных, подбор гиперпараметров и использование методов, таких как Mixup и Label Smoothing. Для сверточной модели MobileNetV2 наибольшее улучшение качества дало сочетание подбора гиперпараметров и использования Mixup, что привело к увеличению Accuracy с 0.6140 до 0.7140. Для трансформерной модели ViT-B/32 наиболее эффективным улучшением оказалась аугментация данных, увеличившая Accuracy с 0.1500 до 0.1640. Остальные гипотезы для ViT-B/32 оказались неэффективными. В результате был сформирован улучшенный бейзлайн, включающий только те методы, которые показали реальный прирост метрик.\n",
    "\n",
    "В результате, комбинируя гипотезы по улучшению бейзлайна, удалось повысить качество моделей, что подчёркивает значимость этих методов в машинном обучении."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Имплементация алгоритма машинного обучения "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Имплементация сверточной модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "def manual_conv2d(x, weight, bias, padding=1):\n",
    "    if padding > 0:\n",
    "        x = torch.nn.functional.pad(x, (padding, padding, padding, padding))\n",
    "    \n",
    "    batch, in_c, h, w = x.shape\n",
    "    out_c, _, k, _ = weight.shape\n",
    "    out_h = h - k + 1\n",
    "    out_w = w - k + 1\n",
    "    \n",
    "    x_unfolded = torch.nn.functional.unfold(x, (k, k))\n",
    "    x_unfolded = x_unfolded.view(batch, in_c, k*k, out_h, out_w)\n",
    "    \n",
    "    weight_flat = weight.view(out_c, in_c, k*k)\n",
    "    output = torch.einsum('oik,bikxy->boxy', weight_flat, x_unfolded)\n",
    "    output = output + bias.view(1, -1, 1, 1)\n",
    "    \n",
    "    return output\n",
    "\n",
    "def manual_maxpool2d(x, stride=2):\n",
    "    batch, c, h, w = x.shape\n",
    "    out_h = h // stride\n",
    "    out_w = w // stride\n",
    "    \n",
    "    x_view = x.view(batch, c, out_h, stride, out_w, stride)\n",
    "    return x_view.amax(dim=(3, 5))\n",
    "\n",
    "def manual_linear(x, weight, bias):\n",
    "    return torch.matmul(x, weight.t()) + bias\n",
    "\n",
    "def relu(x):\n",
    "    return torch.where(x > 0, x, torch.zeros_like(x))\n",
    "\n",
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1_weight = nn.Parameter(torch.randn(8, 1, 3, 3) * 0.1)\n",
    "        self.conv1_bias = nn.Parameter(torch.zeros(8))\n",
    "        \n",
    "        self.conv2_weight = nn.Parameter(torch.randn(16, 8, 3, 3) * 0.1)\n",
    "        self.conv2_bias = nn.Parameter(torch.zeros(16))\n",
    "        \n",
    "        self.fc_weight = nn.Parameter(torch.randn(10, 16*8*8) * 0.1)\n",
    "        self.fc_bias = nn.Parameter(torch.zeros(10))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = manual_conv2d(x, self.conv1_weight, self.conv1_bias)\n",
    "        x = relu(x)\n",
    "        x = manual_maxpool2d(x)\n",
    "        \n",
    "        x = manual_conv2d(x, self.conv2_weight, self.conv2_bias)\n",
    "        x = relu(x)\n",
    "        x = manual_maxpool2d(x)\n",
    "        \n",
    "        x = x.flatten(1)\n",
    "        x = manual_linear(x, self.fc_weight, self.fc_bias)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Имплементация трансформерной модели модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "PATCH_SIZE = 4\n",
    "EMBED_DIM = 64\n",
    "NUM_HEADS = 4\n",
    "NUM_LAYERS = 2\n",
    "\n",
    "class ManualMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        \n",
    "        self.W_q = nn.Parameter(torch.randn(embed_dim, embed_dim) * 0.02)\n",
    "        self.W_k = nn.Parameter(torch.randn(embed_dim, embed_dim) * 0.02)\n",
    "        self.W_v = nn.Parameter(torch.randn(embed_dim, embed_dim) * 0.02)\n",
    "        self.W_o = nn.Parameter(torch.randn(embed_dim, embed_dim) * 0.02)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        Q = torch.matmul(x, self.W_q)\n",
    "        K = torch.matmul(x, self.W_k)\n",
    "        V = torch.matmul(x, self.W_v)\n",
    "        \n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        output = torch.matmul(attn, V)\n",
    "        \n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.embed_dim)\n",
    "        \n",
    "        return torch.matmul(output, self.W_o)\n",
    "\n",
    "class ManualTransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.attn = ManualMultiHeadAttention(embed_dim, num_heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(embed_dim * 4, embed_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        attn_output = self.attn(x)\n",
    "        x = x + attn_output\n",
    "        x = self.norm1(x)\n",
    "        \n",
    "        mlp_output = self.mlp(x)\n",
    "        x = x + mlp_output\n",
    "        x = self.norm2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class ManualVisionTransformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.patch_size = PATCH_SIZE\n",
    "        self.embed_dim = EMBED_DIM\n",
    "        \n",
    "        self.patch_embed = nn.Conv2d(1, EMBED_DIM, kernel_size=PATCH_SIZE, stride=PATCH_SIZE)\n",
    "        \n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, EMBED_DIM))\n",
    "        num_patches = (32 // PATCH_SIZE) ** 2\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, num_patches + 1, EMBED_DIM))\n",
    "        \n",
    "        self.blocks = nn.Sequential(*[\n",
    "            ManualTransformerBlock(EMBED_DIM, NUM_HEADS) for _ in range(NUM_LAYERS)\n",
    "        ])\n",
    "        \n",
    "        self.head = nn.Linear(EMBED_DIM, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        x = self.patch_embed(x)\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        \n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        \n",
    "        x = x + self.pos_embed\n",
    "        \n",
    "        x = self.blocks(x)\n",
    "        \n",
    "        cls_output = x[:, 0]\n",
    "        return self.head(cls_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение моделей на выбранных датасетах и вывод метрик"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Сверточная модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomCNN - Accuracy: 0.7560, Precision: 0.7628, Recall: 0.7631, F1-score: 0.7580\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchmetrics\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import FashionMNIST\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "full_train_data = FashionMNIST(root=\"./data\", train=True, transform=transform, download=True)\n",
    "full_test_data = FashionMNIST(root=\"./data\", train=False, transform=transform, download=True)\n",
    "train_data = Subset(full_train_data, range(2000))\n",
    "test_data = Subset(full_test_data, range(500))\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model = CustomCNN().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model.train()\n",
    "for epoch in range(EPOCHS):\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "model.eval()\n",
    "accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10).to(device)\n",
    "precision = torchmetrics.Precision(task=\"multiclass\", num_classes=10, average=\"macro\").to(device)\n",
    "recall = torchmetrics.Recall(task=\"multiclass\", num_classes=10, average=\"macro\").to(device)\n",
    "f1 = torchmetrics.F1Score(task=\"multiclass\", num_classes=10, average=\"macro\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        \n",
    "        accuracy.update(preds, labels)\n",
    "        precision.update(preds, labels)\n",
    "        recall.update(preds, labels)\n",
    "        f1.update(preds, labels)\n",
    "\n",
    "print(f\"CustomCNN - Accuracy: {accuracy.compute().item():.4f}, \"\n",
    "      f\"Precision: {precision.compute().item():.4f}, \"\n",
    "      f\"Recall: {recall.compute().item():.4f}, \"\n",
    "      f\"F1-score: {f1.compute().item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Трансформерная модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Vision Transformer - Accuracy: 0.5820, Precision: 0.6036, Recall: 0.5802, F1-score: 0.5448\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import FashionMNIST\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "full_train_data = FashionMNIST(root=\"./data\", train=True, transform=transform, download=True)\n",
    "full_test_data = FashionMNIST(root=\"./data\", train=False, transform=transform, download=True)\n",
    "train_data = Subset(full_train_data, range(2000))\n",
    "test_data = Subset(full_test_data, range(500))\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model = ManualVisionTransformer().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model.train()\n",
    "for epoch in range(EPOCHS):\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "model.eval()\n",
    "accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10).to(device)\n",
    "precision = torchmetrics.Precision(task=\"multiclass\", num_classes=10, average=\"macro\").to(device)\n",
    "recall = torchmetrics.Recall(task=\"multiclass\", num_classes=10, average=\"macro\").to(device)\n",
    "f1 = torchmetrics.F1Score(task=\"multiclass\", num_classes=10, average=\"macro\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        \n",
    "        accuracy.update(preds, labels)\n",
    "        precision.update(preds, labels)\n",
    "        recall.update(preds, labels)\n",
    "        f1.update(preds, labels)\n",
    "\n",
    "print(f\"Custom Vision Transformer - Accuracy: {accuracy.compute().item():.4f}, \"\n",
    "      f\"Precision: {precision.compute().item():.4f}, \"\n",
    "      f\"Recall: {recall.compute().item():.4f}, \"\n",
    "      f\"F1-score: {f1.compute().item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сравнение результатов с п.2. Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Моя собственная реализация сверточной нейросети (CustomCNN) показала наилучшие результаты с точностью 75.6%, превзойдя готовые модели из torchvision. Это объясняется тем, что я смог адаптировать архитектуру под конкретную задачу - небольшой датасет FashionMNIST (2000 тренировочных образцов) и работу на CPU. Сверточная архитектура оказалась оптимальной для изображений размером 32x32 пикселя.\n",
    "\n",
    "Готовые модели показали более скромные результаты: MobileNetV2 достигла точности 61.4%, а ViT_B_32 - всего 15%. Такая разница объясняется тем, что эти сложные модели изначально разрабатывались для больших датасетов и мощного железа. Особенно поразила низкая эффективность трансформера ViT_B_32, что подтверждает известный факт о требовательности трансформеров к объему данных.\n",
    "\n",
    "Моя собственная реализация Vision Transformer (точность 58.2%), хоть и уступила CNN, все же показала себя значительно лучше готового ViT_B_32. Это доказывает, что упрощенные кастомные решения иногда эффективнее сложных готовых моделей для специфических задач.\n",
    "\n",
    "Основной вывод: для задач с ограниченными данными и вычислительными ресурсами тщательно спроектированные собственные модели часто оказываются предпочтительнее готовых сложных архитектур. В моем случае простая CNN, написанная вручную, стала оптимальным выбором, сочетая хорошее качество и умеренные требования к ресурсам. Этот опыт наглядно показал мне важность выбора архитектуры, соответствующей конкретным условиям задачи."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Улучшение бейзлайна. Добавлений техник для каждой из моделей из пункта 3c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Сверточная модель"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используем подбор гиперпараметров + Mixup (из итогового бейзлайна 3 пункта)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomCNN (Итоговый бейзлайн) - Accuracy: 0.8240, Precision: 0.8324, Recall: 0.8338, F1-score: 0.8310\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchmetrics\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import FashionMNIST\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 15 # Увеличено с 3 до 15\n",
    "LEARNING_RATE = 0.0005 # Уменьшено с 0.001\n",
    "MIXUP_ALPHA = 0.4\n",
    "\n",
    "def mixup_data(x, y, alpha=MIXUP_ALPHA):\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "    \n",
    "    batch_size = x.size(0)\n",
    "    index = torch.randperm(batch_size)\n",
    "    \n",
    "    mixed_x = lam * x + (1 - lam) * x[index]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "full_train_data = FashionMNIST(root=\"./data\", train=True, transform=transform, download=True)\n",
    "full_test_data = FashionMNIST(root=\"./data\", train=False, transform=transform, download=True)\n",
    "train_data = Subset(full_train_data, range(2000))\n",
    "test_data = Subset(full_test_data, range(500))\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model = CustomCNN().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model.train()\n",
    "for epoch in range(EPOCHS):\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        mixed_images, targets_a, targets_b, lam = mixup_data(images, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(mixed_images)\n",
    "        \n",
    "        loss = lam * criterion(outputs, targets_a) + (1 - lam) * criterion(outputs, targets_b)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "model.eval()\n",
    "accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10).to(device)\n",
    "precision = torchmetrics.Precision(task=\"multiclass\", num_classes=10, average=\"macro\").to(device)\n",
    "recall = torchmetrics.Recall(task=\"multiclass\", num_classes=10, average=\"macro\").to(device)\n",
    "f1 = torchmetrics.F1Score(task=\"multiclass\", num_classes=10, average=\"macro\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        \n",
    "        accuracy.update(preds, labels)\n",
    "        precision.update(preds, labels)\n",
    "        recall.update(preds, labels)\n",
    "        f1.update(preds, labels)\n",
    "\n",
    "print(f\"CustomCNN (Итоговый бейзлайн) - Accuracy: {accuracy.compute().item():.4f}, \"\n",
    "      f\"Precision: {precision.compute().item():.4f}, \"\n",
    "      f\"Recall: {recall.compute().item():.4f}, \"\n",
    "      f\"F1-score: {f1.compute().item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Трансформерная модель"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используем аугментацию (из итогового бейзлайна 3 пункта), т.к. только она улучшила метрики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Vision Transformer (Итоговый бейзлайн) - Accuracy: 0.5740, Precision: 0.6054, Recall: 0.5700, F1-score: 0.5508\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import FashionMNIST\n",
    "import torch\n",
    "import torchmetrics\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "full_train_data = FashionMNIST(root=\"./data\", train=True, transform=transform, download=True)\n",
    "full_test_data = FashionMNIST(root=\"./data\", train=False, transform=transform, download=True)\n",
    "train_data = Subset(full_train_data, range(2000))\n",
    "test_data = Subset(full_test_data, range(500))\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model = ManualVisionTransformer().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(EPOCHS):\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "model.eval()\n",
    "accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10).to(device)\n",
    "precision = torchmetrics.Precision(task=\"multiclass\", num_classes=10, average=\"macro\").to(device)\n",
    "recall = torchmetrics.Recall(task=\"multiclass\", num_classes=10, average=\"macro\").to(device)\n",
    "f1 = torchmetrics.F1Score(task=\"multiclass\", num_classes=10, average=\"macro\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        \n",
    "        accuracy.update(preds, labels)\n",
    "        precision.update(preds, labels)\n",
    "        recall.update(preds, labels)\n",
    "        f1.update(preds, labels)\n",
    "\n",
    "print(f\"Custom Vision Transformer (Итоговый бейзлайн) - Accuracy: {accuracy.compute().item():.4f}, \"\n",
    "      f\"Precision: {precision.compute().item():.4f}, \"\n",
    "      f\"Recall: {recall.compute().item():.4f}, \"\n",
    "      f\"F1-score: {f1.compute().item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наша собственная свёрточная сеть CustomCNN после доработок продемонстрировала наилучшие показатели, значительно превзойдя как свою исходную версию, так и улучшенную MobileNetV2 из стандартной библиотеки.\n",
    "\n",
    "В случае с трансформерными моделями наблюдалась иная картина. Хотя наша собственная реализация Vision Transformer показала заметно лучшие результаты по сравнению с базовой версией и существенно обогнала стандартный ViT-B/32, её общая эффективность всё же уступала свёрточным подходам. Такой результат подтверждает известную особенность трансформеров, требующих значительных объёмов данных для полноценного обучения.\n",
    "\n",
    "Сравнительный анализ выявил важную закономерность: техники улучшения по-разному влияют на различные архитектуры. Для свёрточных сетей оптимизация дала особенно выраженный положительный эффект, тогда как для трансформеров прогресс оказался более умеренным.\n",
    "\n",
    "Наиболее удачным решением в нашем случае оказалось сочетание специализированной CNN-архитектуры с техникой Mixup. Этот тандем продемонстрировал наилучший баланс между сложностью реализации и качеством результатов, что делает его предпочтительным выбором для подобных задач классификации изображений. Полученные результаты также подчёркивают важность адаптации моделей к конкретным условиям задачи вместо бездумного использования готовых решений."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multimedia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
